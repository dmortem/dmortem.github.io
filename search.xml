<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[深度学习技巧]]></title>
    <url>%2F2018%2F02%2F14%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8A%80%E5%B7%A7%2F</url>
    <content type="text"><![CDATA[在这里总结一下看到过的各路炼丹技巧 1、 深度学习的7大实用技巧* 链接 数据：有标签的数据越多，模型的性能越好。重温深度学习时代数据不可思议的有效性 优化器：RMSprop，Adadelta和Adam法都是自适应优化算法，会自动更新学习率。普通的随机梯度下降法需要手动选择学习率和动量参数。 在实践中，自适应优化器往往比普通的梯度下降法更快地让模型达到收敛状态。然而，选择这些优化器的模型最终性能都不太好，而普通的梯度下降法通常能够达到更好的收敛最小值，从而获得更好的模型性能，但这可能比某些优化程序需要更多的收敛时间。此外，随机梯度下降法也更依赖于有效的初始化方法和学习速率衰减指数的设置，这在实践中是很难确定的。如下图，Adam初期效果好，但SGD在训练结束后可以获得更好的全局最小值 可以混合使用两类优化器：由Adam优化器过渡到随机梯度下降法来优化模型。在训练的早期阶段，使用Adam优化器来启动模型的训练，这将为模型的训练节省很多参数初始化和微调的时间。一旦模型的性能有所起伏，我们就可以切换到带动量的随机梯度下降法来进一步优化模型，以达到最佳的性能 处理数据不平衡：不同类的训练数据数量差异较大，会使得预测结果倾向于数量大的类别。 对损失函数使用类别权重：对于数据量小的类别，损失函数中使用较高的权重值。这样对该类的任何错误都将导致非常大的损失值，以此来惩罚错误分类过度抽样：对于小样本类别，重复进行采样欠采样：对于大样本类别，跳过不选择部分数据数据增强：对于小样本类别进行数据增强，生成更多训练样本 迁移学习 数据增强：保证数据原始类别标签的同时，对一些原始图像进行非线性的图像变换，来生成/合成新的训练样本。即对图像进行任何操作，改变图像的外观，但不能改变整体的内容。如对于猫的图像，我们可以把图像进行任意的几何变换，只要还是一只猫，那么label就不会变，但在图像以外的问题上能否直接使用数据增强有待进一步查阅资料 水平或垂直旋转/翻转图像随机改变图像的亮度和颜色随机模糊图像随机裁剪图像 集成模型：对于一个特定的任务，训练多个模型并根据模型的整体性能决定最优的组合方案。具体的实现细节还不太清楚，但模型集成有三种比较典型的方式：链接。下2中有关于集成的几种常见设计 模型剪枝加速：目的是为了在提高训练速度的同时保持模型的高性能，目前已有许多相关的研究，如MobileNet等等。核心思想是按照一定标准为神经元进行排序，从而将排名低(即冗余的神经元，对模型贡献不大)的移除模型，也就是把不重要的卷积滤波器丢弃(因为神经元和卷积核一一对应) 一般的剪枝流程如下：Network -&gt; Evaluate Importance of Neurons -&gt; Remove the Least Important Neuron -&gt; Fine-tuning -&gt; Continue Pruning -&gt; Back to Evaluate Importance of Neurons 2、 知乎专栏 参数初始化：Xavier和He随便选一个，但一定要做 uniform均匀分布初始化：w = np.random.uniform(low=-scale, high=scale, size=[n_in,n_out]) Xavier初始法，适用于普通激活函数(tanh,sigmoid)：scale = np.sqrt(3/n) He初始化，适用于ReLU：scale = np.sqrt(6/n) normal高斯分布初始化：w = np.random.randn(n_in,n_out) * stdev # stdev为高斯分布的标准差，均值设为0 Xavier初始法，适用于普通激活函数 (tanh,sigmoid)：stdev = np.sqrt(n) He初始化，适用于ReLU：stdev = np.sqrt(2/n) svd初始化：对RNN有比较好的效果。参考论文：https://arxiv.org/abs/1312.6120 数据预处理 常用：zero-center X -= np.mean(X, axis = 0) # zero-center X /= np.std(X, axis = 0) # normalize 不常用：白化 训练技巧 要做梯度归一化，即算出来的梯度除以minibatch size clip c(梯度裁剪): 限制最大梯度，其实是value = sqrt(w1^2+w2^2….)。如果value超过了阈值，就算一个衰减系系数，让value的值等于阈值: 5,10,15 Dropout对小数据防止过拟合有很好的效果，值一般设为0.5。小数据上dropout+sgd效果提升都非常明显。dropout的位置比较有讲究, 对于RNN,建议放到输入-&gt;RNN与RNN-&gt;输出的位置。关于RNN如何用dropout,可以参考论文 优化器：同1中所述，SGD收敛慢，但结果好。对于SGD，可以选择从1.0或者0.1的学习率开始，隔一段时间在验证集上检查一下，如果cost没有下降，就对学习率减半。也可以先用ada系列先跑，最后快收敛的时候，更换成sgd继续训练，同样也会有提升。据说adadelta一般在分类问题上效果比较好，adam在生成问题上效果比较好 除了gate之类的地方需要把输出限制成0-1之外，尽量不要用sigmoid。可以用tanh或者relu之类的激活函数。sigmoid的问题: 1. sigmoid函数在-4到4的区间里，才有较大的梯度。其他区间的梯度接近0，很容易造成梯度消失问题。2. 如果输入是0均值的，sigmoid函数的输出不是0均值的 rnn的dim和embdding size，一般从128上下开始调整。batch size一般从128左右开始调整，合适最重要，并不是越大越好。关于Batch Size设置的分析可参考这篇文章。对于batch，建议是把卡塞满的2的n次方 word2vec初始化，在小数据上不仅可以有效提高收敛速度，也可以可以提高结果 尽量对数据做shuffle LSTM的forget gate的bias，用1.0或者更大的值做初始化可以取得更好的结果，来自这篇论文。实际使用中，不同的任务可能需要尝试不同的值 使用Batch Normalization可以提升效果 如果模型包含全连接层（MLP），且输入和输出大小一样，可以考虑将MLP替换成Highway Network，对结果有一点提升，建议作为最后提升模型的手段。原理很简单，就是给输出加了一个gate来控制信息的流动，详细介绍请参考论文 一轮加正则，一轮不加正则，反复进行 Ensemble(模型集成) Ensemble是论文刷结果的终极核武器，一般有以下几种方式： 同样的参数，不同的初始化方式 不同的参数，通过cross-validation，选取最好的几组 同样的参数，模型训练的不同阶段，即不同迭代次数的模型 不同的模型，进行线性融合。例如RNN和传统模型 3、 知乎回答 Relu+Bn可以满足95%的情况，除非有些特殊情况会用identity，比如回归问题，比如resnet的shortcut支路 Dropout，分类问题用dropout ，只需要最后一层softmax 前用基本就可以了，能够防止过拟合，可能对accuracy提高不大，但是dropout 前面的那层如果是之后要使用的feature的话，性能会大大提升 数据的shuffle和augmentation，aug不能瞎加，比如行人识别一般就不会加上下翻转的，因为不会碰到头朝下的异型种 降学习率，随着网络训练的进行，学习率要逐渐降下来。如果用tensorboard就能发现，在学习率下降的一瞬间，网络会有个巨大的性能提升。同样的fine-tuning也要根据模型的性能设置合适的学习率，比如一个训练的已经非常好的模型你上来就1e-3的学习率，那之前就白训练了，就是说网络性能越好，学习率要越小 tensorboard，帮助你监视网络的状态，来调整网络参数 随时存档模型，要有validation。把每个epoch和其对应的validation 结果存下来，可以分析出开始overfitting的时间点，方便下次加载fine-tuning 网络层数，参数量什么的都不是大问题，在性能不丢的情况下，减到最小。（不是说网络越深越好？） 对于batch size，建议是把卡塞满的2的n次方 输入减不减mean归一化在有了bn之后已经不那么重要了 卷积核的分解。从最初的5×5分解为两个3×3，到后来的3×3分解为1×3和3×1，再到resnet的1×1，3×3，1×1，再xception的3×3 channel-wise conv+1×1，网络的计算量越来越小，层数越来越多，性能越来越好，这些都是设计网络时可以借鉴的 不同尺寸的feature maps的concat，只用一层的feature map一把梭可能不如concat好，pspnet就是这种思想，这个思想很常用 resnet的shortcut确实会很有用，重点在于shortcut支路一定要是identity，主路是什么conv都无所谓，这是我亲耳听resnet作者所述 针对于metric learning，对feature加个classification 的约束通常可以提高性能加快收敛 4、链接和链接]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>炼丹技巧</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[六大聚类算法]]></title>
    <url>%2F2018%2F02%2F12%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%2F%E5%85%AD%E5%A4%A7%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[介绍六类主流的聚类算法，并总结了相应的优缺点 K-Means 均值漂移聚类 基于密度的聚类方法（DBSCAN） 用高斯混合模型（GMM）的最大期望（EM）聚类 凝聚层次聚类 图团体检测 原文来自机器之心 链接序号写成”.”格式全部崩了。。把分隔符去掉又显得太挤了。。。以后找到办法了再回来改TAT 1、 K-Means 优点：速度快，O(N) 缺点：类别数需要事先人为估计，无法通过算法来确定类别数；结果有可能不可重复 变种：K-Medians，每次用中值向量而不是均值来作为中心。对异常值不敏感，对大数据集速度慢，因为需要做排序 2、 均值漂移聚类 算法：随机选择n个中心点，有n个半径为r的圆形滑动窗口，同时开始迭代。每次迭代时，滑动窗口的中心点都不断移向窗口内所有点的均值点，也就是移向点密度更大的区域，直到收敛(点密度无法再增加)。收敛时，若多个窗口发生重叠时，保留点数最多的那个窗口。这样就可以得到类别数以及每个类的中心位置。其他各个点属于哪一类则取决于迭代过程中该点在哪一类的窗口中的次数最多 优点：类别数量由算法自动获得；聚类中心朝最大点密度聚集的事实也是非常令人满意的 缺点：窗口大小/半径「r」的选择可能是不重要的 3、 基于密度的聚类方法（DBSCAN） 算法：每次从一个未被访问过的数据点开始，若该点的邻域(ε 距离内的所有点都是邻域点)内点的数量大于参数minPoints，则该点为一个新类的第一个点，否则该点就是噪声点。若该点成为了一个新类的第一个点，则该点邻域内的所有点也成为了该类的一部分，同时以这些新点再去找它们邻域中的点并加到该类中来，直到收敛。收敛后，在未被访问过的点中再找一个重新开始之前的步骤 优点：不需要人为确定类别数；能够辨别噪声；能很好地找到任意大小和任意形状的类 缺点：当簇的密度不同时，它的表现不如其他聚类算法(因为密度不同，参数ε和minPoints的设置也需要不一样，难以估计) 4、 用高斯混合模型（GMM）的最大期望（EM）聚类 K-Means中假设数据点的分布是圆形的(其分类原理是离哪个中心点更近就认为是哪一类)，限制较大。GMM中假设数据点是高斯分布的，意味着类可以是各种类型的形状(二维中为椭圆，因为x和y方向各自有标准差)。于是，任务就变成了使用EM来找到每个类别的高斯函数。下图是典型的K-Means不适用的情况： 算法：和K-Means一样，先选择类别数量，并随机初始化高斯分布参数。随后计算每个数据点属于一个特定类的概率(一个点越靠近高斯的中心，它就越可能属于该类)。基于这些概率，计算一组新的高斯分布参数使得类内数据点的概率最大化。其中，新参数是由数据点位置的加权和得到，权重就是之前提到的概率。重复之前的操作直到收敛 优点：类形状任意(K-Means其实是GMM的一个特殊情况，即所有维度的协方差均接近0)；由于GMM使用概率，因此每个数据点可以属于很多类，且对于每个类都有一个相应的概率 5、 凝聚层次聚类 层次聚类分为自上而下和自下而上。对于自下而上，每个数据点都是单独的一个类，随后不断合并两个类，直到所有类最后都合并成一个包含所有数据点的类。自下而上层次聚类又被称为凝聚式层次聚类(HAC)。 算法：一开始有N个数据点，也就是N个类。随后自行定义一种距离度量标准如平均距离等，每次迭代时选择将两个距离最近的类合并在一起，直到只剩下一个类。我们只需要选择何时停止合并，就可以选择最终需要多少个类 优点：不需要指定类个数，我们可以自由选择看起来最好的类个数；对距离度量标准不敏感； 缺点：效率低，O(N^2) 6、图团体检测（Graph Community Detection） 当我们的数据可以被表示为一个网络或图（graph）时，我们可以使用图团体检测方法完成聚类。聚类的质量由模块性分数进行评估，模块性越高，该网络聚类成不同团体的程度就越好。因此通过最优化方法寻找最大模块性就能发现聚类该网络的最佳方法。 模块性可以使用以下公式进行计算： 其中L代表网络中边的数量，k_i和k_j是指每个顶点的degree，它可以通过将每一行和每一列的项加起来而得到。两者相乘再除以2L表示当该网络是随机分配的时候顶点i和j之间的预期边数。括号中的项表示了该网络的真实结构和随机组合时的预期结构之间的差。当A_ij==1且(k_i*k_j)/2L很小时，其返回的值最高。即当在定点i和j之间存在一个「非预期」的边时，得到的值更高。最后的δc_i, c_j是克罗内克δ函数（Kronecker-delta function）。 优点：在典型的结构化数据中和现实网状数据都有非常好的性能 缺点：会忽略一些小的集群，且只适用于结构化的图模型]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>无监督学习</tag>
        <tag>聚类</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[自编码器]]></title>
    <url>%2F2018%2F02%2F11%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%2F%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8%2F</url>
    <content type="text"><![CDATA[关于AE到VAE的介绍 待看：论文、cs231nLet13中的VAE推导]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>无监督学习</tag>
        <tag>自编码器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[cs231n学习笔记]]></title>
    <url>%2F2018%2F02%2F11%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%2Fcs231n%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[把cs231n 2017年的课程重新刷一遍，记录相关的笔记]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>公开课</tag>
        <tag>计算机视觉</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Daily Reading]]></title>
    <url>%2F2018%2F02%2F10%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%2FDaily-Reading%2F</url>
    <content type="text"><![CDATA[记录一下每天看的公众号和知乎相关内容 2017-12-26 十大机器学习Python库(机器之心) 大部分都没见过，可以看看 2017-12-27 如何在NLP领域干成第一件事(AI科技评论) 找开源项目并重现，争取进一步改善；找如ACL会议论文集，找感兴趣的方向8，了解该方向的方法/数据/团队； Python相关文章(AI科技评论)* 静态代码检查工具Flake；视频合成器；从零开始用遗传算法和深度学习演化有机体的生长过程；破解验证码；Chutes&amp;Ladders游戏模拟；创建Chatbot；图像散列；介绍BP；Memorization技术，加速Python；分享Python学习技巧 Attention模型(机器学习算法与自然语言处理) 各种Attention机制的综述 神经进化策略(机器之心)* 可能未来使用神经进化策略替代反向传播，了解一下 2017-12-28 CS中的线代(机器之心)* 一篇普渡的论文，45页 2018AI研究趋势(机器之心)** 汇总资源：开放资源(介绍了DeepMind/OpenAI等的博客，着重推荐Distill)；机器翻译；理解视频(预测下一帧等+数据集介绍)；多任务/多模式学习；强化学习(游戏Dota/星际争霸)；解释人工智能(可视化/InterpretNet)；保护人工智能被对抗样本愚弄；超越梯度(合成梯度/进化策略/SGD改进/学习优化/不同空间的优化)；3D和图形的几何深度学习 FoolNLTK：一个便捷的中文处理工具(机器之心) 基于BiLSTM的开源中文分词模型 NIPS 2017的收获与思考(AI科技评论) 3篇Best paper：博弈论相关，打败德州扑克顶级玩家；随机优化风险最小化问题；提出全新的拟合优度算法。都好高端，不知道讲的是什么这篇文章作者主要关注了强化学习； *还介绍了4篇MSRA的paper：有一篇关于机器翻译的论文可能可以用在LipNet项目中？ 如何理解hekaiming的Focal Loss(PaperWeekly) 解决分类问题中类别不平衡、分类难度差异的一个loss 草根学Python(机器学习研究会)** 寒假和笨方法学Python一起看 2017-12-29 2017最火的五篇深度学习论文(专知) CycleGAN-图像迁移；Wasserstein GAN-提出更好的用于训练GAN的目标函数；simGAN-产生模拟数据，使用未标记的真实数据来改进模拟数据(无监督的)AlphaGo zero-无人类知识先验的情况下学会下围棋深度图像先验-理解神经网络模型中先验的作用(没搞懂) 机器学习、NLP、Python和Math最好的150余个教程(AI科技大本营)** 资源汇总 2017-12-30 ICCV2017: 基于检测和跟踪的视频中人体姿态估计(专知)* 人体姿态估计相关 如何与深度学习服务器优雅的交互(夕小瑶)** 服务器相关操作 机器之心年度盘点：2017年人工智能领域备受关注的科研成果(机器之心) AlphaGo-无须人类知识标注，自我对抗德州扑克击败人类-深度学习/纳什均衡的博弈求解自归一化-比BN更好的归一化GAN和各种变体深度神经网络碰上语音合成-WaveNet大批量数据并行训练ImageNet-分布式同步SGD训练(将ResNet-50在ImageNet上的训练时间缩短到48分钟)Capsule-抛弃反向传播递归皮质网络-新型概率生成模型，旨在超越神经网络从TPU到NPU-NPU是华为麒麟970手机端的芯片 机器学习非凸优化技术(机器之心) 在线深度学习：在数据流中实时学习深度神经网络(机器之心) 2017-12-31 2018-02-10 想要实现深度神经网络？一张 Excel 表格就够了(机器之心) 链接 有一个比较完整的CNN流程，关于CNN的一些理解：链接 Facebook提出DensePose数据集和网络架构：可实现实时的人体姿态估计(机器之心)* 链接 人体姿态估计相关 2018-02-11 从语义上理解卷积核行为，UCLA朱松纯等人使用决策树量化解释CNN(机器之心)* 链接 解释CNN模型：借助决策树在语义层面上解释 CNN 做出的每一个特定预测，即哪个卷积核（或物体部位）被用于预测最终的类别，以及其在预测中贡献了多少。斯坦福也有类似的工作。 李沐《动手学深度学习》课程视频汇总(机器之心) 链接 使用Apache MXNet的最新前端Gluon作为开发工具。视频链接；中文文档 一文读懂什么是变分自编码器(专知) 链接 AE到VAE 2018-02-12 机器学习的经典算法 链接 回归算法：线性回归(数值问题，最小二乘法，使用梯度下降逼近求解函数极值问题) &amp; 逻辑回归(分类问题，结果中加一个Sigmoid函数) 用于拟合逻辑回归中非线性分类线的两种算法：神经网络(ANN) &amp; SVM*(和高斯“核”结合，表达复杂的分类界限。核最典型的特征就是可以将低维的空间映射到高维空间，低维空间的非线性分类线就相当于高维空间的线性分类线) 聚类算法：无监督的一种，K-Means。见3降维算法：无监督的一种，PCA*推荐算法：基于物品内容的推荐（将与用户购买的内容近似的物品推荐给用户） &amp; 基于用户相似度的推荐（将与目标用户兴趣相同的其他用户购买的东西推荐给目标用户），协同过滤算法*其他：高斯判别、朴素贝叶斯、决策树 ​爬虫与反爬虫 链接 有趣的爬虫与反爬虫工作介绍 六大聚类算法* 链接 参看博文六大聚类算法 最新云端&amp;单机GPU横评 链接 性能：Volta性能优于Nvidia 1080Ti（约1.1-1.3倍）和P100（约1.2-1.5倍）成本：Paperspace Volta性价比高。Google P100比Paperspace Volta贵10%，Amazon Volta比Paperspace Volta贵40% - Paperspace Volta适合只需要1个GPU的用户，性能好 - Google P100最为灵活，它允许用户在任意实例上使用1、2、4个P100 GPU（或最多 8个K80 GPU），允许用户自定义CPU和GPU配置来满足计算需求。尽管由于架构所限，Tesla P100的性能略显落后，但从成本角度考虑，其性价比很有优势 - Amazon Volta性能优于Google P100，也可以连接1、4或8个GPU。但用户无法自定义基础实例类型。性价比比较低。如果迫切需要用8个GPU或在EC2上搭建模型，那么目前仍推荐使用Amazon Volta 理解深度学习中的矩阵运算* 链接 论文链接 最新7篇VAE相关论文 链接 利用带混合解码器的条件变分自编码器生成主题汉语诗歌一种使用条件变分自编码器的Zero Shot Learning的生成模型变分自编码器在不同模态之间的双向生成使用深度密度先验的MR图像重建变分递归神经机器翻译变分自编码器的推断次优性使用条件VAEs和GANs从视觉属性中合成人脸 UC Berkeley提出特征选择新方法：条件协方差最小化* 链接 模型解释性。这篇论文的综述部分可以看一下，论文本身数学性比较强，Github降维可以增强模型的可解释性，特征选择则是常用的降维方法。特征选择算法通常可分为：滤波器（filter）、封装（wrapper）以及嵌入（embedded）。滤波器方法基于数据的本质属性选择特征，与所用的学习算法无关。如可以计算每个特征和响应变量之间的相关性，然后选择相关性最高的变量封装方法的目标是寻找能够使某个预测器的性能最优化的特征。如可以训练多个支持向量机，每个支持向量机使用不同的特征子集，然后选择在训练数据上损失最小的特征子集。因为特征子集的数量是指数规模的，所以封装方法通常会使用贪心算法嵌入方法将特征选择和预测结合成一个问题。它通常会优化一个目标函数，这个目标函数结合了拟合优度和对参数数量的惩罚。一个例子就是构建线性模型的LASSO方法，它用L1 penalty来表征对参数数目的惩罚本文提出了条件协方差最小化（CCM）方法，这是一个统一前两个观点的特征选择方法。这个方法基于最小化条件协方差算子的迹来进行特征选择。思想是选择能够最大化预测基于协变量响应依赖的特征。 深度学习的7大实用技巧* 链接参看博文深度学习技巧 2018-02-13]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>论文阅读</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Pytorch学习]]></title>
    <url>%2F2018%2F02%2F10%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%2FPytorch%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[记录一下Pytorch学习过程以及踩过的一些坑]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>Pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Welcome to my blog]]></title>
    <url>%2F2018%2F02%2F09%2F%E9%9A%8F%E7%AC%94%2FWelcome-to-my-blog%2F</url>
    <content type="text"><![CDATA[一直想搞个博客，但是一直懒得搞，拖延癌晚期/(ㄒoㄒ)/~~。。之前搞过一个jekyll的，这次想了想还是换hexo了 搭建细节主要参考了http://blog.csdn.net/gdutxiaoxu/article/details/53576018 大致流程如下： 安装node.js 本地安装Hexo 配置Hexo 把Hexo和Github Page联系 随后就是换主题、搞一下私人定制的事情啦，接下来几天慢慢看看官方文档，把这个博客做的好看一点o(￣▽￣)ブ 2018-02-10 update新添站内搜索功能、生成站点地图功能，参考https://www.ezlippi.com/blog/2017/02/hexo-search.html**删除安装的插件直接npm unistall 插件名即可根据官方文档通过修改_config.yml添加了一系列功能 2018-02-12 update在博文中添加图片，参考链接在博文中的写法：Markdown中写注释，参考链接 待添加：评论功能(next继承来必力)；http://blog.csdn.net/qq_33699981/article/details/72716951To be continued…]]></content>
      <categories>
        <category>随笔</category>
      </categories>
      <tags>
        <tag>随笔</tag>
      </tags>
  </entry>
</search>
