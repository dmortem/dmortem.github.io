<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Coarse-to-Fine Volumetric Prediction for Single-Image 3D Human Pose]]></title>
    <url>%2F2018%2F04%2F15%2FCoarse-to-Fine-Volumetric-Prediction-for-Single-Image-3D-Human-Pose%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[Two-Stream Convolutional Networks for Action Recognition in Videos]]></title>
    <url>%2F2018%2F04%2F11%2FTwo-Stream-Convolutional-Networks-for-Action-Recognition-in-Videos%2F</url>
    <content type="text"><![CDATA[著名的双流模型，发表于2014年的NIPS：Two-Stream Convolutional Networks for Action Recognition in Videos 空间流处理静态图像，使用在ImageNet上预训练的ConvNet 时间流处理连续多帧稠密光流，得到运动信息。由于用于时间流训练的数据集小，文中提出一个multi-task learning 两个流经softmax输出的score进行平均或使用multi-class linear SVM进行融合 Two-stream architecture for video recognition 使用同样结构的ConvNet分别对一张静态帧和L张光流进行处理，最后对双流经softmax输出的score进行平均或使用multi-class linear SVM进行融合 空间流网络就直接使用在ImageNet上大放异彩的网络，并使用预训练的参数 注：这里双流只是结构一样，具体的参数值肯定不一样，一个是提取空间信息，一个是时间信息 Optical flow ConvNets 细节还没仔细看 最后是下采样得到224*224*2L送入网络 Multi-task learning 空间流网络可以在ImageNet上预训练，但时间流网络的训练集相对而言就比较小了。为了减小由于训练集小而带来的过拟合问题，若把使用的两个数据集并成一个则会很麻烦 使用multi-task learning: ConvNet旨在学习一个general的video representation，对于两个数据集，在CNN后面有两个不同的分类器，分别用于输出UCF-101和HMDB-51数据集的score，而总loss就是两个分类器的loss相加 Implementation detailsConvNets Configuration 对于这个网络结构，和引用的[3]还有[31]比较类似，可以去查看这两个网络的细节 每一层都使用了Relu、3/*3 stride=2的max-pooling、使用AlexNet提出的Local Response Normalisation(现在已经不用了) 空间流和时间流的区别就在于把时间流中的第二个归一化层去掉了(看上面的图) Training 训练主要参考了AlexNet那篇文章的细节 使用mini-batch stochastic gradient descent, momentum=0.9 mini-batch_size=256个video，里面的帧是随机挑选的 从原始帧中随机提取224/224(区别于AlexNet中先从原始分辨率中提取center的256/256，再提取224/224，这里直接从原始分辨率中随机提取224/224，这也是作者认为在ILSVRC-2012准确率更高的原因)，并经历random horizontal flipping和RGB jittering 初始学习率是10^-2，随后的变化遵循一套固定的schedule：对于从头训练的ConvNet，在50K次迭代后学习率变成10^-3，在70K次迭代后变成10^-4，在80K次迭代后停止；对于fine-tuning的情况，学习率在14K次迭代后变为10^-3，在20K次迭代后停止训练 Testing 对于一个视频，我们提取固定数量的帧(这里是25)，这些帧在时间上等间隔。对于每一帧，我们通过数据增强都能获得10个不同的input 整个video的score是对所有选取的帧的score的平均 Pre-training on ImageNet ILSVRC-2012 在ImageNet上对空间流网络进行预训练时，同样要使用上面说的数据增强技术，并发现结果比[31]中更好，作者认为的原因是：在这里，他们是对图片从原始分辨率直接提取224/*224 Optical Flow 是在训练前调用OpenCV得到的 存储方面，为了减少对于float的存储，把光流数据rescaled到[0, 255]，并使用JPEG进行压缩 Evaluation 作者还尝试了slow fusion，发现slow fusion只有56.4%准确率，远不如双流好 在后面的实验中，空间流使用的是training the last layer on top of a pre-trained ConvNet Multi-task learning of temporal ConvNets 因为对于时间流的训练较难，尤其是在HMDB-51数据集上（数据量小），所以尝试不同的方案，发现multi-task learning最好，因为这种方案利用了最多的数据 若是对于HMDB-51的训练，那么在multi-task learning的过程中，可以把UCF-101的全部数据都用于训练 Two-Stream ConvNets 空间流和时间流网络互补，两个加在一起可以大幅提升准确率 使用SVM做fusion更好 使用bi-directional flow在这里没有提升 对时间流使用multi-task learning可以提升效果 Comparison with the state of the art 注：下表是对数据集的3个split求了平均，而上表则是split 1的结果 Conclusions and directions for improvement Training a temporal ConvNet on optical flow (as here) is significantly better than training on raw stacked frames Extra training data is beneficial for our temporal ConvNet There still remain some essential ingredients of the state-of-the-art shallow representation, which are missed in our current architecture.]]></content>
      <categories>
        <category>计算机视觉</category>
      </categories>
      <tags>
        <tag>计算机视觉</tag>
        <tag>Action Recognition</tag>
        <tag>深度学习</tag>
        <tag>论文阅读</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[cs231n Let11 Detection and Segmentation]]></title>
    <url>%2F2018%2F04%2F06%2Fcs231n-Let11-Detection-and-Segmentation%2F</url>
    <content type="text"><![CDATA[Lecture 11 Detection and Segmentation slides video Overview Semantic Segmentation 图片中每个像素点都有一个label 不区分instances。如下图有多只牛，不同牛位置的pixel相应的label都是cow，并不会作区分。而Instance Segmentation则会对具体的object做区分 Sliding Window 在原始图像中逐像素提取patch，把这些patch一一扔进CNN架构中进行分类。这些patch都是以当前用来分类的像素点作为中心，即该patch的label是基于中心点像素的 效率很低；而且从相邻的两个像素提取的patch几乎长的一样，最后训练得到的network权重也基本一样，这部分可以考虑shared，不需要重新去训练 全卷积(Fully Convolutional) 由于逐像素效率低，那么就直接把一整张图片送入网络，同时对所有像素点进行分类，最后输出一张C*H*W的分类图，每个像素点都有C个类别的score，这样还做到了上面所说的相邻像素可以直接使用shared feature 在原始分辨率下做卷积比较昂贵，内存占用等都会很大，尤其是当每一层的feature map数也很多的时候 改进的Fully Convolutional 既然在原始分辨率下做卷积代价高，那么就进行降维，随后再upsampling恢复原来的分辨率，得到具有原始分辨率的label图 Unsampling Classification + Localization 一些trick 同时做分类与回归bounding box？————有些人可能会对不同的类分别做bounding box的回归(因为类别不同可能影响到物体的位置与大小识别，相当于回归的loss会因为类别的识别错误而大大增加)，然后取ground truth对应的两个loss相加两个loss相加时的权重怎么看？————当作超参数进行训练 Human Pose Estimation Object Detection 相比于Classification + Localization，Object Detection的每张输入图像都对应不同数量的output，即每张图片里有若干数量未知的待测物体 Object Detection as Regression 若使用类似Classification + Localization的方法，那么由于每张图片中有不同的输出，若使用某个固定的网络，输出数量只能是一定的，因此无法简单的进行regression Object Detection as Classification: Sliding Window 使用Semantic Segmentation中提到的idea，使用sliding window后，就是对每一个window过一次网络，因此就不用担心每张图片有不同的输出了。因为这样做的话，对于一张图片，其经过k次网络，也就有k个output了，k是window的个数 window的大小、每次移动的位置都无法直接确定，而只能不断尝试，通过最后网络训练得到的结果进行判断。那么这种做法需要进行大量的训练，代价太大 Region Based Methods 由于用Sliding Window有太多的candidate window了，所以计算代价太大，那么我们可以考虑使用算法来直接找到一些出现object概率比较大的candidate区域，减小搜索范围（如可以先找edge，然后再找有没有形成闭合回路的edge，若有那么这些闭合的曲线很有可能就是object，可以作为candidate region R-CNN 先在input image上提取2000个ROI(ROI是固定算法)，随后把ROI warp成一样的size，再送入ConvNet，分别用SVM进行分类以及做线性回归(回归的是bounding box基于ROI的offset)。其中，在正式训练前，还需要对ConvNet进行微调，这时候是使用softmax进行微调 由于仍然有2000个ROI，训练耗时还是很久，而且占用磁盘、内存资源；且测试速度也慢，无法做到实时性；ROI不能通过学习进行改进，可能算法本身的识别会有误差 Fast R-CNN 既然对2000个ROI都训练ConvNet太耗时间，那么我们可以考虑把ROI提取放在feature map上，即先对input image用ConvNet进行特征提取，随后在提取的特征图上再做ROI提取，这样相当于只需要训练一个ConvNet就可以了，大大减少运行时间与其他资源 *但具体做的时候，其实ROI提取的步骤还是在input image上执行的，只是把提取的ROI根据卷积操作的空间对应性映射到feature map上 Faster R-CNN Fast R-CNN的测试时间主要是花在ROI的提取上了。而且ROI提取是固定算法，无法通过训练加以改进，可能本身的识别存在一些问题。因此，Faster R-CNN把ROI改进成RPN(Region Proposal Network)，在这里Region Proposal的提取都是可以通过学习得到。模型具体的detail比较复杂，还得去看看paper R-CNN -&gt; Fast R-CNN -&gt; Faster R-CNN Detection without Proposals Methods: YOLO/SSD 相比于Region based method对于每个region都要做一次独立的处理，YOLO和SSD直接使用一个很大的CNN对所有潜在的region做预测 Faster R-CNN速度慢，但更准确；SSD更快但没那么准确 Instance SegmentationMask R-CNN]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>计算机视觉</tag>
        <tag>深度学习</tag>
        <tag>公开课</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习及应用]]></title>
    <url>%2F2018%2F03%2F26%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%8F%8A%E5%BA%94%E7%94%A8%2F</url>
    <content type="text"><![CDATA[ZJU 机器学习及应用课程 2018-03-06 课程介绍 &amp; Word Vector Representations课程介绍 赵洲 zhaozhou@zju.edu.cn 助教 陈哲乾 博三 zheqianchen@gmail.com Project 40% + Presentation 20% + 考试 40% 另一门课手把手教tenforslow数据集越完善 -&gt; 准确率越高，加分图像识别，还能识别出什么笔记本，有加分：iphone和华为手机；微软和苹果电脑 这门课focus on nlp NLP Levels Phonetic Analysis (Speech)/Tokenization (Text) Morphological analysis Syntactic analysis Semantic Interpretation Discourse Processing NLP Applications Spell checking, keyword search (拼写检查、关键词搜索) Extracting Information from websites (从网站提取关键信息) Product price, dates, locations, company names Classifying, reading level of texts, positive/negative sentiment of longer documents Machine translation (机器翻译) Spoken dialog systems (对话系统) Complex question answering (问答系统) NLP in Industry Search (written and spoken) Online advertisement Sentiment analysis for marketing Speech recognition Automating customer support Word Vector Representations 相关paper：Efficient Estimation of Word Representation in Vector Space 目前主流方法是把word看作原子符号 传统使用 one-hot representation，即使用含有1个1和若干个0的向量来表示一个word 存在的问题：近义词没办法表示，即判断不出来两个词语义相似程度；且随着word数量增加，vector维度增大，计算要求高 改正的idea：应该根据周围的词来表示当前词，因为如果某两个词经常出现在一起，就表明两者有一定联系（如地名和located） Word2Vec：对每一个word，预测它周围长度为m的窗口内的words Objective function: Maximize the log probability of any context word given the current center word wt是中心词，w(t+j)是长度为m的窗口内的词，T是整句话的长度Why use log? 用log就可以写成加法，更容易优化Why use softmax? 平滑，适于求导 每个词都有两个vector u &amp; v：v是作为center word的vector；u是作为surrouding word的vector 训练时：对于每个窗口，都要计算并更新该窗口下中心词的向量v以及邻近词的向量u word2vec最终能保证：Xapple - Xapples = Xcar - Xcars；Xking - Xman = Xqueen - Xwoman SGD：每次取一些sample，而不是所有一起求梯度；weight_grad = gradient(loss_fun, data, weights)，在这里传入的参数即为(J, corpus, theta) 一种skip-gram model with negative sampling Distributed Representations of Words and Phrases and their Compositionality 2018-03-13 Neural Networks and backpropagation s和sc怎么操作的，流程？ 为什么要对x求导 ————可能对x也需要调优 2018-03-20 Recurrent Neural Networkshttp://www.makaidong.com/%E5%8D%9A%E5%AE%A2%E5%9B%AD%E7%B2%BE/20150925/43179.html E需要训练吗？]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>ZJU课程</tag>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Fast Image Processing with Fully-Convolutional Networks]]></title>
    <url>%2F2018%2F03%2F16%2FFast-Image-Processing-with-Fully-Convolutional-Networks%2F</url>
    <content type="text"><![CDATA[CVPR 2017的一篇文章：Fast Image Processing with Fully-Convolutional Networks，作者陈启峰 提出一种使用FCN来替代原始图像处理的方法，可以对任意分辨率的图像进行操作 处理速度快、比原来的其他替代方法准确率更高、所有操作都基于同一个模型(只有具体的参数值不同，超参数也完全一致) 网上看到一篇不错的博文：链接 Abstract 提出一种基于全卷积网络(FCN)来加速图像处理操作的方法，训练完可以直接用这个网络替代原先的图像处理操作符(即不再需要原来的操作来实现对应的图像处理效果了) 该新方法可以在各种分辨率下对图像进行操作，且只需要恒定的时间 相比之前别的替代方法，该新方法准确率更高、速度更快 Introduction 目前的很多图像处理方法需要较大的计算资源、运行时间也比较长(如双边滤波)。基于此，有很多人针对于特定的某一种图像处理方法提出一些改进策略来加速，但这些方法不能泛化到各种不同图像处理方法 一种普适的加速方法是：先对图像进行下采样，在低分辨率下执行图像处理操作，随后再上采样 这么做有两个问题：1. 即使在低分辨率下，速度还是不够快；2. 在低分辨率下执行操作可能会影响图像处理的效果，即不够精准 本文的方法与之前的方法有所不同，能够直接在原始分辨率上执行图像处理操作，且通过端到端的训练可以最大化精确度，训练完也就可以直接取代原来的图像处理操作 对模型的判定依据有三条：效果的精确度Accuracy、运行时间Speed、模型的大小Compactness 对于文中的十种不同的图像处理方法，可以使用同一个模型去取代，即只有具体的参数值不一样，超参数都完全一致 Related Work 见原文 MethodPreliminaries Any network that has been used for a pixelwise classification problem such as semantic segmentation can instead be trained with a regression loss to produce continuous color rather than a discrete label per pixel. However, not all network architectures will yield high accuracy in this regime and most are not compact. (可以用回归，这样得到的像素点的RGB值是连续的，但精确度可能不够高，而且模型可能很大) High-level vision的架构(尤其是针对图像分割的)被应用到low-level的图像处理问题上，表现都非常好，主要原因可能在于High-level vision的架构的感受野更大，因为图像处理问题本身大多都是基于全局的优化 Context aggregation networks (CAN) 随着网络深度的增加，CAN的感受野呈指数增加（见下条的rs）。这保证了对高分辨率图像提取全局信息的同时，还能够保证参数不至于过多。CAD还具有运行时占内存空间小的优点：因为没有跳跃连接，所以只需要把两层保存在内存当中就可以了；而且由于每一层的大小都是一样的，只需要分配两块固定大小的buffer 模型细节：L0和Ld都是m*n*3，m和n即为输入图像的尺寸。中间层Ls是m*n*w。不同层之间的计算如下。其中，下标i表示第i个feature map，rs是表示卷积核的dilation，即空洞卷积操作，随网络深度的增加而指数增长。对于L(d-1)，不使用dilation；对于Ld，使用了一个线性的变换(1*1的卷积)，将最后一层映射到了RGB空间。Φ则是leaky rectified linear unit，即Φ(x) = max(αx, x)。Ψs 是adaptive normalization function Adaptive normalization BN对于一部分图像处理方法能提升效果，一部分效果反而更差了。为此，提出了一种Adaptive normalization，结合了BN和identity mapping 后续待更新]]></content>
      <categories>
        <category>计算机视觉</category>
      </categories>
      <tags>
        <tag>计算机视觉</tag>
        <tag>深度学习</tag>
        <tag>论文阅读</tag>
        <tag>图像处理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ZOJ1007 Numerical Summation of a Series]]></title>
    <url>%2F2018%2F03%2F10%2FZOJ1007-Numerical-Summation-of-a-Series%2F</url>
    <content type="text"><![CDATA[数值分析Lab1，也是ZOJ1007 链接 一道数学题，给的Hint非常强 题目难点有两个： 不知道要计算多少项才能满足精度 一旦多算就可能导致超时 好在题目给了一个Hint：构造一个序列：ϕ(x)−ϕ(1)，这样收敛快且roundoff loss小还给了一个求上界的不等式： 因此，对于这两个难点的解决办法如下： 对于收敛项数，可以假设我们需要n项才能满足精度，那么可以根据不等式求出第n项后面所有余项和的上界，保证该上界小于误差范围即可，从而可以求解出n的最小值 同时，ϕ(x)和ϕ(x-1)之间可以发现存在一个关系（ϕ(x) = ϕ(x-1) * (x - 1) / x + 1 / (x * x);，直接通过该关系可以避免很多重复的计算 那么为何要构造一个新序列呢？ 从收敛速度来看，ϕ(x)本身分母是2次方，按照上述方法求解出来的n比较大；而构造的这个新序列的分母有三次方，只需要更少的项就可以收敛，而且ϕ(1)为1，在求得新序列后较容易算得要求的ϕ(x)。网上还看到了一个更强的推导，分母有4次方，推导如下: 具体的roundoff loss会不会更小不好说，需要分析一下哪个计算次数更多了 代码：123456789101112131415161718192021222324252627282930313233343536373839404142#include &lt;stdio.h&gt;void Series_Sum(double sum[]);int main()&#123; int i; double x, sum[3001]; Series_Sum(sum); x = 0.0; for (i = 0; i&lt;3001; i++) printf("%6.2f %16.12f\n", x + (double)i * 0.10, sum[i]); system("pause"); return 0;&#125;void Series_Sum(double sum[])&#123; int i, k; double x; double ans = 0; for (i = 0; i &lt; 10; i++) &#123; x = i * 0.1; for (k = 1; k &lt; 100000; k++) &#123; ans += 1.0 / ((k + x) * (k + 2) * (k + 1) * k); &#125; //printf("%.6f\n", ans + 1); sum[i] = (1.0 - x) * ((2.0 - x) * ans + 0.25) + 1; ans = 0; &#125; sum[10] = 1; for (i = 11; i &lt;= 3000; i++) &#123; x = i * 0.1; sum[i] = sum[i - 10] * (x - 1) / x + 1 / (x * x); &#125;&#125;]]></content>
      <categories>
        <category>数值分析</category>
      </categories>
      <tags>
        <tag>数值分析</tag>
        <tag>刷题</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python常用函数]]></title>
    <url>%2F2018%2F03%2F09%2FPython%E5%B8%B8%E7%94%A8%E5%87%BD%E6%95%B0%2F</url>
    <content type="text"><![CDATA[记录了一些python的常用函数 列表的大小：len(L) 判断是否包含子串：’Acc’ in str[i] 字符串截取：str[s:t] 二维数组访问：a[1,1]；字符串列表访问：str[i][s:t] 找索引值：.index(‘loss’) 去除字符串内多余的空格：a.strip()]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Action Recognition Survey]]></title>
    <url>%2F2018%2F03%2F09%2FAction-Recognition-Survey%2F</url>
    <content type="text"><![CDATA[关于Action Recognition领域的综述，Going Deeper into Action Recognition: A Survey 链接 What is an action? Action is the most elementary human 1-surrounding interaction with a meaning. Taxonomy Representation based Solutions Earliest works in action recognition use 3D models to describe actions Constructing 3D models is difficult and expensive Holistic Representation Motion Energy Image (MEI) and Motion History Image (MHI). “Bobick and Davis, 2001” The MHI template shows how the motion image is moving. Each pixel in MHI is a function of the temporal history of the motion at that point (i.e., higher intensities correspond to more recent movements) The volumetric extension of MEI templates. （见下左图） “Blank, 2005” represent an action by a 3D shape induced from its silhouettes in the space-time Space-Time Volume (STV) “Yilmaz and Shah, 2005” An STV is build by stacking the object contours along the time axis Holistic representations flooded the research in action recognition roughly between 1997 to 2007. However, nowadays local and deep representations are favored Local Representation interest point detection -&gt; local descriptor extraction -&gt; aggregation of local descriptors (从找到一个感兴趣点，到从兴趣点周围的一片区域提取一个描述子，再到把一堆局部描述子聚合在一起) Interest point detection 3D-Harris Detector “Laptev, 2005” The 3D-Harris detector identifies points with large spatial variations and non-constant motions 3D-Hessian Detector “Willems, 2008” In certain domains, e.g., facial expressions, true spatiotemporal corners are quite rare, even if an interesting motion is occurring disintegrate spatial filtering from the temporal one action clips are more likely to be obtained in uncontrolled environments a shaky camera can fire a series of irrelevant interest pointsprune irrelevant features using statistical properties of the detected interest points spatiotemporal features obtained from background, known as static features, especially the ones that are near motion regions are useful for action recognition Local Desscriptors To obtain the local descriptor at an interest point, earlier works almost unanimously opt for cuboids. Later people introduced the notion of trajectories Edge and Motion Descriptors Histogram of Gradient Orientations (HOG3D) “Klaser, 2008”Optical Flow Fields: Histogram of Optical Flow (HoF) “Laptev, 2008” &amp; Motion Boundary Histogram (MBH) “Dalal, 2006” Pixel Pattern Descriptors Volume Local Binary Patterns (VLBP) “Zhao, 2007” &amp; Local Binary Pattern histograms from Three Orthogonal Planes (LBP-TOP) “Kellokkumpu, 2008”To describe a region R in an image, first use low-level features or mid-level features to extract a set of features zi, then use d*d covariance matrix of zi (RCD) as the descriptor for region R “Tuzel, 2006” From Cuboids to Trajectories An spatiotemporal interest point might not reside at the exact same spatial location within the temporal extends of a cuboid.A trajectory is a properly tracked feature over time Use dense interest points instead of sparse AggregationDeep Architectures for Action Recognition Spatiotemporal networks (空间+时间；3D-CNN) Multiple stream networks (拆分为空间流和时间流) Deep generative networks (无监督) Temporal coherency networks (作为预训练网络) Spatiotemporal Networks (From 3D-CNN to 3D-CNN+LSTM) Arm the CNN with temporal information —— 3D CNN. Use 3D kernels(filter extended along the time axis) to extract features from both spatial and temporal dimension. “3D Convolutional Neural Networks for Human Action Recognition“ 3D CNN have a very rigid temporal structure. The network accepts a predefined number of frames as the input.在空间上可以通过pooling来解决fixed spatial dimension，但是时间上为什么可以还没有解释；同时对于不同动作，输入的帧数也不明确 How temporal information should be fed into CNN ? (Different Fusion Schemes) Max-pooling更好. “Ng, 2015” Slow Fusion: CNN accepts several consecutive parts of the video, and processes them through the very same set of layers to produce responses across temporal domain. These responses are then processed by fully connected layers to produce the video descriptor. “Karpathy, 2014” Early Fusion: the network is fed with a set of adjacent frames. “3D Convolutional Neural Networks for Human Action Recognition“ Late Fusion: frame-wise features are fused at the last layer. “Karpathy, 2014” Multi-resolutional approach using two separate networks not only boosts the accuracy, but also reduces the number of parameterss to be learned. (见上右图) “Karpathy, 2014” Find generic video descriptors based on a 3D convolutional network (C3D). “Tran 2015” A network with 3 × 3 × 3 homogeneous filters (constant depth at every layer) performs better than varying the temporal depth on filters. Flexibility on the temporal extent is obtained with the inclusion of 3D pooling layers. Obtained by averaging the outputs of the first fully connected layer. Improvements are observed by extending the temporal depth of the input as well as combining the decision of networks with different temporal awareness at the input. “Varol, 2016” Factorizing a 3D filter into a combination of a 2D and 1D filters, solving the problem of increasing the number of parameters of the 3D-CNN network. Feed an LSTM network with features extracted from a 3D convolutional network. The two networks are trained separately. “Baccouche, 2011” Another end-to-end architecture based on LSTM named LRCN. (下图a) “Donahue, 2015” Group is a set of convolutional filters operating only on a particular set of feature maps from the previous layer. Multiple Stream Networks A class of deep neural networks is devised to separate appearance based information from motion related ones for action recognition. “Simonyan, 2014” Multiple-stream deep CNN, with two parallel networks Spatial stream network accepts raw video frames while the temporal stream network gets optical flow fields as input Fine-tuning a pretrained network on the ILSVRC-2012 image dataset (Russakovsky et al., 2015) leads to higher accuracy Stacking optical flow fields at the input of the temporal stream network (i.e., early fusion) is beneficial The temporal stream network is modified to have more than one classification layer. Each classification layer operates on a specific dataset, aiming to learn a representation, which is not only applicable to the task in question, but also to other tasks. The two streams are fused together using the softmax scores Fusion at an intermediate layer not only improves the performance but also reduces the number of parameters significantly “Feichtenhofer, 2016” having the fusion right after the convolutional layers will remove the requirement of costly fully connected layers in both streams Extension of the two stream network Dense trajectories traced over convolutional feature maps of the two-stream network are aggregated using the Fisher vector “Wang, 2015” A third stream using audio signal is added to the network “Wu, 2015” The optical flow frames are the only motion related information used in two stream networks “Feichtenhofer, 2016” Optical flow cannot capture subtle but long-term motion dynamics Certain details in actions are still out-of-reach in deep solutions Deep Generative Models Dynencoder: a class of deep auto-encoders to capture video dynamics Has three layers: xt -&gt; ht -&gt; ht+1 -&gt; xt+1 To reduce the training complexity, the parameters of the network are learned in two stages. In the pretraining stage, each layer is trained separately. Once pretraining is completed, an end-to-end fine tuning is performed The reconstruction error of a video given a Dynencoder can be used as a mean for classification LSTM Autoencoder Model Consist of the encoder LSTM and the decoder LSTM The states of the encoder LSTM contain the appearance and dynamics of the sequence Has reconstructive decoder and predictive decoder Adversarial Models The discriminative model learns to determine whether a sample is coming from the generative model or the data itself. Temporal Coherency Networks Temporal Coherency is a form of weak supervision Temporal coherency states that consecutive video frames are correlated both semantically and dynamically (i.e., abrupt motions are less likely) Temporal coherency is not always a strong assumption to rely on Siamese Network (Chopra, 2005; Varior, 2016; Lu, 2016) is trained with tuples to determine whether a given sequence is coherent or not. Give more attention to human posesAvoid ambiguities between positive and negative tuplesCompared to networks trained from scratch, pretrained networks based on the temporal coherency have potential to improve the accuracy Action is split into two phases for classification, the precondition set Xp and the effect set Xe. An action is identified by the transformation required to map a high-level descriptor extracted from Xp to a high-level descriptor extracted from Xe. Rank pooling (Fernando, 2015) is an effective solution for capturing temporal evolutionof a sequence. Datasets No universal solution for all datasets the KTH and the Weizmann datasets contain human actions in controlled conditions, and their scope is limited to basic actions such as walking, running and jumping. comparing solutions on the KTH and Weizmann datasets is less insightful unless a specific need is considered. HMDB-51 and UCF-101 datasets contain camera motion (and shakes), viewpoint variations and resolution inconsistencies. these datasets are not well-suited for measuring the performance of action localization Hollywood2 and Sports-1M datasets contain view-point/editing complexities (sudden viewpoint variations in the video streams) methods that rely on temporal coherency may fail on Sports-1M Algorithms benefiting from object details are expected to perform better For Deep Learning, Sports-1M dataset is great while KTH and Wiezmann often leads to unsatisfactory performance Performance the state-of-the-art solutions based on both representation and deep learning perform equally well One reason is the insufficiency of dataA dominant theme to get around this limitation is to benefit from models pre-trained on images Current State-of-the-art solutionsHandcrafted Solutions Dense trajectory descriptors, incorporated in various pooling strategies such as FV’s and Rank-Pooling Deep-Net Solutions the spatiotemporal networks and two-stream networks outperform other network structures Equipped with 3D convolution filtersTraining deeper networks demands more rigorous data augmentation techniques Fusion with Dense Trajectories always help The structures learned by deep networks are complementary to the handcrafted trajectory descriptors What algorithmic changes to expect in the future? Moving towards deep architectures for action recognition is dominating the action recognition research lately Training deep networks difficult on video data -&gt; Knowledge Transfer (Use models trained on images or other sources) Blend 3D convolutions, temporal pooling, optical flow frames, and LSTMs to boost the performance Use data augmentation techniques, foveated architecture and distinct frame sampling strategies Most State-of-the-art ProgressQuo Vadis, Action Recognition? A New Model and the Kinetics Dataset (CVPR 2017) Propose a new dataset named “Kinetics Human Action Video dataset”, with 400 human action classes and over 400 clips per class, and is collected from realistic, challenging YouTube videos Introduce a new Two-Stream Inflated 3D ConvNet (I3D), reaching 80.2% on HMDB-51 and 97.9% on UCF-101 after pre-training on Kinetics]]></content>
      <categories>
        <category>计算机视觉</category>
      </categories>
      <tags>
        <tag>计算机视觉</tag>
        <tag>Action Recognition</tag>
        <tag>深度学习</tag>
        <tag>论文阅读</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Qua Vadis, Action Recognition? A New Model and the Kinetics Dataset]]></title>
    <url>%2F2018%2F03%2F07%2FQua-Vadis-Action-Recognition-A-New-Model-and-the-Kinetics-Dataset%2F</url>
    <content type="text"><![CDATA[CVPR 2017的一篇文章: Qua Vadis, Action Recognition? A New Model and the Kinetics Dataset 在一个规模更大的新video数据集Kinetics上，重新评估了当下state-of-the-art的模型结构，并和在小数据集上训练的结构进行比较 提出一个新模型I3D，在Kinetics上预训练后，在HMDB-51数据集上取得了80.2%的准确率，在UCF-101上取得了97.9%的准确率 本文的idea*： 根据之前3D-ConvNets的缺点(1. 参数多；2. 无法利用在ImageNet上预训练过的2D网络)，提出一种benefit from ImageNet 2D ConvNet design and their learned parameters的方法，并探究了在时间维度上的感受野要如何设置 吸收了之前state-of-the-art的模型，把双流的思想加到3D-ConvNet当中来，取得优异的效果（作者认为既然现在有了大数据集，那么3D ConvNet原先因为参数多而难训练的缺陷可以被大幅度改进。因此作者使用了3D ConvNet，同时双流这个思想还是很有用，因此这个新模型使用双流+3D ConvNet） 验证了视频模型的迁移学习同样有效，即在Kinetics上进行预训练能够提升模型效果 Abstract 在一个规模更大的新video数据集Kinetics Human Action Video dataset(包含大量数据，有400个动作分类，每个分类有超过400个实例，来源于YouTube，更有挑战性)上，重新评估了当下state-of-the-art的模型结构，并在该数据集上预训练后查看在小数据集上训练的结果提升 提出一个新的模型Two-Stream Inflated 3D ConvNet (I3D)。该模型在Kinetics上预训练后，在HMDB-51数据集上取得了80.2%的准确率，在UCF-101上取得了97.9%的准确率 Introduction 有大量成功的事实证明：在ImageNet上训练过的架构可以有效地用在其他地方（即迁移学习）。但在视频领域，不知道是否在一个量较大的数据集上进行训练也可以有助于提升性能 本篇论文就是基于这个目的，重新实现了各种代表性的模型，并发现在大数据集上进行预训练确实能够提升性能，但提升幅度取决于具体模型类型 基于上述发现，提出一个新模型I3D，能够充分发挥预训练的效果。它是基于当下最先进的图像分类模型，但把其中的卷积核以及pooling核都扩张成3D的形式 一个基于Inception v1的I3D模型效果远超之前最好的方法 本文没有把传统方法一起纳入比较，如bag-of-visual-words Action Classification Architectures 目前针对于video的模型架构还不明确，主要集中于以下几个问题 卷积核是2D还是3D输入网络的是原始RGB视频还是预计算得到的光流对于2D的ConvNets来说，不同帧之间的信息是使用LSTM还是feature aggregation 比较的范围有三类对象 2D ConvNets with LSTM on topTwo-stream networks3D ConvNets 下列是本文重新实现的5种代表性模型结构 之前由于缺乏数据，所使用的3D ConvNets模型都比较浅，最多只有8层。本文使用了如VGG、Inception等非常深的网络，并导入这些预训练网络的参数（除了C3D，因为没法导入，卷积核之类的都少一维），将这些网络扩展成时空特征描述子。同时，作者发现，这样的情况下，双流(two-stream)依然有用 本文使用的CNN结构是Inception v1加上BN ConvNet + LSTM 由于图像分类网络效果特别好，因此人们总想尽可能原封不动地把模型应用到视频上，如Karpathy做的那篇early/late/slow fusion的文章，但这样就会导致模型忽视了时序结构（如不能分辨开门与关门） 因此，可以在模型后面加上LSTM来处理时序问题 模型细节：把LSTM和BN加在Inception v1的最后一个average pooling层后面（即分类器之前），有512个节点。在模型最顶部加一个全连接层用于分类；The model is trained using cross-entropy losses on the outputs at all time steps. During testing we consider only the output on the last frame；输入帧是在25帧/s的视频流中每5帧取1帧，根据表1给出的信息，作者应该是从数据集的video中选取了5s的视频片段，所以总共是5s * 25帧/s * 1/5 = 25张rgb图像 3D ConvNet 3D ConvNet看上去是一种很自然的方法，能够直接对于时空数据创建高层表征 但这个模型有两个问题： 相比于2D，参数更多，也就更难训练（因为数据量不足），所以之前3D ConvNet用的都是浅层的架构由于都是三维的核，无法直接用在ImageNet上预训练过的网络，因此只能在video数据集上train from scratch。由于之前的数据集量都太小，因此效果不是太有竞争力。但这种方法可能会比较适用于大数据集 模型细节：是原论文中C3D的变种。8层卷积、5层pooling、2层全连接。与C3D的区别在于这里的卷积和全连接层后面加BN；且在第一个pooling层使用stride=2，这样使得batch_size可以更大。输入是16帧，每帧112*112。 Two-Stream Networks LSTM缺点：能model高层变化却不能捕捉低层运动(因为在低层，每个帧都是独立地被CNN提取特征)，有些低层运动可能是重要的；训练很昂贵 Two-Stream Networks: 将单独的一张RGB图片和一叠计算得到的光流帧分别送入在ImageNet上预训练的ConvNet中，再把两个通道的score取平均 这种方法在现在的数据集上效果很好训练和测试都十分经济 一个改进(Fused Two-Stream): 在最后一层卷积层之后，使用3D ConvNet把空间流和时间流融合（相比于传统双流是在softmax后才做fusion，把softmax输出的score进行平均） 在HMDB数据集上提升了效果，测试时间也更短 模型细节：输入是每隔10帧取连续的5帧以及相应的光流。在Inception v1之后，是一个3*3*3的3D卷积层，输出是512个channel，随后是一个3*3*3的3D max-pooling层以及全连接层。这个新的网络是用高斯随机初始化 对于双流网络有两种实现，一种实现是训练时把两个流分开训练，测试的时候在最后把两个流的预测结果做平均；第二种是直接端到端进行训练。在c)和d)的实现中使用的是端到端；而在e)的实现中使用了第一种实现 New*: Two-Stream Inflated 3D ConvNets 结论：3D ConvNets可以受益于在ImageNet上训练过的2D ConvNet模型，并有选择性的使用相应的预训练参数；虽然3D ConvNets可以直接从RGB流中学习到时序信息，但是使用光流还是可以提升效率 Inflating 2D ConvNets into 3D 把一些很成功的2D模型转移成3D，通过把所有卷积核以及pooling核增加时间的一维 Bootstrapping 3D filters from 2D filters 想要利用在ImageNet上预训练好的2D模型的参数： Idea*: 若是把ImageNet中的同一张图片反复复制生成一个序列，那么这个序列就可以当作是一个video来训练3D模型了 具体实现：把2D模型中的核参数在时间维上不断复制，形成3D核的参数，同时除以N，保证输出和2D上一样；别的非线性层结构都与原来的2D模型一样 Pacing receptive field growth in space, time and network depth (在时间维度上的感受野要如何变化，即conv和pooling的stride怎么选) 在Image模型中，对待水平和垂直两个空间维度往往是一致的，也就是两个维度上pooling核大小以及stride都一样 在时间维度上这样的对称对待未必是最优的(也就是时间维度上的pooling核大小选与空间上的一致是不可取的)，因为这取决于帧率和图像大小之间的相对值 具体实现：在Inception v1中，涉及到感受野变化的就是第一个卷积核(stride=2)以及后续4个max-pooling(stride=2)，还有最后的一个7*7的average-pooling层。在本文的实验中，作者发现：在前两个max-pooling层上，时间维度上的stride取1；而在别的max-pooling层上使用对称的stride(即时间维度上的stride和空间上的一致)；最后的average pooling使用2*7*7的核 Two 3D Streams 作者发现双流还是有价值的，可能因为3D ConvNet只有纯前馈计算，而光流提供了迭代的思想在里面 训练时，分别训练这两个网络，测试的时候在最后把两个流的预测结果做平均 Implementation Details 见原文 The Kinetics Human Action Video Dataset 数据集细节参见另一篇提出这个数据集的文章 The Kinetics Human Action Video Dataset Experimental Comparison of Architectures (模型本身的比较) 在UCF-101和HMDB-51上，I3D取得的效果也要好于之前的。这十分有趣，因为UCF-101和HMDB-51数据集本身比较小，而I3D模型参数非常多，按道理来说训练效果应该不会太好。这说明在ImageNet上预训练的效果可以扩展到3D 在UCF-101上效果最好，miniKinetics次之，HMDB-51最差，这和数据本身的难度、数据量大小都有关系 LSTM和3D-ConvNet在miniKinetics上表现的更有竞争力，这是因为这两种方法对数据量的需求比较大 在miniKinetics上，光流要比RGB流效果差，而在其他两个数据集上则相反，这是由于miniKinetics数据集本身有许多相机抖动 相比于其他模型，I3D模型似乎从光流中获益最大，这有可能是因为I3D的时间长度更大(有64帧) Experimental Evaluation of Features (用在迁移学习中，即把各种模型先在Kinetics数据集上做预训练，预训练得到的网络作为特征提取器，再去别的数据集上训练与测试) 固定预训练的参数，在新数据集上只重新训练一个分类器 整个网络在新数据集上fine-tuning 固定预训练的参数，在新数据集上只重新训练一个分类器的做法在I3D和3D ConvNet上也有不少提升，但在其他几种方法上几乎没有变化整个网络在新数据集上fine-tuning的结果都得到大幅提升，尤其是I3D和3D ConvNet迁移学习对于I3D影响最大，这也可能是因为I3D的时间长度更大，在大数据集上预训练就可以学到更好的时间结构而迁移学习对于没有使用3D ConvNet结构的影响不大，这可能是因为那些方法的输入都是离散的帧而不是连续的帧，而那些2D方法都用了ImageNet预训练的参数，这些独立的帧其实更像是image而不是video，所以已经在image的数据集上预训练过的结构在video的大数据上进一步pre-train不一定有明显提升]]></content>
      <categories>
        <category>计算机视觉</category>
      </categories>
      <tags>
        <tag>计算机视觉</tag>
        <tag>Action Recognition</tag>
        <tag>深度学习</tag>
        <tag>论文阅读</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数值分析]]></title>
    <url>%2F2018%2F03%2F05%2F%E6%95%B0%E5%80%BC%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[ZJU 数值分析课程笔记 2018-03-05课程介绍黄劲hj@cad.zju.edu.cn蒙民伟楼 431 30分Lab Pro + 18分Discussion + 7分Research Topic + 5分Q&amp;A + 40分考试 考过高斯积分，尽可能提高精度 discussion：2个人一组做报告，3-5分钟 research topic：按组完成，有18个topic，5-10分钟展示 数值分析（Numerical Analysis）的作用是什么？ 硬件只做最简单的+/-/*等，剩下的复杂计算都依赖于软件实现（尽管现在有部分简单的计算被加进了CPU） 计算机里面的数学几乎全是近似，如0.7都无法在计算机里面精确表示 这门课将要介绍什么 介绍经典的近似技术/逼近技术（approximation technique） 解释为何这些技术可以有效地工作 是将来从事科学计算的基础 Chapter 1 Mathematical Preliminaries1.2 Roundoff Errors and Computer Arithmetic误差来源： Truncation Error：对于无穷项/无穷小数等，必须进行截断，即舍去后面的项（如泰勒展开） Roundoff Error：计算机本身进行实数运算时由于不精确产生的误差（因为计算机在运算时只能保留有限位数） Roundoff Error何时会影响结果的精确性？ 做除法时，如果除数非常小，那么被除数的一个小误差，在商当中会产生巨大的误差，ex/y很大(因为y很小) 如果一个值非常大，那么一定涉及了除法，如tan、log 结论*：必须先把式子简化后再输入计算机，这样可以减少计算机在运算过程中产生的Roundoff的误差（如秦九韶算法） 对于一个无限小数，有两种近似的处理： Chopping（直接截断） Rounding（四舍五入） 误差分类： Absolute Error: |p-p*| Relative Error: |p-p*|/|p| 或 f~(x+e) - f(x) / f(x) 有效数字是一个相对误差Rounding的每一步相对误差都较小（对于单次，四舍五入肯定比直接舍去要更精确），但是对于整个运算来说不一定 一个定义： 1.3 Algorithms and Convergence定义： stable：输入有一个小的扰动，输出的扰动也很小 unstable：输入有一个小的扰动，输出的扰动很大 conditionally stable：仅当输入的数据是某几个特定选择时，是stable的 定义： E0 表示初始误差，En 表示第 n 次操作后的误差。 若 En == C*n*E0 (C是常数)，那么error的增长是Linear的 若 En == C^n*E0 ，那么error的增长是Exponential的 Linear的误差是不可避免的，但要尽可能避免Exponential的误差 Example： Method 1： |En| = |In - In*| = |(1-n*In-1) - (1-n*In-1*)| = n*|En-1| = … = n!*|E0|随着n的增大，误差的增长是Exponential的，若去计算（每一次保留若干位）就会发现，明明In是递减的，其算到n比较大时，会增大，甚至在正负之间跳动 Method 2： 区别于上面由In-1推到In，这里先定In，再推回In-1：由1/(e*(n+1)) &lt; In &lt; 1/(N+1)，取In’ = [1/(e*(n+1)) + 1/(n+1)]/2|En| = |In - In’| -&gt; 0，当 n -&gt; ∞；|Ej| = |En|/(n*(n-1)*(n-2)…(j+1))因为该方法的Error是有界的，就是区间长度，此时向前算，误差就会越来越小，所以就不会出现Method 1的大误差。实际计算中，该方法算出来的I0、I1也都是很准确的 2018-03-12Chapter 2 Solutions of Equations in One Variable2.1 The Bisection Method中值定理 若f属于C[a, b]，K是介于f(a)和f(b)之间的一个数(当然f是连续函数)，那么在(a, b)上必定存在一个数p，使得f(p) = K 二分法 若f属于C[a, b]，且f(a) * f(b) &lt; 0，那么由二分法可以产生一个逼近f零点p的序列，并保证： 二分的条件：单调、有界 二分时，使用 p = a + (b - a) / 2 而不是 p = (a + b) / 2 的原因是保证精度；使用sign(FA)*sign(FP)来判断是否大于0是因为担心使用FA*FP判断时会溢出 二分法的优点： 很容易，只需要f单调、有界；而且总能收敛到某个解 二分法的缺点： 收敛慢；可能在收敛过程中会丢掉某一个好的中间近似解；若有多个解，只能收敛到某一个solution；————可以事先把区间划成多段[ak, bk]，保证f(ak) * f(bk) &lt; 0二分法不能用来找复数根，因为complex number无序，不可排序 2.2 Fixed-Point Iteration 思想：把求根问题转化成求不动点(若g(p) = p，则p被定义成g的一个不动点)的问题：把f(x)转化成等价的x = g(x) 一个f(x)可以对应一堆g(x)g(x)目标：能收敛，且收敛到f(x)的root (如g(x)=x^2，初值选了&gt;1，那么永远不可能收敛) 求解方法：从一个初始的p0开始，随后通过pn = g(pn-1)生成一个序列，若这个序列收敛并且g是连续函数，则最终收敛得到的p=g(p)即为所求不动点 (因为pn = g(pn-1) = g(pn)，当n趋向于无限大） 不动点定理 存在性：g属于C[a, b]且g(x)属于[a, b]，对于一切x属于[a, b]成立，则g在[a, b]有一个不动点唯一性：在上述存在的条件下，若g’(x)在(a, b)存在，且存在一个正常数k &lt; 1，使得|g’(x)| &lt;= k，那么不动点在[a, b]上唯一在不动点存在且唯一的条件下，使用上述求解方法可以收敛于[a, b]内的唯一不动点p 若g满足不动点定理的条件，则用pn逼近p的误差界如下： 因此，根据这个误差界，我们可以在计算前估计大概要算多久。且我们发现k越小收敛越快 构造g(x)时，要保证g’(x) &lt;= k &lt; 1，且k越小越好 2.3 Newton-Raphson Method有条件收敛 2018-03-192.4 Error Analysis for Iterative Methods 定义： a越大，收敛越快，因此人们都试图找到产生高阶收敛序列的方法 2.5 Accelerating ConvergenceAitken’s 2 Method的假设：线性收敛，所以(Pn+1 - P)/(Pn - P) = lenda = (Pn - P)/(Pn-1 - P)，所以P可由三个P猜测，n越大lenda越小，所以猜测的P越准 2018-03-26Chapter 6 Direct Methods for Solving Linear Systems6.1 Linear Systems of Equations 向后代换的高斯消元法 先把A变成上三角矩阵，随后用向后代换的方式解未知数akk被称为主元，若akk=0，则要把第k行和第i行做interchange(其中，aki!=0)运算次数： 6.2 Pivoting Strategies 若某个主元非常小，会引起较大的误差]]></content>
      <categories>
        <category>数值分析</category>
      </categories>
      <tags>
        <tag>数值分析</tag>
        <tag>ZJU课程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[资源汇总]]></title>
    <url>%2F2018%2F02%2F17%2F%E8%B5%84%E6%BA%90%E6%B1%87%E6%80%BB%2F</url>
    <content type="text"><![CDATA[记录了各个地方看到的资源整合，供日后查阅 Zero to Hero：2017年机器之心AI高分概述文章全集 链接入门 深度 | David Silver 全面解读深度强化学习：从基础概念到 AlphaGo 深度 | 迁移学习全面概述：从基本概念到相关研究 深度 | 理解深度学习中的卷积 综述 | 知识图谱研究进展 盘点 | 机器学习入门算法：从线性模型到神经网络 深度神经网络全面概述：从基本概念到实际模型和硬件基础 机器理解大数据的秘密：聚类算法深度详解 想了解概率图模型？你要先理解图论的基本定义与形式 人工智能能骗过人类吗？愚人节特写：这不是玩笑 三张图读懂机器学习：基本概念、五大流派与九种常见算法 LSTM 入门必读：从基础知识到工作方式详解 从语言学到深度学习 NLP，一文概述自然语言处理 最全的 DNN 概述论文：详解前馈、卷积和循环神经网络技术 从贝叶斯定理到概率分布：综述概率论基本定义 追根溯源！一图看尽深度学习架构谱系 徒手实现 CNN：综述论文详解卷积网络的数学本质 读懂概率图模型：你需要从基本概念和参数估计开始 从零开始：教你如何训练神经网络 开发者必读：计算机科学中的线性代数 理论机器学习 学界 | 定量研究：当前机器学习领域十大研究主题 机器学习和深度学习引用量最高的 20 篇论文（2014-2017） 从贝叶斯角度，看深度学习的属性和改进方法 良心 GitHub 项目：各种机器学习任务的顶级结果（论文）汇总 深度 | 从朴素贝叶斯到维特比算法：详解隐马尔科夫模型 揭秘深度学习成功的数学原因：从全局最优性到学习表征不变性 深度 | 从 AlexNet 到残差网络，理解卷积神经网络的不同架构 从 Pix2Code 到 CycleGAN：2017 年深度学习重大研究进展全解读 前沿方向 OpenAI 详解进化策略方法：可替代强化学习 从自编码器到生成对抗网络：一文纵览无监督学习研究现状 资源 | 从文本到视觉：各领域最前沿的论文集合 从决策树到随机森林：树型算法的原理与实现 从概率论到多分类问题：综述贝叶斯统计分类 从遗传算法到 OpenAI 新方向：进化策略工作机制全解 GAN 综述 | 一文帮你发现各种出色的 GAN 变体 资源 | 生成对抗网络及其变体的论文汇总 生成对抗网络综述：从架构到训练技巧，看这篇论文就够了 计算机视觉 干货 | 物体检测算法全概述：从传统检测方法到深度神经网络框架 重磅 | 自动驾驶计算机视觉研究综述：难题、数据集与前沿成果（附 67 页论文下载） 神经风格迁移研究概述：从当前研究到未来方向（附论文和代码） 深度学习目标检测模型全面综述：Faster R-CNN、R-FCN 和 SSD 计算机视觉这一年：这是最全的一份 CV 技术报告 计算机视觉这一年：2017 CV 技术报告 Plus 之卷积架构、数据集与新趋势 深度 | 2017 CV 技术报告之图像分割、超分辨率和动作识别 深度 | 2017CV 技术报告：从 3D 物体重建到人体姿态估计 自然语言处理 语音合成到了跳变点？深度神经网络变革 TTS 最新研究汇总 资源 | 从全连接层到大型卷积核：深度学习语义分割全指南 学界 | 词嵌入 2017 年进展全面梳理：趋势和未来方向 深度 | 一文概述 2017 年深度学习 NLP 重大进展与趋势 推荐系统 学界 | 一文综述所有用于推荐系统的深度学习方法 使用深度学习构建先进推荐系统：近期 33 篇重要研究概述 深度学习框架 业界｜谷歌 TensorFlow 的一份全面评估报告：好的坏的及令人讨厌的 初学者怎么选择神经网络环境？对比 MATLAB、Torch 和 TensorFlow 硬件 业界 | 剖析用于深度学习的硬件：GPU、FPGA、ASIC 和 DSP 神经形态计算与神经网络硬件最全调查：从研究全貌到未来前景 从 GPU、TPU 到 FPGA 及其它：一文读懂神经网络硬件平台战局 优化 从浅层模型到深度模型：概览机器学习优化算法 综述论文：当前深度神经网络模型压缩和加速方法速览 深度 | 从修正 Adam 到理解泛化：概览 2017 年深度学习优化算法的最新研究进展 一文概览深度学习中的五大正则化方法和七大优化策略 代码实现 从强化学习基本概念到 Q 学习的实现，打造自己的迷宫智能体 回归、分类与聚类：三大方向剖解机器学习算法的优缺点（附 Python 和 R 实现） 基于 TensorFlow 理解三大降维技术：PCA、t-SNE 和自编码器 一文读懂遗传算法工作原理（附 Python 实现） 10 大深度学习架构：计算机视觉优秀从业者必备（附代码实现） 从算法到训练，综述强化学习实现技巧与调试经验 2017 年度盘点：15 个最流行的 GitHub 机器学习项目 人工智能从入门到进阶，2016年机器之心高分技术文章全集 链接学习资源 深度学习资料大全：从基础到各种网络 30个深度学习库：按Python和C++等10种语言分类 2016年不可错过的21个深度学习视频、教程和课程 程序员实用深度学习免费课程:从入门到实践 9本不容错过的深度学习和神经网络书籍 深度学习专业名词表：从激活函数到word2vec 2016年年度十大Python库盘点 2010-2016年被引用次数最多的深度学习论文 自学数据科学&amp;机器学习，19个数学和统计学公开课推荐 Yoshua Bengio新书《Deep Learning》中文版开放预览（附PDF下载链接） 数据科学家应该掌握的12种机器学习算法（附信息图） 从入门到研究，人工智能领域最值得一读的20份资料 关于数据科学的十本好书 哈佛大学九大自然语言处理开源项目 ICML 2016演讲视频：数百个演讲带你读懂机器学习 Yoshua Bengio研究生科研指导演讲：解读人工智能全貌和下一个前沿 Yann LeCun演讲：人工智能的下一个前沿——无监督学习 今年GitHub排名前20的Python机器学习开源项目 机器学习工程师和数据科学家最应该读的16本书 任何阶段的学习者都适用的参考：机器学习领域书目全集 Yoshua Bengio深度学习暑期班学习总结，35个授课视频全部开放 Andrej Karpathy CS294课程总结：可视化和理解深度神经网络 Geoffrey Hinton最新演讲梳理：从人工神经网络到RNN应用 吴恩达NIPS 2016演讲现场直击：如何使用深度学习开发人工智能应用 NIPS 2016最全盘点：主题详解、前沿论文及下载资源 斯坦福大学周末学习盛宴：12位大牛解读深度学习 提升深度学习模型的表现，你需要这20个技巧 TensorFlow开源一周年：这可能是一份最完整的盘点 五大主流深度学习框架比较分析：MXNET是最好选择 基础介绍文章 2016伦敦深度学习峰会观感：人工智能面临的三大难题 2016机器学习与自然语言处理学术全景图：卡耐基梅隆大学排名第一 2016年美国机器人路线图出炉，最新机器人产业盘点 一张图看懂全球Bot布局 Yoshua Bengio：深度学习崛起带来人工智能的春天 神经网络架构演进史：全面回顾从LeNet5到ENet十余种架构 Andrej Karpathy：计算机科学博士的生存指南 贝叶斯神经网络简史 百度首席科学家吴恩达刊文：人工智能的能力和不足 高盛百页人工智能生态报告：美国仍是主导力量，中国正高速成长 伯克利教授Stuart Russell：人工智能基础概念与34个误区 初学者必读：解读14个深度学习关键词 智能时代每个人都应该了解：什么是深度学习？ CMU机器学习系负责人：人工智能与人类的未来是共生自主 CMU教授邢波：人工智能的路径、方向与未来 从供应链优化到差异化定价：机器学习十种方式变革制造业 对比深度学习十大框架：TensorFlow最流行但并不是最好 当AI遇上AR ——从微软HPU说起 洞悉AlphaGo超越围棋大师的力量：机器之心邀你一起强化学习 东南大学漆桂林教授：知识图谱不仅是一项技术，更是一项工程 人工智能全局概览：通用智能的当前困境和未来可能 访谈百度IDL林元庆：百度大脑如何在人脸识别上战胜人类「最强大脑」 TensorFlow 生态系统：与多种开源框架的融合 Kaggle创始人问答：深度学习会淘汰其他的机器学习方法吗？ 机器之心年度盘点 | 从技术角度，回顾2016年语音识别的发展 机器学习的基本局限性：从一个数学脑筋急转弯说起 人们都在说人工智能，其实现在我们真正做的是智能增强 人工智能、机器学习、深度学习，三者之间的同心圆关系 R vs Python：R是现在最好的数据科学语言吗？ 斯坦福NLP团队介绍交互式语言学习：从语言游戏到日程规划 斯坦福大学副教授Reza Zadeh：神经网络越深就越难优化 神经网络和深度学习简史（三）：强化学习与递归神经网络 神经网络和深度学习简史（四）：深度学习终迎伟大复兴 深度学习技术在股票交易上的应用研究调查 深度学习十大飙升趋势 深度学习入门，以及它在物联网和智慧城市中的角色 服务器端人工智能，FPGA和GPU到底谁更强？ Science「机器人子刊」创刊号，五大研究解读机器人领域最新进展 西红柿还是猕猴桃？一个案例帮你入门机器学习 详细解读神经网络十大误解，再也不会弄错它的工作原理 基于图的机器学习技术：谷歌众多产品和服务背后的智能 技术起点 10种深度学习算法的TensorFlow实现 数十种TensorFlow实现案例汇集：代码+笔记 机器学习入门必备：如何用Python从头实现感知器算法 ACM 最新月刊文章：强化学习的复兴 人工智能开发者的入门指南 从入门到精通：卷积神经网络初学者指南 机器学习敲门砖：任何人都能看懂的TensorFlow介绍 DeepMind提出的可微神经计算机架构的TensorFlow实现 RNN 怎么用？给初学者的小教程 深度学习教程：从感知器到深层网络 OpenAI 的 PixelCNN++实现：基于 Python的实现 Geoffrey Hinton最新演讲梳理：从人工神经网络到RNN应用 官方指南：如何通过玩TensorFlow来理解神经网络 一篇文章带你进入无监督学习:从基本概念到四种实现模型 NIPS 2016上22篇论文的实现汇集 机器学习初学者入门实践：怎样轻松创造高精度分类网络 解决真实世界问题：如何在不平衡类上使用机器学习？ 卷积神经网络架构详解：它与神经网络有何不同？ LSTM和递归网络基础教程 门外汉如何使用谷歌的Prediction API做机器学习 NVIDIA趣味解读：深度学习训练和推理有何不同？ 如何利用 Python 打造一款简易版 AlphaGo 如何用图像识别技术来变革商业？这里有份操作指南 MIT生成视频模型，预测静态图片的未来场景 EMNLP 2016干货：从原理到代码全面剖析可用于NLP的神经网络 使用机器学习翻译语言：神经网络和seq2seq为何效果非凡？ 神经网络快速入门：什么是多层感知器和反向传播？ 神经网络中激活函数的作用 逐层剖析，谷歌机器翻译突破背后的神经网络架构是怎样的？ 深度学习漫游指南：强化学习概览 深度学习与神经网络全局概览：核心技术的发展历程 最全的深度学习硬件指南 主流深度学习框架对比：看你最适合哪一款？ 继续进阶 40年认知架构研究概览：实现通用人工智能的道路上我们已走了多远？ 第四范式联合创始人陈雨强：机器学习在工业应用中的新思考 词嵌入系列博客Part1：基于语言建模的词嵌入模型 词嵌入系列博客Part2：比较语言建模中近似 softmax 的几种方法 词嵌入系列博客Part3：word2vec 的秘密配方 从分割到识别，全面解析Facebook开源的3款机器视觉工具 从硬件到软件：OpenAI 解读自家的深度学习基础架构 分布式深度学习：神经网络的分布式训练 Embedding 新框架模型：Exponential Family Embeddings FPGA vs. ASIC，谁将引领移动端人工智能潮流？ 概述性论文：卷积神经网络的近期研究进展 《Nature》 封面文章：人工智能引发材料科学变革 概述论文：迁移学习研究全貌 Andrej Karpathy：你为什么应该理解反向传播 GAN之父NIPS 2016演讲现场直击：全方位解读生成对抗网络的原理及未来 Google Brain 讲解注意力模型和增强RNN 谷歌Magenta项目是如何教神经网络编写音乐的？ 基于MXNet 的神经机器翻译实现 2016深度学习重大进展：从无监督学习到生成对抗网络 深度学习遇上基因组，诊断疾病和揭示深层生物原理或迎来突破 King+Woman-Man=Queen:用基于Spark的机器学习来捕捉词意 初学者必读:从迭代的五个层面理解机器学习 轻量级Matlab深度学习框架LightNet的实现 如何基于机器学习设计一套智能交易系统？ 如何在TensorFlow中用深度学习修复图像？ 机器学习中的并行计算：GPU、CUDA和实际应用 深度解读AlphaGo胜利背后的力量：强化学习 英伟达自动驾驶技术解读：用于自动驾驶汽车的端到端深度学习 深度解读最流行的优化算法：梯度下降 Science：斯坦福大学用迁移学习预测非洲贫困状况 用 Word2vec 轻松处理新金融风控场景中的文本类数据 Science：实用量子计算机已近在咫尺 深度学习硬件架构简述 深度学习系列Part2：迁移学习和微调深度卷积神经网络 图文并茂的神经网络架构大盘点：从基本原理到衍生关系 为你的深度学习任务挑选性价比最高GPU 详解谷歌神经网络图像压缩技术：高质量地将图像压缩得更小 用于视觉任务的CNN为何能在听觉任务上取得成功？ 自然语言处理领域深度学习研究总结：从基本概念到前沿成果 专访谷歌Jeff Dean：强化学习适合的任务与产品化应用 重磅论文：解析深度卷积神经网络的14种设计模式 前沿研究 并行运算，Facebook提出门控卷积神经网络的语言建模 FAIR与微软研究院合著论文：通过虚拟问答衡量机器智能 FusionNet融合三个卷积网络：识别对象从二维升级到三维 谷歌新论文提出神经符号机：使用弱监督在Freebase上学习语义解析器 Google Brain与OpenAI合作论文：规模化的对抗机器学习 谷歌新论文：使用生成对抗网络的无监督像素级域适应 谷歌ICLR 2017论文提出超大规模的神经网络：稀疏门控专家混合层 谷歌论文：使用循环神经网络的全分辨率图像压缩 谷歌新论文提出适应性生成对抗网络AdaGAN：增强生成模型 谷歌技术论文：用于YouTube推荐的深度神经网络 谷歌与微软合著论文：由知识引导的结构化注意网络 谷歌深度解读：机器人可以如何通过共享经历学习新技能 谷歌NIPS 2016提交的8篇论文：从无监督学习到生成模型 谷歌DeepMind论文：使用合成梯度的解耦神经接口 谷歌提交ICLR 2017论文：学习记忆罕见事件 谷歌提出深度概率编程语言Edward：融合了贝叶斯、深度学习和概率编程 谷歌新论文提出预测器架构：端到端的学习与规划 DeepMind最新论文：线性时间的神经机器翻译 DeepMind David Silver论文：学习跨多个数量级的值 DeepMind论文：在线Segment to Segment神经传导 DeepMind深度解读Nature论文：可微神经计算机 DeepMind论文：调控运动控制器的学习和迁移 DeepMind NIPS 2016论文盘点（Part1）：强化学习正大步向前 DeepMind NIPS 2016论文盘点（Part2）：无监督学习的新进展 ECCV 2016 最佳论文新鲜出炉 Geoffrey Hinton论文：使用快速权重处理最近的过去 哈工大讯飞联合实验室最新论文刷新机器阅读理解纪录 华盛顿大学论文：使用机器学习分析科学文献中的视觉信息 Ian Goodfellow 论文：通过视频预测的用于物理交互的无监督学习 Ian Goodfellow 论文：用于隐私训练数据的深度学习的半监督知识迁移 IBM论文：多尺度循环神经网络在对话生成中的应用 ICLR2016会议，不可错过Facebook提交的七篇论文 计算机科学领导者：卡内基梅隆大学ACL2016论文汇总 论文：TensorFlow，一个大规模机器学习系统 论文：通过连续奖励策略梯度学习在线比对 论文：基准评测当前最先进的深度学习软件工具 论文：一种用于训练循环网络的新算法Professor Forcing 论文：高斯混合模型的似然方法中的局部极大值：结构结果和算法结果 NIPS 2016现场：谷歌发布 28 篇机器学习论文 Nature论文：无监督表征学习，用电子健康病历增强临床决策 Nature论文：从不确定性表征到自动建模 NIPS 2016 公布571篇接收论文 OpenAI与NASA论文：用于张拉整体机器人运动的深度强化学习 OpenAI论文：神经GPU的扩展和限制 苹果发布第一篇人工智能研究论文：模拟+无监督方法改善合成图像 商汤科技论文解析：人脸检测中级联卷积神经网络的联合训练 斯坦福大学李飞飞最新论文：弱监督动作标记的连接时序模型 Vicarious在ICLR2017提交无监督学习论文：层级组合特征学习 Yann LeCun论文：基于能量的生成对抗网络 Yoshua Bengio 论文：一种神经知识语言模型 Yann LeCun提交ICLR 2017论文汇总：从GAN到循环实体网络等 Bengio论文：用于序列预测的actor-critic算法 Yoshua Bengio论文：迈向生物学上可信的深度学习 Yoshua Bengio论文：使用线性分类器探头理解中间层 Yoshua Bengio论文：Mollifying Networks 微软重磅论文提出LightRNN：高效利用内存和计算的循环神经网络 微软ACL 2016论文汇集，自然语言技术逼近人类对话水平 自然语言顶级会议ACL 2016谷歌论文汇集 专知汇总 链接 【专知荟萃01】深度学习知识资料大全集（入门/进阶/论文/代码/数据/综述/领域专家等）（附pdf下载） 【专知荟萃02】自然语言处理NLP知识资料大全集（入门/进阶/论文/Toolkit/数据/综述/专家等）（附pdf下载） 【专知荟萃03】知识图谱KG知识资料全集（入门/进阶/论文/代码/数据/综述/专家等）（附pdf下载） 【专知荟萃04】自动问答QA知识资料全集（入门/进阶/论文/代码/数据/综述/专家等）（附pdf下载） 【专知荟萃05】聊天机器人Chatbot知识资料全集（入门/进阶/论文/软件/数据/专家等）(附pdf下载) 【专知荟萃06】计算机视觉CV知识资料大全集（入门/进阶/论文/课程/会议/专家等）(附pdf下载) 【专知荟萃07】自动文摘AS知识资料全集（入门/进阶/代码/数据/专家等）(附pdf下载) 【专知荟萃08】图像描述生成Image Caption知识资料全集（入门/进阶/论文/综述/视频/专家等） 【专知荟萃09】目标检测知识资料全集（入门/进阶/论文/综述/视频/代码等） 【专知荟萃10】推荐系统RS知识资料全集（入门/进阶/论文/综述/视频/代码等） 【专知荟萃11】GAN生成式对抗网络知识资料全集（理论/报告/教程/综述/代码等） 【专知荟萃12】信息检索 Information Retrieval 知识资料全集（入门/进阶/综述/代码/专家，附PDF下载） 【专知荟萃13】工业学术界用户画像 User Profile 实用知识资料全集（入门/进阶/竞赛/论文/PPT，附PDF下载） 【专知荟萃14】机器翻译 Machine Translation知识资料全集（入门/进阶/综述/视频/代码/专家，附PDF下载） 【专知荟萃15】图像检索Image Retrieval知识资料全集（入门/进阶/综述/视频/代码/专家，附PDF下载） 【专知荟萃16】主题模型Topic Model知识资料全集（基础/进阶/论文/综述/代码/专家，附PDF下载） 【专知荟萃17】情感分析Sentiment Analysis 知识资料全集（入门/进阶/论文/综述/视频/专家，附查看） 【专知荟萃18】目标跟踪Object Tracking知识资料全集（入门/进阶/论文/综述/视频/专家，附查看） 【专知荟萃19】图像识别Image Recognition知识资料全集（入门/进阶/论文/综述/视频/专家，附查看） 【专知荟萃20】图像分割Image Segmentation知识资料全集（入门/进阶/论文/综述/视频/专家，附查看） 【专知荟萃21】视觉问答VQA知识资料全集（入门/进阶/论文/综述/视频/专家，附查看） 【专知荟萃22】机器阅读理解RC知识资料全集（入门/进阶/论文/综述/代码/专家，附查看） 【专知荟萃23】深度强化学习RL知识资料全集（入门/进阶/论文/综述/代码/专家，附查看） 【专知荟萃24】视频描述生成(Video Captioning)知识资料全集（入门/进阶/论文/综述/代码/专家，附查看） 【专知荟萃25】文字识别OCR知识资料全集（入门/进阶/论文/综述/代码/专家，附查看） 【专知荟萃26】行人重识别 Person Re-identification知识资料全集（入门/进阶/论文/综述/代码，附查看） 2017年机器之心发布的教程 链接概念1. 机器学习基础 一文读懂机器学习、数据科学、人工智能、深度学习和统计学之间的区别 人人都能读懂的无监督学习：什么是聚类和降维？ 如何解读决策树和随机森林的内部工作机制？ 教程 | 拟合目标函数后验分布的调参利器：贝叶斯优化 入门 | 区分识别机器学习中的分类与回归 深度 | 思考VC维与PAC：如何理解深度神经网络中的泛化理论？ 教程 | 理解XGBoost机器学习模型的决策过程 业界 | 似乎没区别，但你混淆过验证集和测试集吗？ 教程 | 初学者如何学习机器学习中的L1和L2正则化 机器学习算法集锦：从贝叶斯到深度学习及各自优缺点 入门 | 机器学习新手必看10大算法 教程 | 详解支持向量机SVM：快速可靠的分类算法 干货 | 详解支持向量机（附学习资源） 教程 | 遗传算法的基本概念和实现（附Java实现案例） 教程 | 利用达尔文的理论学习遗传算法 深度 | 详解可视化利器t-SNE算法：数无形时少直觉 入门 | 如何构建稳固的机器学习算法：Boosting&amp;Bagging 资源 | 神经网络调试手册：从数据集与神经网络说起 观点 | 三大特征选择策略，有效提升你的机器学习水准 教程 | 如何为单变量模型选择最佳的回归函数 机器学习老中医：利用学习曲线诊断模型的偏差和方差 教程 | 如何为时间序列数据优化K-均值聚类速度？ 入门 | 将应用机器学习转化为求解搜索问题 从重采样到数据合成：如何处理机器学习中的不平衡分类问题？ 2. 深度模型基础 从零开始：教你如何训练神经网络 教程 | 深度学习初学者必读：张量究竟是什么？ 解读 | 通过拳击学习生成对抗网络（GAN）的基本原理 干货 | 直观理解GAN背后的原理：以人脸图像生成为例 教程 | 从基本概念到实现，全卷积网络实现更简洁的图像识别 资源 | 初学者指南：神经网络在自然语言处理中的应用 教程 | 深度学习：自动编码器基础和类型 入门 | 请注意，我们要谈谈神经网络的注意机制和使用方法 教程 | 经典必读：门控循环单元（GRU）的基本概念与原理 入门 | 迁移学习在图像分类中的简单应用策略 解读 | 如何从信号分析角度理解卷积神经网络的复杂机制？ 教程 | 无监督学习中的两个非概率模型：稀疏编码与自编码器 深度 | 从任务到可视化，如何理解LSTM网络中的神经元 教程 | 将注意力机制引入RNN，解决5大应用领域的序列预测问题 教程 | 听说你了解深度学习最常用的学习算法：Adam优化算法？ 教程 | 如何解决LSTM循环神经网络中的超长序列问题 教程 | 一个基于TensorFlow的简单故事生成案例：带你了解LSTM 教程 | 如何判断LSTM模型中的过拟合与欠拟合 教程 | 如何估算深度神经网络的最优学习率 教程 | 如何为神经机器翻译配置编码器-解码器模型？ 教程 | 如何用深度学习处理结构化数据？ 改进卷积神经网络，你需要这14种设计模式 3. 强化学习基础 从强化学习基本概念到Q学习的实现，打造自己的迷宫智能体 教程 | Keras+OpenAI强化学习实践：深度Q网络 一份数学小白也能读懂的「马尔可夫链蒙特卡洛方法」入门指南 入门 | 蒙特卡洛树搜索是什么？如何将其用于规划星际飞行？ 教程 | Keras+OpenAI强化学习实践：行为-评判模型 从贝叶斯定理到概率分布：综述概率论基本定义 想了解概率图模型？你要先理解图论的基本定义与形式 数学 干货 | 机器学习需要哪些数学基础？ 深度神经网络中的数学，对你来说会不会太难？ 观点 | Reddit 热门话题：如何阅读并理解论文中的数学内容？ 教程 | 基础入门：深度学习矩阵运算的概念和代码实现 从概率论到多分类问题：综述贝叶斯统计分类 机器之心最干的文章：机器学习中的矩阵、向量求导 致初学者 教程 | Kaggle CTO Ben Hamner ：机器学习的八个步骤 教程 | Kaggle初学者五步入门指南，七大诀窍助你享受竞赛 从零开始，教初学者如何征战Kaggle竞赛 只需十四步：从零开始掌握Python机器学习（附资源） 如何从初入行者进阶为人工智能先锋青年？ 观点 | 如何从一名软件工程师转行做人工智能？ 教程 | 如何转行成为一名数据科学家？ 初学者怎么选择神经网络环境？对比MATLAB、Torch和TensorFlow 教程 | 初学者如何选择合适的机器学习算法（附速查表） 经验之谈：如何为你的机器学习问题选择合适的算法？ 资源 | 企业应该怎样选择数据科学&amp;机器学习平台？ 实验研究工作流程详解：如何把你的机器学习想法变成现实 观点 | 机器学习新手工程师常犯的6大错误 教程 | 如何用Docker成为更高效的数据科学家？ 从标题到写作流程：写好一篇论文的十条基本原则 论文格式排版你真的做对了吗? 常用格式及其LaTeX书写方法介绍 课程 蒙特利尔大学开放MILA 2017夏季深度学习与强化学习课程视频（附完整PPT） 斯坦福CS231n Spring 2017开放全部课程视频（附大纲） 斯坦福大学秋季课程《深度学习理论》STATS 385开讲 资源 | CMU统计机器学习2017春季课程：研究生水平 教程 | 斯坦福CS231n 2017最新课程：李飞飞详解深度学习的框架实现与对比 三天速成！香港科技大学TensorFlow课件分享 四天速成！香港科技大学 PyTorch 课件分享 吴恩达Deeplearning.ai课程学习全体验：深度学习必备课程（已获证书） 算法实现1. 机器学习基础实现 教程 | 从头开始：用Python实现带随机梯度下降的线性回归 初学TensorFlow机器学习：如何实现线性回归？（附练习题） 教程 | 从头开始：用Python实现带随机梯度下降的Logistic回归 教程 | 从头开始：用Python实现随机森林算法 教程 | 从头开始：用Python实现基线机器学习算法 教程 | 从头开始：用Python实现决策树算法 听说你用JavaScript写代码？本文是你的机器学习指南 教程 | 如何使用JavaScript构建机器学习模型 教程 | 初学文本分析：用Python和scikit-learn实现垃圾邮件过滤器 教程 | 如何通过牛顿法解决Logistic回归问题 每个Kaggle冠军的获胜法门：揭秘Python中的模型集成 教程 | 如何在Python中快速进行语料库搜索：近似最近邻算法 2. 深度网络基础实现 教程 | 初学者入门：如何用Python和SciKit Learn 0.18实现神经网络？ 教程 | 如何用30行JavaScript代码编写神经网络异或运算器 教程 | 使用MNIST数据集，在TensorFlow上实现基础LSTM网络 教程 | 如何使用Keras集成多个卷积网络并实现共同预测 教程 | 在Python和TensorFlow上构建Word2Vec词嵌入模型 教程 | 详解如何使用Keras实现Wassertein GAN 机器之心GitHub项目：从零开始用TensorFlow搭建卷积神经网络 教程 | 如何基于TensorFlow使用LSTM和CNN实现时序分类任务 作为TensorFlow的底层语言，你会用C++构建深度神经网络吗？ 入门 | 十分钟搞定Keras序列到序列学习（附代码实现） 入门 | 想实现DCGAN？从制作一张门票谈起！ 教程 | 通过PyTorch实现对抗自编码器 教程 | 基于Keras的LSTM多变量时间序列预测 3. 计算机视觉实现 教程 | TensorFlow从基础到实战：一步步教你创建交通标志分类神经网络 教程 | 如何使用TensorFlow和自编码器模型生成手写数字 教程 | 无需复杂深度学习算法，基于计算机视觉使用Python和OpenCV计算道路交通 教程 | 深度学习 + OpenCV，Python实现实时视频目标检测 教程 | 如何通过57行代码复制价值8600万澳元的车牌识别项目 教程 | 百行代码构建神经网络黑白图片自动上色系统 教程 | 盯住梅西：TensorFlow目标检测实战 深度 | 从数据结构到Python实现：如何使用深度学习分析医学影像 仅需15分钟，使用OpenCV+Keras轻松破解验证码 教程 | 如何使用TensorFlow API构建视频物体识别系统 教程 | 经得住考验的「假图片」：用TensorFlow为神经网络生成对抗样本 先读懂CapsNet架构然后用TensorFlow实现，这应该是最详细的教程了 教程 | 如何使用深度学习去除人物图像背景 资源 | 如何通过CRF-RNN模型实现图像语义分割任务 4. 自然语言处理实现 如何解决90％的自然语言处理问题：分步指南奉上 资源 | Github项目：斯坦福大学CS-224n课程中深度NLP模型的PyTorch实现 谷歌开放GNMT教程：如何使用TensorFlow构建自己的神经机器翻译系统 教程 | 从头开始在Python中开发深度学习字幕生成模型 资源 | 谷歌全attention机器翻译模型Transformer的TensorFlow实现 教程 | 如何使用TensorFlow构建、训练和改进循环神经网络 教程 | Kaggle网站流量预测任务第一名解决方案：从模型到代码详解时序预测 教程 | 用于金融时序预测的神经网络：可改善移动平均线经典策略 教程 | 如何用PyTorch实现递归神经网络？ 教程 | 用数据玩点花样！如何构建skip-gram模型来训练和可视化词向量 教程 | 利用TensorFlow和神经网络来处理文本分类问题 5. 强化学习实现 教程 | 深度强化学习入门：用TensorFlow构建你的第一个游戏AI 资源 | 价值迭代网络的PyTorch实现与Visdom可视化 解读 | 如何使用深度强化学习帮助自动驾驶汽车通过交叉路口？ 6. 深度学习框架 分布式TensorFlow入坑指南：从实例到代码带你玩转多机器深度学习 教程 | 从零开始：TensorFlow机器学习模型快速部署指南 资源 | TensorFlow极简教程：创建、保存和恢复机器学习模型 快速开启你的第一个项目：TensorFlow项目架构模板 TensorFlow初学者指南：如何为机器学习项目创建合适的文件架构 教程 | 七个小贴士，顺利提升TensorFlow模型训练表现 教程 | 如何从TensorFlow转入PyTorch 教程 | 如何利用C++搭建个人专属的TensorFlow 谷歌云大会教程：没有博士学位如何玩转TensorFlow和深度学习（附资源） 教程 | 维度、广播操作与可视化：如何高效使用TensorFlow 教程 | PyTorch内部机制解析：如何通过PyTorch实现Tensor 贾扬清撰文详解Caffe2：从强大的新能力到入门上手教程 教程 | TensorFlow 官方解读：如何在多系统和网络拓扑中构建高性能模型 教程 | 如何使用TensorFlow中的高级API：Estimator、Experiment和Dataset 教程 | Docker Compose + GPU + TensorFlow 所产生的奇妙火花 工具方法 教程 | 如何优雅而高效地使用Matplotlib实现数据可视化 教程 | 如何用百度深度学习框架PaddlePaddle做数据预处理 教程 | 一文入门Python数据分析库Pandas 代码优化指南：人生苦短，我用Python 资源 | 从数组到矩阵的迹，NumPy常见使用大总结 教程 | Python代码优化指南：从环境设置到内存分析（一） 资源 | 如何利用VGG-16等模型在CPU上测评各深度学习框架 教程 | 手把手教你可视化交叉验证代码，提高模型预测能力 教程 | 如何使用Kubernetes GPU集群自动训练和加速深度学习？ 教程 | Prophet：教你如何用加法模型探索时间序列数据 初学机器学习的你，是否掌握了这样的Linux技巧？ 云端 教程 | 新手指南：如何在AWS GPU上运行Jupyter noterbook？ 教程 | 只需15分钟，使用谷歌云平台运行Jupyter Notebook 入门 | 完全云端运行：使用谷歌CoLaboratory训练神经网络 边缘设备 教程 | BerryNet：如何在树莓派上实现深度学习智能网关 机器之心实操 | 亚马逊详解如何使用MXNet在树莓派上搭建实时目标识别系统 手把手教你为iOS系统开发TensorFlow应用（附开源代码） 教程 | 如何使用Swift在iOS 11中加入原生机器学习视觉模型 教程 | 如何使用谷歌Mobile Vision API 开发手机应用 开源 | 深度安卓恶意软件检测系统：用卷积神经网络保护你的手机 专栏 | 手机端运行卷积神经网络实践：基于TensorFlow和OpenCV实现文档检测功能 资源 | 用苹果Core ML实现谷歌移动端神经网络MobileNet 教程 | 如何用TensorFlow在安卓设备上实现深度学习推断 深度 | 向手机端神经网络进发：MobileNet压缩指南 硬件 成本14,000元，如何自己动手搭建深度学习服务器？ 资源 | 只需1200美元，打造家用型深度学习配置（CPU+GTX 1080） 教程 | 从硬件配置、软件安装到基准测试，1700美元深度学习机器构建指南 从硬件配置到框架选择，请以这种姿势入坑深度学习 从零开始：深度学习软件环境安装指南 这是一份你们需要的Windows版深度学习软件安装指南 教程 | 一步步从零开始：使用PyCharm和SSH搭建远程TensorFlow开发环境 实用指南：如何为你的深度学习任务挑选最合适的 GPU?（最新版） 深度 | 英伟达Titan Xp出现后，如何为深度学习挑选合适的GPU？这里有份性价比指南 Titan XP值不值？一文教你如何挑选深度学习GPU 吃喝玩乐撸撸猫 教程 | 你来手绘涂鸦，人工智能生成「猫片」：edges2cats图像转换详解 教程 | 萌物生成器：如何使用四种GAN制造猫图 学界 | 宅男的福音：用GAN自动生成二次元萌妹子 深度 | 如何使用神经网络弹奏出带情感的音乐？ 深度 | 人工智能如何帮你找到好歌：探秘Spotify神奇的每周歌单 解读 | 艺术家如何借助神经网络进行创作？ 教程 | 用生成对抗网络给雪人上色，探索人工智能时代的美学 圣诞快乐——Keras+树莓派：用深度学习识别圣诞老人 教程 | 摄影爱好者玩编程：利用Python和OpenCV打造专业级长时曝光摄影图 教程 | 基于遗传算法的拼图游戏解决方案 教程 | AI玩微信跳一跳的正确姿势：跳一跳Auto-Jump算法详解 Money, Money, Money 教程 | 从零开始：如何使用LSTM预测汇率变化趋势 自创数据集，使用TensorFlow预测股票入门 资源 | 利用深度强化学习框架解决金融投资组合管理问题（附 GitHub 实现） 比特币突破8000美元，我们找到了用DL预测虚拟货币价格的方法 教程 | 如何使用深度学习硬件的空余算力自动挖矿 教程 | 如何用Python和机器学习炒股赚钱？ 最流行的神经网络变体综述 链接]]></content>
      <categories>
        <category>资源</category>
      </categories>
      <tags>
        <tag>资源汇总</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习中的模型评价与选择]]></title>
    <url>%2F2018%2F02%2F16%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BB%B7%E4%B8%8E%E9%80%89%E6%8B%A9%2F</url>
    <content type="text"><![CDATA[介绍机器学习中如何评估、选择模型 Holdout方法和正态逼近置信区间 重复Holdout和Bootstrap 交叉验证和超参数优化 结论 节选自机器之心 链接，原文还涉及很多偏数学、理论方面的东西英文原文 链接 机器学习模型的性能估计主要分为以下三步 使用训练集训练模型 用模型预测测试集的分类 计算在测试集上的错误率 Holdout方法 Holdout是最简单的模型评估技术，把数据集随机拆分成训练集和测试集 Resubstitution验证或Resubstitution评估指的是在同一个训练数据集上训练和评估一个模型 随机抽样可能导致训练集和测试集分布不平衡，甚至测试集中可能不包含少数类的样本。因此，可以使用层次化的方式划分数据集，保证划分得到的训练集和测试集保持原始比例 在数据量大、类别较平衡的数据集上，非层次化的随机抽样不是一个大问题，但最好还是使用层次化抽样 正态逼近置信区间：计算模型预测精度或计算误差置信区间。假设预测结果会遵循一个正态分布，然后根据中心极限定理计算单次训练-测试划分的平均值的置信区间。通过正态逼近计算出基于单个测试集的性能估计的不确定性。详细推导见原文 重复Holdout和Bootstrap 模型评估的偏差与方差：将数据集的较多数据作为测试集容易给评估带来悲观偏差。随着测试集样本数量的减少，悲观偏差降低，性能估计的方差增加 重复Holdout验证：多次随机划分训练集和测试集，重复Holdout方法估计模型性能然后取平均值的方法获得更具鲁棒性的评估，又称为蒙特卡洛交叉验证 Bootstrap方法：从经验分布中采样生成新样本，衡量性能估计的不确定性。如果把Holdout方法理解为不放回采样，那么bootstrap就可以理解为通过有放回重采样产生新数据 Leave-one-out Bootstrap（LOOB）方法：这种方法测试集数据和训练集数据没有重叠 交叉验证和超参数优化 three-way Holdout方法：将数据集划分为训练、验证和测试集三部分，用于超参数调优 K-Fold交叉验证：数据集中的每个样本都有被测试的机会。与重复holdout方法相比，k-fold交叉验证的测试数据没有重叠，而重复holdout是重复使用样本进行测试 2-fold和留一法（Leave-One-Out）交叉验证：分别是K-Fold交叉验证中k=2和k=n的特殊情况。留一交叉验证法中，每次迭代中，模型都在其中n-1个样本上进行拟合，然后在剩余的一个样本上进行评估。这种方法对小数据集特别有用 在计算可行的情况下，留一法交叉验证更值得推荐，近似无偏，但方差大；当数据集很大的时候，出于计算效率的考虑我们就会更倾向于holdout方法（相当于分成2部分，迭代2次即可），我们在深度学习中使用的就是Holdout方法而不是交叉验证 在k-fold交叉验证中，随着k的增加有如下趋势： 性能估计偏差减小（更准确）性能估计方差增大（更大的变化性）计算成本增加（在拟合过程中训练集更大，需要的迭代次数更多）在k-fold交叉验证中将k的值降到最小（如2或3）也会增加小数据集上模型估计的方差，因为随机抽样变化较大推荐用10折交叉验证 如果我们对数据归一化或进行特征选择，我们通常会在k-fold交叉验证循环中执行这些操作，即做归一化时不考虑测试集部分的数据，但可能效果不好。关于这一选择可参考链接。看了以后感觉不是很有所谓，差异不大(“for the greater majority of cases, IN and OUT are not significantly different”) 结论 当处理的样本量较大时，使用holdout方法进行模型评价非常合适 对于超参数优化，推荐10折交叉验证 对于小样本，留一法交叉验证则是一个不错的选择]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>模型评估</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度学习技巧]]></title>
    <url>%2F2018%2F02%2F14%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8A%80%E5%B7%A7%2F</url>
    <content type="text"><![CDATA[在这里总结一下看到过的各路炼丹技巧 深度学习的7大实用技巧* 链接 数据：有标签的数据越多，模型的性能越好。重温深度学习时代数据不可思议的有效性 优化器：RMSprop，Adadelta和Adam法都是自适应优化算法，会自动更新学习率。普通的随机梯度下降法需要手动选择学习率和动量参数。 在实践中，自适应优化器往往比普通的梯度下降法更快地让模型达到收敛状态。然而，选择这些优化器的模型最终性能都不太好，而普通的梯度下降法通常能够达到更好的收敛最小值，从而获得更好的模型性能，但这可能比某些优化程序需要更多的收敛时间。此外，随机梯度下降法也更依赖于有效的初始化方法和学习速率衰减指数的设置，这在实践中是很难确定的。如下图，Adam初期效果好，但SGD在训练结束后可以获得更好的全局最小值 可以混合使用两类优化器：由Adam优化器过渡到随机梯度下降法来优化模型。在训练的早期阶段，使用Adam优化器来启动模型的训练，这将为模型的训练节省很多参数初始化和微调的时间。一旦模型的性能有所起伏，我们就可以切换到带动量的随机梯度下降法来进一步优化模型，以达到最佳的性能 处理数据不平衡：不同类的训练数据数量差异较大，会使得预测结果倾向于数量大的类别。 对损失函数使用类别权重：对于数据量小的类别，损失函数中使用较高的权重值。这样对该类的任何错误都将导致非常大的损失值，以此来惩罚错误分类过度抽样：对于小样本类别，重复进行采样欠采样：对于大样本类别，跳过不选择部分数据数据增强：对于小样本类别进行数据增强，生成更多训练样本 迁移学习 数据增强：保证数据原始类别标签的同时，对一些原始图像进行非线性的图像变换，来生成/合成新的训练样本。即对图像进行任何操作，改变图像的外观，但不能改变整体的内容。如对于猫的图像，我们可以把图像进行任意的几何变换，只要还是一只猫，那么label就不会变，但在图像以外的问题上能否直接使用数据增强有待进一步查阅资料 水平或垂直旋转/翻转图像随机改变图像的亮度和颜色随机模糊图像随机裁剪图像 集成模型：对于一个特定的任务，训练多个模型并根据模型的整体性能决定最优的组合方案。具体的实现细节还不太清楚，但模型集成有三种比较典型的方式：链接。下2中有关于集成的几种常见设计 模型剪枝加速：目的是为了在提高训练速度的同时保持模型的高性能，目前已有许多相关的研究，如MobileNet等等。核心思想是按照一定标准为神经元进行排序，从而将排名低(即冗余的神经元，对模型贡献不大)的移除模型，也就是把不重要的卷积滤波器丢弃(因为神经元和卷积核一一对应) 一般的剪枝流程如下：Network -&gt; Evaluate Importance of Neurons -&gt; Remove the Least Important Neuron -&gt; Fine-tuning -&gt; Continue Pruning -&gt; Back to Evaluate Importance of Neurons 知乎专栏 参数初始化：Xavier和He随便选一个，但一定要做 uniform均匀分布初始化：w = np.random.uniform(low=-scale, high=scale, size=[n_in,n_out]) Xavier初始法，适用于普通激活函数(tanh,sigmoid)：scale = np.sqrt(3/n) He初始化，适用于ReLU：scale = np.sqrt(6/n) normal高斯分布初始化：w = np.random.randn(n_in,n_out) * stdev # stdev为高斯分布的标准差，均值设为0 Xavier初始法，适用于普通激活函数 (tanh,sigmoid)：stdev = np.sqrt(n) He初始化，适用于ReLU：stdev = np.sqrt(2/n) svd初始化：对RNN有比较好的效果。参考论文：https://arxiv.org/abs/1312.6120 数据预处理 常用：zero-center X -= np.mean(X, axis = 0) # zero-center X /= np.std(X, axis = 0) # normalize 不常用：白化 训练技巧 要做梯度归一化，即算出来的梯度除以minibatch size clip c(梯度裁剪): 限制最大梯度，其实是value = sqrt(w1^2+w2^2….)。如果value超过了阈值，就算一个衰减系系数，让value的值等于阈值: 5,10,15 Dropout对小数据防止过拟合有很好的效果，值一般设为0.5。小数据上dropout+sgd效果提升都非常明显。dropout的位置比较有讲究, 对于RNN,建议放到输入-&gt;RNN与RNN-&gt;输出的位置。关于RNN如何用dropout,可以参考论文 优化器：同1中所述，SGD收敛慢，但结果好。对于SGD，可以选择从1.0或者0.1的学习率开始，隔一段时间在验证集上检查一下，如果cost没有下降，就对学习率减半。也可以先用ada系列先跑，最后快收敛的时候，更换成sgd继续训练，同样也会有提升。据说adadelta一般在分类问题上效果比较好，adam在生成问题上效果比较好 除了gate之类的地方需要把输出限制成0-1之外，尽量不要用sigmoid。可以用tanh或者relu之类的激活函数。sigmoid的问题: 1. sigmoid函数在-4到4的区间里，才有较大的梯度。其他区间的梯度接近0，很容易造成梯度消失问题。2. 如果输入是0均值的，sigmoid函数的输出不是0均值的 rnn的dim和embdding size，一般从128上下开始调整。batch size一般从128左右开始调整，合适最重要，并不是越大越好。关于Batch Size设置的分析可参考这篇文章。对于batch，建议是把卡塞满的2的n次方 word2vec初始化，在小数据上不仅可以有效提高收敛速度，也可以可以提高结果 尽量对数据做shuffle LSTM的forget gate的bias，用1.0或者更大的值做初始化可以取得更好的结果，来自这篇论文。实际使用中，不同的任务可能需要尝试不同的值 使用Batch Normalization可以提升效果 如果模型包含全连接层（MLP），且输入和输出大小一样，可以考虑将MLP替换成Highway Network，对结果有一点提升，建议作为最后提升模型的手段。原理很简单，就是给输出加了一个gate来控制信息的流动，详细介绍请参考论文 一轮加正则，一轮不加正则，反复进行 Ensemble(模型集成) Ensemble是论文刷结果的终极核武器，一般有以下几种方式： 同样的参数，不同的初始化方式 不同的参数，通过cross-validation，选取最好的几组 同样的参数，模型训练的不同阶段，即不同迭代次数的模型 不同的模型，进行线性融合。例如RNN和传统模型 知乎回答 Relu+Bn可以满足95%的情况，除非有些特殊情况会用identity，比如回归问题，比如resnet的shortcut支路 Dropout，分类问题用dropout ，只需要最后一层softmax 前用基本就可以了，能够防止过拟合，可能对accuracy提高不大，但是dropout 前面的那层如果是之后要使用的feature的话，性能会大大提升 数据的shuffle和augmentation，aug不能瞎加，比如行人识别一般就不会加上下翻转的，因为不会碰到头朝下的异型种 降学习率，随着网络训练的进行，学习率要逐渐降下来。如果用tensorboard就能发现，在学习率下降的一瞬间，网络会有个巨大的性能提升。同样的fine-tuning也要根据模型的性能设置合适的学习率，比如一个训练的已经非常好的模型你上来就1e-3的学习率，那之前就白训练了，就是说网络性能越好，学习率要越小 tensorboard，帮助你监视网络的状态，来调整网络参数 随时存档模型，要有validation。把每个epoch和其对应的validation 结果存下来，可以分析出开始overfitting的时间点，方便下次加载fine-tuning 网络层数，参数量什么的都不是大问题，在性能不丢的情况下，减到最小。（不是说网络越深越好？） 对于batch size，建议是把卡塞满的2的n次方 输入减不减mean归一化在有了bn之后已经不那么重要了 卷积核的分解。从最初的5×5分解为两个3×3，到后来的3×3分解为1×3和3×1，再到resnet的1×1，3×3，1×1，再xception的3×3 channel-wise conv+1×1，网络的计算量越来越小，层数越来越多，性能越来越好，这些都是设计网络时可以借鉴的 不同尺寸的feature maps的concat，只用一层的feature map一把梭可能不如concat好，pspnet就是这种思想，这个思想很常用 resnet的shortcut确实会很有用，重点在于shortcut支路一定要是identity，主路是什么conv都无所谓，这是我亲耳听resnet作者所述 针对于metric learning，对feature加个classification 的约束通常可以提高性能加快收敛 链接和链接 Loss曲线：如果loss趋于平稳，但是没有降到足够低，随着learning rate的进一步降低，loss还可能继续减小。把test(validation)的accurcy也放在同一张图中，查看何时过拟合，参看链接。loss波动大的，考虑减小lr]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>炼丹技巧</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[六大聚类算法]]></title>
    <url>%2F2018%2F02%2F12%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%2F%E5%85%AD%E5%A4%A7%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[介绍六类主流的聚类算法，并总结了相应的优缺点 K-Means 均值漂移聚类 基于密度的聚类方法（DBSCAN） 用高斯混合模型（GMM）的最大期望（EM）聚类 凝聚层次聚类 图团体检测 原文来自机器之心 链接序号写成”.”格式全部崩了。。把分隔符去掉又显得太挤了。。。以后找到办法了再回来改TAT K-Means 优点：速度快，O(N) 缺点：类别数需要事先人为估计，无法通过算法来确定类别数；结果有可能不可重复 变种：K-Medians，每次用中值向量而不是均值来作为中心。对异常值不敏感，对大数据集速度慢，因为需要做排序 均值漂移聚类 算法：随机选择n个中心点，有n个半径为r的圆形滑动窗口，同时开始迭代。每次迭代时，滑动窗口的中心点都不断移向窗口内所有点的均值点，也就是移向点密度更大的区域，直到收敛(点密度无法再增加)。收敛时，若多个窗口发生重叠时，保留点数最多的那个窗口。这样就可以得到类别数以及每个类的中心位置。其他各个点属于哪一类则取决于迭代过程中该点在哪一类的窗口中的次数最多 优点：类别数量由算法自动获得；聚类中心朝最大点密度聚集的事实也是非常令人满意的 缺点：窗口大小/半径「r」的选择可能是不重要的 基于密度的聚类方法（DBSCAN） 算法：每次从一个未被访问过的数据点开始，若该点的邻域(ε 距离内的所有点都是邻域点)内点的数量大于参数minPoints，则该点为一个新类的第一个点，否则该点就是噪声点。若该点成为了一个新类的第一个点，则该点邻域内的所有点也成为了该类的一部分，同时以这些新点再去找它们邻域中的点并加到该类中来，直到收敛。收敛后，在未被访问过的点中再找一个重新开始之前的步骤 优点：不需要人为确定类别数；能够辨别噪声；能很好地找到任意大小和任意形状的类 缺点：当簇的密度不同时，它的表现不如其他聚类算法(因为密度不同，参数ε和minPoints的设置也需要不一样，难以估计) 用高斯混合模型（GMM）的最大期望（EM）聚类 K-Means中假设数据点的分布是圆形的(其分类原理是离哪个中心点更近就认为是哪一类)，限制较大。GMM中假设数据点是高斯分布的，意味着类可以是各种类型的形状(二维中为椭圆，因为x和y方向各自有标准差)。于是，任务就变成了使用EM来找到每个类别的高斯函数。下图是典型的K-Means不适用的情况： 算法：和K-Means一样，先选择类别数量，并随机初始化高斯分布参数。随后计算每个数据点属于一个特定类的概率(一个点越靠近高斯的中心，它就越可能属于该类)。基于这些概率，计算一组新的高斯分布参数使得类内数据点的概率最大化。其中，新参数是由数据点位置的加权和得到，权重就是之前提到的概率。重复之前的操作直到收敛 优点：类形状任意(K-Means其实是GMM的一个特殊情况，即所有维度的协方差均接近0)；由于GMM使用概率，因此每个数据点可以属于很多类，且对于每个类都有一个相应的概率 凝聚层次聚类 层次聚类分为自上而下和自下而上。对于自下而上，每个数据点都是单独的一个类，随后不断合并两个类，直到所有类最后都合并成一个包含所有数据点的类。自下而上层次聚类又被称为凝聚式层次聚类(HAC)。 算法：一开始有N个数据点，也就是N个类。随后自行定义一种距离度量标准如平均距离等，每次迭代时选择将两个距离最近的类合并在一起，直到只剩下一个类。我们只需要选择何时停止合并，就可以选择最终需要多少个类 优点：不需要指定类个数，我们可以自由选择看起来最好的类个数；对距离度量标准不敏感； 缺点：效率低，O(N^2) 图团体检测（Graph Community Detection） 当我们的数据可以被表示为一个网络或图（graph）时，我们可以使用图团体检测方法完成聚类。聚类的质量由模块性分数进行评估，模块性越高，该网络聚类成不同团体的程度就越好。因此通过最优化方法寻找最大模块性就能发现聚类该网络的最佳方法。 模块性可以使用以下公式进行计算： 其中L代表网络中边的数量，k_i和k_j是指每个顶点的degree，它可以通过将每一行和每一列的项加起来而得到。两者相乘再除以2L表示当该网络是随机分配的时候顶点i和j之间的预期边数。括号中的项表示了该网络的真实结构和随机组合时的预期结构之间的差。当A_ij==1且(k_i*k_j)/2L很小时，其返回的值最高。即当在定点i和j之间存在一个「非预期」的边时，得到的值更高。最后的δc_i, c_j是克罗内克δ函数（Kronecker-delta function）。 优点：在典型的结构化数据中和现实网状数据都有非常好的性能 缺点：会忽略一些小的集群，且只适用于结构化的图模型]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>无监督学习</tag>
        <tag>聚类</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[cs231n学习笔记]]></title>
    <url>%2F2018%2F02%2F11%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%2Fcs231n%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[把cs231n 2017年的课程重新刷一遍，记录相关的笔记 cs231n课程主页 cs231n课程大纲 cs231n课程视频 Lecture 2 Image Classification课程资源Course Materials python/numpy tutorial image classification notes Course Materials 翻译 Python Numpy教程 图像分类笔记 课程笔记Python NumpyPython python –version # 查看Python版本 Python没有x++和x—的操作符 布尔逻辑：and；or；not；!=(xor) 字符串：文档 hw = ‘hello’ + ‘ ‘ + ‘world’ hw = ‘%s %s %d’ % (hello, world, 12) print(s.capitalize()) # 把第一个字母变大写，其余小写 print(s.upper()) # 全部大写 print(s.rjust(7)) # 右对齐，最左端不足补空格 print(s.center(7)) # 居中对齐 print(s.replace(‘l’, ‘(ell)’)) # 替换 print(‘ world ‘.strip()) # 去掉空格 容器：文档 列表Lists：Python中的数组，但是列表长度可变，能包含不同类型元素xs = [3, 1, 2] # 创建一个列表 print xs, xs[2] # 输出：&quot;[3, 1, 2] 2&quot; print xs[-1] # 即xs[2] xs[2] = &apos;foo&apos; # 列表可以容纳不同类型的元素 print xs # 输出：&quot;[3, 1, &apos;foo&apos;]&quot; xs.append(&apos;bar&apos;) # 向列表末尾中加元素 print xs # 输出：&quot;[3, 1, &apos;foo&apos;, &apos;bar&apos;]&quot; x = xs.pop() # 移除最后列表最后一个元素，并返回该元素 print x, xs # 输出：&quot;bar [3, 1, &apos;foo&apos;]&quot; xs = xs + [&quot;fig&quot;, &quot;melon&quot;] # 添加元素可以直接&quot;+&quot;或用append my_list_copy = my_list # list在变量中存的是引用，这样赋值的话两个变量对应的是同一片内存地址 my_list_copy = list(my_list) my_list_copy = my_list[:] # 以上两种是复制列表中的值而不是引用 切片Slicing：用于获取列表中的元素，语法list[start_index: end_index: step]nums = range(5) # range是一个内置函数，创建一个列表，包含0到5的integer print nums # 输出：&quot;[0, 1, 2, 3, 4]&quot; print nums[2:4] # 获得一个索引从2到4（不包括4）的切片; 输出：&quot;[2, 3]&quot; print nums[2:] # 获得一个索引从2到末尾的切片; 输出：&quot;[2, 3, 4]&quot; print nums[:2] # 获得一个索引从开头到2（不包括2）的切片; 输出：&quot;[0, 1]&quot; print nums[:] # 获得一个索引全部元素的切片; 输出：&quot;[0, 1, 2, 3, 4]&quot; print nums[:-1] # 切片的索引也可以是负数; 输出：&quot;[0, 1, 2, 3]&quot; nums[2:4] = [8, 9] # 把一个列表赋值给一个切片 print nums # 输出：&quot;[0, 1, 8, 8, 4]&quot; 循环Loops animals = [&apos;cat&apos;, &apos;dog&apos;, &apos;monkey&apos;] for animal in animals: print animal # 输出：&quot;cat&quot;, &quot;dog&quot;, &quot;monkey&quot;, 一个一行 animals = [&apos;cat&apos;, &apos;dog&apos;, &apos;monkey&apos;] for idx, animal in enumerate(animals): print &apos;#%d: %s&apos; % (idx + 1, animal) # 输出：&quot;#1: cat&quot;, &quot;#2: dog&quot;, &quot;#3: monkey&quot;, 一个一行 列表推导List comprehensions：避免通过循环等操作去对列表元素赋值nums = [0, 1, 2, 3, 4] even_squares = [x ** 2 for x in nums if x % 2 == 0] print even_squares # 输出：&quot;[0, 4, 16]&quot; 字典Dictionaries：文档d = {&apos;cat&apos;: &apos;cute&apos;, &apos;dog&apos;: &apos;furry&apos;} # 创建字典 print d[&apos;cat&apos;] # 输出：&quot;cute&quot; print &apos;cat&apos; in d # 检测字典中是否有&apos;cat&apos;的索引; 输出：&quot;True&quot; d[&apos;fish&apos;] = &apos;wet&apos; # 在字典中添加一个条目 print d[&apos;fish&apos;] # 输出：&quot;wet&quot; # print d[&apos;monkey&apos;] # KeyError: &apos;monkey&apos; not a key of d print d.get(&apos;monkey&apos;, &apos;N/A&apos;) # Get an element with a default; 输出：&quot;N/A&quot; print d.get(&apos;fish&apos;, &apos;N/A&apos;) # Get an element with a default; 输出：&quot;wet&quot; del d[&apos;fish&apos;] # 把&apos;fish&apos;:&apos;wet&apos;条目从字典中去掉 print d.get(&apos;fish&apos;, &apos;N/A&apos;) # 字典中没有fish这个索引了; 输出：&quot;N/A&quot; 循环Loops d = {&apos;person&apos;: 2, &apos;cat&apos;: 4, &apos;spider&apos;: 8} for animal in d: legs = d[animal] print &apos;A %s has %d legs&apos; % (animal, legs) # 输出：&quot;A person has 2 legs&quot;, &quot;A spider has 8 legs&quot;, &quot;A cat has 4 legs&quot; d = {&apos;person&apos;: 2, &apos;cat&apos;: 4, &apos;spider&apos;: 8} for animal, legs in d.iteritems(): print &apos;A %s has %d legs&apos; % (animal, legs) # 输出：&quot;A person has 2 legs&quot;, &quot;A spider has 8 legs&quot;, &quot;A cat has 4 legs&quot; 字典推导Dictionary comprehensions：与列表类似nums = [0, 1, 2, 3, 4] even_num_to_square = {x: x ** 2 for x in nums if x % 2 == 0} print even_num_to_square # 输出：&quot;{0: 0, 2: 4, 4: 16}&quot; 集合Sets：文档 animals = {&apos;cat&apos;, &apos;dog&apos;} # 创建集合 print &apos;cat&apos; in animals # 检测集合中是否有&apos;cat&apos;; 输出：&quot;True&quot; print &apos;fish&apos; in animals # 输出：&quot;False&quot; animals.add(&apos;fish&apos;) # 往集合里加&apos;fish&apos; print &apos;fish&apos; in animals # 输出：&quot;True&quot; print len(animals) # 集合元素数量; 输出：&quot;3&quot; animals.add(&apos;cat&apos;) # 往集合中添加已有的元素，什么都不会做 print len(animals) # 输出：&quot;3&quot; animals.remove(&apos;cat&apos;) # 从集合中移除&apos;cat&apos; print len(animals) # 输出：&quot;2&quot; 元组Tuples：文档d = {(x, x + 1): x for x in range(10)} # Create a dictionary with tuple keys print d t = (5, 6) # Create a tuple print type(t) # Prints &quot;&lt;type &apos;tuple&apos;&gt;&quot; print d[t] # Prints &quot;5&quot; print d[(1, 2)] # Prints &quot;1&quot; # 可以作为字典的索引，也可以作为集合的元素 Numpy 文档 科学计算库 创建数组 文档a = np.array([1, 2, 3]) b = np.array([[1,2,3],[4,5,6]]) a = np.zeros((2,2)) # Prints &quot;[[ 0. 0.] # [ 0. 0.]]&quot; b = np.ones((1,2)) # Prints &quot;[[ 1. 1.]]&quot; c = np.full((2,2), 7) # Prints &quot;[[ 7. 7.] # [ 7. 7.]]&quot; d = np.eye(2) # Prints &quot;[[ 1. 0.] # [ 0. 1.]]&quot; e = np.random.random((2,2)) # Might print &quot;[[ 0.91940167 0.08143941] # [ 0.68744134 0.87236687]]&quot; 访问数组 文档 切片 a = np.array([[1,2,3,4], [5,6,7,8], [9,10,11,12]]) # [[ 1 2 3 4] # [ 5 6 7 8] # [ 9 10 11 12]] b = a[:2, 1:3] # [[2 3] # [6 7]] # 修改b数组也会改变a数组 row_r1 = a[1, :] # &quot;[5 6 7 8] (4,)&quot; row_r2 = a[1:2, :] # &quot;[[5 6 7 8]] (1, 4)&quot; # row_r1会导致得到的数组比原数组阶数低 整形数组 a = np.array([[1,2], [3, 4], [5, 6]]) print a[[0, 1, 2], [0, 1, 0]] # 与np.array([a[0, 0], a[1, 1], a[2, 0]])等价 a = np.array([[1,2,3], [4,5,6], [7,8,9], [10, 11, 12]]) b = np.array([0, 2, 0, 1]) print a[np.arange(4), b] # 输出：&quot;[1 6 7 11]&quot;，即a[0, 0]; a[1, 2]; a[2, 0]; a[3, 1] 布尔数组访问a = np.array([[1,2], [3, 4], [5, 6]]) bool_idx = (a &gt; 2) # Prints &quot;[[False False] # [ True True] # [ True True]]&quot; print a[bool_idx] # Prints &quot;[3 4 5 6]&quot;；等价于a[a &gt; 2] x.dtype查看数据类型 文档 数据计算 函数文档；数组操作文档 x = np.array([[1,2],[3,4]], dtype=np.float64) y = np.array([[5,6],[7,8]], dtype=np.float64) print np.add(x, y) print np.subtract(x, y) print np.multiply(x, y) # 逐个相乘，不是矩阵乘法 print np.divide(x, y) print np.sqrt(x) v = np.array([9, 10]) w = np.array([11, 12]) print v.dot(w) # 向量内积；等价于np.dot(v, w) print x.dot(y) # 矩阵乘法 print np.sum(x) # 1+2+3+4=10 print np.sum(x, axis=0) # 计算每列的sum; &quot;[4, 6]&quot; print np.sum(x, axis=1) # 计算每行的sum; &quot;[3, 7]&quot; print x.True # 转置 广播Broadcasting：让不同大小的矩阵直接在一起进行数学计算 文档 import numpy as np x = np.array([[1,2,3], [4,5,6], [7,8,9], [10, 11, 12]]) v = np.array([1, 0, 1]) y = x + v # 使用广播把v加到x的每一行中 print y # &quot;[[ 2 2 4] # [ 5 5 7] # [ 8 8 10] # [11 11 13]]&quot; 广播规则 如果数组的秩不同，使用1来将秩较小的数组进行扩展，直到两个数组的尺寸的长度都一样。 如果两个数组在某个维度上的长度是一样的，或者其中一个数组在该维度上长度为1，那么我们就说这两个数组在该维度上是相容的。 如果两个数组在所有维度上都是相容的，他们就能使用广播。 如果两个输入数组的尺寸不同，那么注意其中较大的那个尺寸。因为广播之后，两个数组的尺寸将和那个较大的尺寸一样。 在任何一个维度上，如果一个数组的长度为1，另一个数组长度大于1，那么在该维度上，就好像是对第一个数组进行了复制。 广播例子 import numpy as np v = np.array([1,2,3]) # v has shape (3,) w = np.array([4,5]) # w has shape (2,) print np.reshape(v, (3, 1)) * w # 矩阵乘法 # [[ 4 5] # [ 8 10] # [12 15]] # 首先v变成(3,1)，而w是(2,)，即(1,2)，两者广播运算后，维度为(3,2) x = np.array([[1,2,3], [4,5,6]]) print x + v # [[2 4 6] # [5 7 9]] # x是(2,3)，v是(3,)/(1,3)，广播后为(2,3) print (x.T + w).T # [[ 5 6 7] # [ 9 10 11]] # x.T为(3,2)，w为(1,2)，相加后为(3,2)，再转置得到(2,3) print x + np.reshape(w, (2, 1)) # 和上面的表达式等价，把w变形后可以直接广播相加 print x * 2 # [[ 2 4 6] # [ 8 10 12]] # 对矩阵乘一个常数 Scipy 文档 图像操作 from scipy.misc import imread, imsave, imresize # Read an JPEG image into a numpy array img = imread(&apos;assets/cat.jpg&apos;) print img.dtype, img.shape # Prints &quot;uint8 (400, 248, 3)&quot; # We can tint the image by scaling each of the color channels # by a different scalar constant. The image has shape (400, 248, 3); # we multiply it by the array [1, 0.95, 0.9] of shape (3,); # numpy broadcasting means that this leaves the red channel unchanged, # and multiplies the green and blue channels by 0.95 and 0.9 # respectively. img_tinted = img * [1, 0.95, 0.9] # Resize the tinted image to be 300 by 300 pixels. img_tinted = imresize(img_tinted, (300, 300)) # Write the tinted image back to disk imsave(&apos;assets/cat_tinted.jpg&apos;, img_tinted) Matlab文件读写(scipy.io.loadmat/scipy.io.savemat) 文档 点之间的距离 文档 import numpy as np from scipy.spatial.distance import pdist, squareform # Create the following array where each row is a point in 2D space: # [[0 1] # [1 0] # [2 0]] x = np.array([[0, 1], [1, 0], [2, 0]]) print x # Compute the Euclidean distance between all rows of x. # d[i, j] is the Euclidean distance between x[i, :] and x[j, :], # and d is the following array: # [[ 0. 1.41421356 2.23606798] # [ 1.41421356 0. 1. ] # [ 2.23606798 1. 0. ]] d = squareform(pdist(x, &apos;euclidean&apos;)) print d Matplotlib 作图库 # [plot文档](https://matplotlib.org/api/pyplot_api.html#matplotlib.pyplot.plot) import numpy as np import matplotlib.pyplot as plt # Compute the x and y coordinates for points on a sine curve x = np.arange(0, 3 * np.pi, 0.1) y_sin = np.sin(x) y_cos = np.cos(x) # Plot the points using matplotlib plt.plot(x, y_sin) plt.plot(x, y_cos) plt.xlabel(&apos;x axis label&apos;) plt.ylabel(&apos;y axis label&apos;) plt.title(&apos;Sine and Cosine&apos;) plt.legend([&apos;Sine&apos;, &apos;Cosine&apos;]) plt.show() # 使用subplot在一张图中画多幅图；[subplot文档](https://matplotlib.org/api/pyplot_api.html#matplotlib.pyplot.subplot) import numpy as np import matplotlib.pyplot as plt # Compute the x and y coordinates for points on sine and cosine curves x = np.arange(0, 3 * np.pi, 0.1) y_sin = np.sin(x) y_cos = np.cos(x) # Set up a subplot grid that has height 2 and width 1, # and set the first such subplot as active. plt.subplot(2, 1, 1) # Make the first plot plt.plot(x, y_sin) plt.title(&apos;Sine&apos;) # Set the second subplot as active, and make the second plot. plt.subplot(2, 1, 2) plt.plot(x, y_cos) plt.title(&apos;Cosine&apos;) # Show the figure. plt.show() # 绘制双Y轴曲线图 x = np.arange(0., np.e, 0.01) y1 = np.exp(-x) y2 = np.log(x) fig = plt.figure() ax1 = fig.add_subplot(111) ax1.plot(x, y1) ax1.set_ylabel(&apos;Y values for exp(-x)&apos;) ax1.set_title(&quot;Double Y axis&quot;) ax2 = ax1.twinx() # this is the important function ax2.plot(x, y2, &apos;r&apos;) ax2.set_xlim([0, np.e]) ax2.set_ylabel(&apos;Y values for ln(x)&apos;) ax2.set_xlabel(&apos;Same X for both exp(-x) and ln(x)&apos;) plt.show() # 图像读入与显示(imread/imwrite) import numpy as np from scipy.misc import imread, imresize import matplotlib.pyplot as plt img = imread(&apos;assets/cat.jpg&apos;) img_tinted = img * [1, 0.95, 0.9] # Show the original image plt.subplot(1, 2, 1) plt.imshow(img) # Show the tinted image plt.subplot(1, 2, 2) # A slight gotcha with imshow is that it might give strange results # if presented with data that is not uint8. To work around this, we # explicitly cast the image to uint8 before displaying it. plt.imshow(np.uint8(img_tinted)) plt.show()]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>计算机视觉</tag>
        <tag>深度学习</tag>
        <tag>公开课</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Daily Reading]]></title>
    <url>%2F2018%2F02%2F10%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%2FDaily-Reading%2F</url>
    <content type="text"><![CDATA[记录一下每天看的公众号和知乎相关内容 2017-12-261. 十大机器学习Python库(机器之心) 大部分都没见过，可以看看 2017-12-271. 如何在NLP领域干成第一件事(AI科技评论) 找开源项目并重现，争取进一步改善；找如ACL会议论文集，找感兴趣的方向8，了解该方向的方法/数据/团队； 2. Python相关文章(AI科技评论)* 静态代码检查工具Flake；视频合成器；从零开始用遗传算法和深度学习演化有机体的生长过程；破解验证码；Chutes&amp;Ladders游戏模拟；创建Chatbot；图像散列；介绍BP；Memorization技术，加速Python；分享Python学习技巧 3. Attention模型(机器学习算法与自然语言处理) 各种Attention机制的综述 4. 神经进化策略(机器之心)* 可能未来使用神经进化策略替代反向传播，了解一下 2017-12-281. CS中的线代(机器之心)* 一篇普渡的论文，45页 2. 2018AI研究趋势(机器之心)** 汇总资源：开放资源(介绍了DeepMind/OpenAI等的博客，着重推荐Distill)；机器翻译；理解视频(预测下一帧等+数据集介绍)；多任务/多模式学习；强化学习(游戏Dota/星际争霸)；解释人工智能(可视化/InterpretNet)；保护人工智能被对抗样本愚弄；超越梯度(合成梯度/进化策略/SGD改进/学习优化/不同空间的优化)；3D和图形的几何深度学习 3. FoolNLTK：一个便捷的中文处理工具(机器之心) 基于BiLSTM的开源中文分词模型 4. NIPS 2017的收获与思考(AI科技评论) 3篇Best paper：博弈论相关，打败德州扑克顶级玩家；随机优化风险最小化问题；提出全新的拟合优度算法。都好高端，不知道讲的是什么这篇文章作者主要关注了强化学习； *还介绍了4篇MSRA的paper：有一篇关于机器翻译的论文可能可以用在LipNet项目中？ 5. 如何理解hekaiming的Focal Loss(PaperWeekly) 解决分类问题中类别不平衡、分类难度差异的一个loss 6. 草根学Python(机器学习研究会)** 寒假和笨方法学Python一起看 2017-12-291. 2017最火的五篇深度学习论文(专知) CycleGAN-图像迁移；Wasserstein GAN-提出更好的用于训练GAN的目标函数；simGAN-产生模拟数据，使用未标记的真实数据来改进模拟数据(无监督的)AlphaGo zero-无人类知识先验的情况下学会下围棋深度图像先验-理解神经网络模型中先验的作用(没搞懂) 2. 机器学习、NLP、Python和Math最好的150余个教程(AI科技大本营)** 资源汇总 2017-12-301. ICCV2017: 基于检测和跟踪的视频中人体姿态估计(专知)* 人体姿态估计相关 2. 如何与深度学习服务器优雅的交互(夕小瑶)** 服务器相关操作 3. 机器之心年度盘点：2017年人工智能领域备受关注的科研成果(机器之心) AlphaGo-无须人类知识标注，自我对抗德州扑克击败人类-深度学习/纳什均衡的博弈求解自归一化-比BN更好的归一化GAN和各种变体深度神经网络碰上语音合成-WaveNet大批量数据并行训练ImageNet-分布式同步SGD训练(将ResNet-50在ImageNet上的训练时间缩短到48分钟)Capsule-抛弃反向传播递归皮质网络-新型概率生成模型，旨在超越神经网络从TPU到NPU-NPU是华为麒麟970手机端的芯片 4. 机器学习非凸优化技术(机器之心)5. 在线深度学习：在数据流中实时学习深度神经网络(机器之心) 2018-02-101. 想要实现深度神经网络？一张 Excel 表格就够了(机器之心) 链接 有一个比较完整的CNN流程，关于CNN的一些理解：链接 2. Facebook提出DensePose数据集和网络架构：可实现实时的人体姿态估计(机器之心)* 链接 人体姿态估计相关 2018-02-111. 从语义上理解卷积核行为，UCLA朱松纯等人使用决策树量化解释CNN(机器之心)* 链接 解释CNN模型：借助决策树在语义层面上解释 CNN 做出的每一个特定预测，即哪个卷积核（或物体部位）被用于预测最终的类别，以及其在预测中贡献了多少。斯坦福也有类似的工作。 2. 李沐《动手学深度学习》课程视频汇总(机器之心) 链接 使用Apache MXNet的最新前端Gluon作为开发工具。视频链接；中文文档 3. 一文读懂什么是变分自编码器(专知) 链接 AE到VAE 2018-02-121. 机器学习的经典算法 链接 回归算法：线性回归(数值问题，最小二乘法，使用梯度下降逼近求解函数极值问题) &amp; 逻辑回归(分类问题，结果中加一个Sigmoid函数) 用于拟合逻辑回归中非线性分类线的两种算法：神经网络(ANN) &amp; SVM*(和高斯“核”结合，表达复杂的分类界限。核最典型的特征就是可以将低维的空间映射到高维空间，低维空间的非线性分类线就相当于高维空间的线性分类线) 聚类算法：无监督的一种，K-Means。见3 降维算法：无监督的一种，PCA* 推荐算法：基于物品内容的推荐（将与用户购买的内容近似的物品推荐给用户） &amp; 基于用户相似度的推荐（将与目标用户兴趣相同的其他用户购买的东西推荐给目标用户），协同过滤算法* 其他：高斯判别、朴素贝叶斯、决策树 2. ​爬虫与反爬虫 链接 有趣的爬虫与反爬虫工作介绍 3. 六大聚类算法 链接 参看博文六大聚类算法 4. 最新云端&amp;单机GPU横评 链接 性能：Volta性能优于Nvidia 1080Ti（约1.1-1.3倍）和P100（约1.2-1.5倍） 成本：Paperspace Volta性价比高。Google P100比Paperspace Volta贵10%，Amazon Volta比Paperspace Volta贵40% Paperspace Volta适合只需要1个GPU的用户，性能好 Google P100最为灵活，它允许用户在任意实例上使用1、2、4个P100 GPU（或最多 8个K80 GPU），允许用户自定义CPU和GPU配置来满足计算需求。尽管由于架构所限，Tesla P100的性能略显落后，但从成本角度考虑，其性价比很有优势 Amazon Volta性能优于Google P100，也可以连接1、4或8个GPU。但用户无法自定义基础实例类型。性价比比较低。如果迫切需要用8个GPU或在EC2上搭建模型，那么目前仍推荐使用Amazon Volta 5. 理解深度学习中的矩阵运算* 链接 论文链接 6. 最新7篇VAE相关论文 链接 -利用带混合解码器的条件变分自编码器生成主题汉语诗歌 一种使用条件变分自编码器的Zero Shot Learning的生成模型 变分自编码器在不同模态之间的双向生成 使用深度密度先验的MR图像重建 变分递归神经机器翻译 变分自编码器的推断次优性 使用条件VAEs和GANs从视觉属性中合成人脸 7. UC Berkeley提出特征选择新方法：条件协方差最小化* 链接 模型解释性。这篇论文的综述部分可以看一下，论文本身数学性比较强，Github 降维可以增强模型的可解释性，特征选择则是常用的降维方法。特征选择算法通常可分为：滤波器（filter）、封装（wrapper）以及嵌入（embedded）。 滤波器方法基于数据的本质属性选择特征，与所用的学习算法无关。如可以计算每个特征和响应变量之间的相关性，然后选择相关性最高的变量 封装方法的目标是寻找能够使某个预测器的性能最优化的特征。如可以训练多个支持向量机，每个支持向量机使用不同的特征子集，然后选择在训练数据上损失最小的特征子集。因为特征子集的数量是指数规模的，所以封装方法通常会使用贪心算法 嵌入方法将特征选择和预测结合成一个问题。它通常会优化一个目标函数，这个目标函数结合了拟合优度和对参数数量的惩罚。一个例子就是构建线性模型的LASSO方法，它用L1 penalty来表征对参数数目的惩罚 本文提出了条件协方差最小化（CCM）方法，这是一个统一前两个观点的特征选择方法。这个方法基于最小化条件协方差算子的迹来进行特征选择。思想是选择能够最大化预测基于协变量响应依赖的特征。 8. 深度学习的7大实用技巧* 链接 参看博文深度学习技巧 2018-02-131. 机器学习中的模型评价、模型选择及算法选择* 链接 参看博文机器学习中的模型评价与选择 2. 波士顿动力的机器人会开门了，中国尚需奋力追赶 链接 里面的几个视频非常有趣 3. R语言入门指导 链接 比较细比较初级的教程 4. 谷歌云TPU服务正式全面开放 链接 张量处理器（Tensor Processing Unit，TPU）是机器学习专属芯片，去年又推出了第二代产品（Cloud TPU）。TPU论文 谷歌云平台（GCP）提供Cloud TPU beta版自2018年2月12日起用，旨在帮助机器学习专家更快地训练和运行ML模型 云TPU如今在数量受限的情况下可用，价格以秒计费，大约为每云TPU每小时6.50美元 2018-02-141. 多维(1-6)数据可视化策略* 链接 -可视化资源：pandas、matplotlib、seaborn、plotly、bokeh库；D3.js；The Visual Display of Quantitative Information.pdf -教程的Github资源 数据集：UCI机器学习库中的Wine Quality Data Set，分为葡萄酒中红色和白色酒 框架：matplotlib和seaborn 一维数据：利用pandas画直方图、核密度图等 二维数据：配对相关性矩阵（pair-wise correlation matrix）并将其可视化为热力图；在感兴趣的属性之间使用配对散点图；平行坐标图；散点图；联合分布图等等 多维数据：高于3维的最好方法是使用图分面、颜色、形状、大小、深度等。还可以使用时间作为维度，为随时间变化的属性制作一段动画。每多一个维度，就使用一种新的表征，如色调、深度、大小等 2018-02-151. 从LeNet到SENet——卷积神经网络回顾** 链接 2018-02-161. 2017年机器之心AI高分概述文章全集** 链接 去年机器之心的盘点：链接 非常好的一份教程全集，有时间多看看 2. 谷歌大脑Wasserstein自编码器：新一代生成模型算法 链接 ICLR 2018. 具有VAE的一些优点，也结合了GAN结构的特性，可以实现更好的性能 3. 机器学习研究的12个宝贵经验 链接 关于机器学习的一些经验分享，非调参经验 2018-02-171. 区块链vs传统数据库：分布式运行有何优势 链接 区块链是一种容错率很高的分布式数据存储模式 去中心化控制消除了中心化控制的风险。任何能够充分访问中心化数据库的人都可以摧毁或破坏其中的数据，因此用户依赖于数据库管理员的安全基础架构。区块链技术使用去中心化数据存储来避开这一问题，从而在自己的结构中建立安全性。区块链技术很适合记录某些种类的信息，传统数据库更适合记录另外一些种类的信息。对于每个组织而言，理解它想从数据库中获得什么非常关键，我们需要在选择数据库之前，判断每种数据库的优缺点 2018-02-181. 胶囊网络为何如此热门 链接 CapsNets需要的训练数据少； CapsNets能更好地处理图像多义性表达 CapsNets具有“同变性”，输入的微小变化会导致输出的细微变化。即详细的姿态信息在整个网络中都被保留 CNNs需要额外的组件来实现自动识别一个部件归属于哪一个对象（如，这条腿属于这只羊）。而CapsNets则免费提供部件的层次结构 CapsNets在如CIFAR10或ImageNet大规模图像测试集上的表现不如CNNs好；需要大量计算，不能检测出相互靠近的同类型的两个对象（这被称为“拥挤问题”，且已被证明人类也存在这个问题） CapsNets的主要思想还是非常有前途的，似乎只需要一些调整就可以发挥全部潜力 2018-02-191. 神经网络“剪枝”的两个方法 链接 一般用于减少网络非零参数数量的方法有三种： (1) 正则化(regularization)：修改目标函数，如使用L0范数 论文(2) 修剪(pruning)：面向大规模神经网络，并删除某些意义上冗余的特征或参数 论文(3) 增长(growing)：从小型网络开始，按某种增长标准逐步增加新的单元 减少非零参数的目的如下： (1) 保持相同性能的前提下降低计算成本，加速推断和训练(2) 减少参数数量可以减少参数空间的冗余，从而提高泛化能力 这篇文章介绍了两篇近期关于神经网络修剪的论文，分别是 L_0 正则化方法和 Fisher 修剪方法 2018-02-201. 从1400篇机器学习文章中精选出Top 10 链接 Google Brain去年干了太多事，Jeff Dean一篇长文都没回顾完 一文详解如何使用Python和Keras构建属于你的 AlphaZero AI 通过概况长序列来生成维基百科 深度学习必备的矩阵微积分知识 一种值得采纳的全局优化算法 Tensorflow-Project-Template：tensorflow项目模板架构的最佳实践 如何解决90％的自然语言处理问题：分步指南奉上 CheXNet：一次深入的回顾 机器学习新手顶级算法之旅 学习数据科学、机器学习与AI没有多大交集，一文告诉你三者最大区别 2. 区块链技术综述* 链接 文章通过解构区块链的核心要素，提出了区块链系统的基础架构模型，详细阐述了区块链及与之相关的比特币的基本原理、技术、方法与应用现状，讨论了智能合约的理念、应用和意义 3. 超越Adam，从适应性学习率家族出发解读ICLR 2018高分论文 链接 讨论了 Adam 等适应性学习率算法的收敛性缺点，并提出了一种新的 Adam 变体 SGD 的一类变体通过使用历史梯度某种形式的范数而调整学习率取得了很大的成功，因为它们能针对不同的参数采用不同的学习率。一般来说，适应性学习率算法的基本思想是若损失函数对于某个给定模型参数的偏导保持相同的符号，那么学习率应该增加 4. Git 的4个阶段的撤销更改 链接 工作流分为工作区、暂存区、本地仓库、远程仓库 git add . 把所有文件放入暂存区；git commit 把所有文件从暂存区提交进本地仓库；git push把所有文件从本地仓库推送进远程仓库 git diff / git diff –cached / git diff master origin/master查看目前在工作流的哪个状态 撤销 (1) 编辑器中修改了文件，还未执行git add .：git checkout .或git reset –hard(2) 执行了git add .，但还没有执行git commit：git reset-&gt;git checkout .或直接git reset –hard(3) git commit也执行了，但还没有push：git reset –hard origin/master(既然污染了本地仓库，那就从远程仓库复制到本地)(4) push也执行了：git reset –hard HEAD^ -&gt; git push -f(先恢复本地仓库，再push到远程仓库) 5. 最流行的神经网络变体综述* 链接 最流行的神经网络变体综述 2018-02-211. 如何用人工智能帮你找论文 链接 介绍了一款论文检索引擎arXiv-sanity，作者Andrej Karpathy 2018-02-221. ICLR 2018 | 阿姆斯特丹大学论文提出球面CNN：可用于3D模型识别和雾化能量回归 提出球面CNN的神经网络，用于检测球面图像上任意旋转的局部模式 2018-02-231. 像玩乐高一样拆解Faster R-CNN：详解目标检测的实现过程 链接2. 深度学习实验流程及 PyTorch 提供的解决方案 链接3. CVPR2018|DiracNets：无需跳层连接，训练更深神经网络，结构参数化与Dirac参数化的ResNet 链接4. 如何在手机上使用TensorFlow 链接 一篇关于手机上训练DL模型的教程 2018-03-011. 谷歌机器学习课程 链接 课程链接 2018-03-051. 弱监督学习综述 链接 三种弱监督：不完全监督：只有一部分训练数据具备标签；不确切监督：训练数据只具备粗粒度标签；不准确监督：给出的标签并不总是真值 2018-03-201. AAAI 2018 行为识别论文概览 链接 AAAI 2018的Action Recognition 2018-03-211. 腾讯AI Lab的21篇CVPR 2018 链接 End-to-End Learning of Motion Representation for Video Understanding (面向视频理解的端到端动作表示学习)]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>论文阅读</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Pytorch学习]]></title>
    <url>%2F2018%2F02%2F10%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%2FPytorch%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[记录一下Pytorch学习过程以及踩过的一些坑 Pytorch官方文档 Pytorch官方教程 中文文档 由于教程中有太多的代码了，因此这里就只记录看过的内容的索引，方便以后自己写代码的时候回来找 60分钟教程 链接What is Pytorch 介绍Tensors 对于Tensor的操作 view改变Tensor维度 Tensor和Numpy的相互转化 Torch的Tensor和Numpy数组使用的是同一块内存空间，改变其中一个也会影响另一个（指的是Numpy数组的内容是用a.numpy()赋值给b的情况） 把Tensor移到GPU Autograd: automatic differentiation（用于反向传播） Variable Variable是核心类，包含Tensor并支持在Tensor上的一切操作。记录了一系列操作.data访问数据；.grad访问梯度(如x.grad访问的是目标函数对于x的梯度)；.grad_fn涉及创建该变量的函数(如b=a+2，那么b.grad_fn就是a+2这个函数)，若变量是人为直接定义的，那.grad_fn就是None Gradients If you want to compute the derivatives, you can call .backward() on a Variable. If Variable is a scalar (i.e. it holds a one element data), you don’t need to specify any arguments to backward(), however if it has more elements, you need to specify a grad_output argument that is a tensor of matching shape.上面那段话在教程中相关的例子： out.backward() # is equivalent to doing out.backward(torch.Tensor([1.0])) gradients = torch.FloatTensor([0.1, 1.0, 0.0001]) y.backward(gradients) # 相当于求得的梯度中，不同维度的值会乘上0.1/1.0/0.0001 Neural Networks（搭建网络） Define the network 只需要定义forward，backward会自动计算获取参数：params = list(net.parameters())；访问参数值：net.conv1.weight/bias零初始化梯度：net.zero_grad Loss Function &amp; Backprop 查看梯度信息（图）：loss.grad_fn / loss.grad_fn.next_functions[0][0] (print输出以后是MseLossBackward object at 0x7fe4c18539e8)查看梯度值：net.conv1.bias.grad Update the weights 不用手动实现weight的更新，直接调用torch.optim中的优化算法SGD、Adam等softmax包含在CrossEntropyLoss里面了 使用param_groups访问优化器中的参数，如学习率(optimizer.param_groups[‘lr’])等 链接 Training a classifier（完整的一个训练例子） What about data? 把数据利用各种库导入到numpy array，再转化到torch的Tensor对于computer vision，Pytorch有一个torchvision库，里面有针对ImageNet、mnist等数据集的dataloader和data transformer Data Parallelism（多GPU并行计算） Data Loading and Processing Tutorial 链接Dataset class 必须继承Dataset(torch.utils.data.Dataset) 必须覆盖__len__和__getitem__两个方法，getitem的使用方式就是a[i]；len的使用方式是len(dataset) 给的example的数据集有一个csv和一堆图片，csv中有图片的名称。建议构造函数(__init__)中只读csv文件(相当于索引)，具体对图片的读取放到__getitem__中，按需要读取 Transforms dataset中transform参数可选，根据需求可以自己写一些变换，如改变图像大小、data augmentation等 tsfm = Transform(params) transformed_sample = tsfm(sample) Iterating through the dataset torch.utils.data.DataLoader是一个迭代器，提供batching the data、shuffling the data、load the data in parallel功能 Transfer Learning tutorial 链接 Finetuning the convnet：先在ImageNet上训练，再在自己的数据集上对该模型进行微调 ConvNet as fixed feature extractor：把FC层之前的都当作一个固定的特征提取器，只调整最后几层全连接层 划分了train和val集 若不要更新，则.requires_grad = False Visualization 链接 坑 x.view(-1, 8)是改变x的维度，如原来x是16*1，有16个元素，使用view(-1, 8)后为了保证还是16个元素，-1那一维就是16/8=2 torch.nn只支持mini-batches，即所有的图像数据格式必须是batch * channel * height * width，因为nn.Conv2d的输入需要是这样的，如果是FC，那么可能不需要channel那一维。但batch维必须要有，如果只有一个batch，那就用input = input.unsqueeze(0)添加batch维 训练的循环里，要把Tensor转换成Variable之后再送进网络，即inputs = Variable(inputs) 若要用GPU训练，需要net.cuda()以及inputs = Variable(inputs.cuda())，即把网络和输入输出都放到GPU当中 似乎把numpy转成Tensor后，还要指定是FloatTensor，即input = input.type(torch.FloatTensor) numpy的图像矩阵储存格式是H * W * C，而Tensor是C * H * W，转换的时候要image = image.transpose((2, 0, 1)) conv1D卷积核的输入维度是Batch_size * Channels * H * W；Linear全连接的输入维度是Batch_size * 维度 label是从0开始计数，注意自己的dataset中的label值 pytorch的CrossEntropyLoss不含正则项，且是平均loss，求总loss的时候还要乘上batch_size（手算验证过） 用matplotlib绘制曲线时，注意x和y都需要是numpy的，不然就np.array(loss)[x] 问题 若数据集太小，小于Batch_size，有没有办法能一次把数据集读2遍？好像dataloader不支持 优质资源 关于参数初始化和Finetune 链接]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>Pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Welcome to my blog]]></title>
    <url>%2F2018%2F02%2F09%2F%E9%9A%8F%E7%AC%94%2FWelcome-to-my-blog%2F</url>
    <content type="text"><![CDATA[一直想搞个博客，但是一直懒得搞，拖延癌晚期/(ㄒoㄒ)/~~。。之前搞过一个jekyll的，这次想了想还是换hexo了 搭建细节主要参考了http://blog.csdn.net/gdutxiaoxu/article/details/53576018 大致流程如下： 安装node.js 本地安装Hexo 配置Hexo 把Hexo和Github Page联系 随后就是换主题、搞一下私人定制的事情啦，接下来几天慢慢看看官方文档，把这个博客做的好看一点o(￣▽￣)ブ 2018-02-10 update新添站内搜索功能、生成站点地图功能，参考https://www.ezlippi.com/blog/2017/02/hexo-search.html**删除安装的插件直接npm unistall 插件名即可根据官方文档通过修改_config.yml添加了一系列功能 2018-02-12 update在博文中添加图片，参考链接在博文中的写法：Markdown中写注释，参考链接 待添加：评论功能(next继承来必力)；http://blog.csdn.net/qq_33699981/article/details/72716951To be continued…]]></content>
      <categories>
        <category>随笔</category>
      </categories>
      <tags>
        <tag>随笔</tag>
      </tags>
  </entry>
</search>
