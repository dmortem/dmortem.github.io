<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[资源汇总]]></title>
    <url>%2F2018%2F02%2F17%2F%E8%B5%84%E6%BA%90%E6%B1%87%E6%80%BB%2F</url>
    <content type="text"><![CDATA[记录了各个地方看到的资源整合，供日后查阅 Zero to Hero：2017年机器之心AI高分概述文章全集 链接入门 深度 | David Silver 全面解读深度强化学习：从基础概念到 AlphaGo 深度 | 迁移学习全面概述：从基本概念到相关研究 深度 | 理解深度学习中的卷积 综述 | 知识图谱研究进展 盘点 | 机器学习入门算法：从线性模型到神经网络 深度神经网络全面概述：从基本概念到实际模型和硬件基础 机器理解大数据的秘密：聚类算法深度详解 想了解概率图模型？你要先理解图论的基本定义与形式 人工智能能骗过人类吗？愚人节特写：这不是玩笑 三张图读懂机器学习：基本概念、五大流派与九种常见算法 LSTM 入门必读：从基础知识到工作方式详解 从语言学到深度学习 NLP，一文概述自然语言处理 最全的 DNN 概述论文：详解前馈、卷积和循环神经网络技术 从贝叶斯定理到概率分布：综述概率论基本定义 追根溯源！一图看尽深度学习架构谱系 徒手实现 CNN：综述论文详解卷积网络的数学本质 读懂概率图模型：你需要从基本概念和参数估计开始 从零开始：教你如何训练神经网络 开发者必读：计算机科学中的线性代数 理论机器学习 学界 | 定量研究：当前机器学习领域十大研究主题 机器学习和深度学习引用量最高的 20 篇论文（2014-2017） 从贝叶斯角度，看深度学习的属性和改进方法 良心 GitHub 项目：各种机器学习任务的顶级结果（论文）汇总 深度 | 从朴素贝叶斯到维特比算法：详解隐马尔科夫模型 揭秘深度学习成功的数学原因：从全局最优性到学习表征不变性 深度 | 从 AlexNet 到残差网络，理解卷积神经网络的不同架构 从 Pix2Code 到 CycleGAN：2017 年深度学习重大研究进展全解读 前沿方向 OpenAI 详解进化策略方法：可替代强化学习 从自编码器到生成对抗网络：一文纵览无监督学习研究现状 资源 | 从文本到视觉：各领域最前沿的论文集合 从决策树到随机森林：树型算法的原理与实现 从概率论到多分类问题：综述贝叶斯统计分类 从遗传算法到 OpenAI 新方向：进化策略工作机制全解 GAN 综述 | 一文帮你发现各种出色的 GAN 变体 资源 | 生成对抗网络及其变体的论文汇总 生成对抗网络综述：从架构到训练技巧，看这篇论文就够了 计算机视觉 干货 | 物体检测算法全概述：从传统检测方法到深度神经网络框架 重磅 | 自动驾驶计算机视觉研究综述：难题、数据集与前沿成果（附 67 页论文下载） 神经风格迁移研究概述：从当前研究到未来方向（附论文和代码） 深度学习目标检测模型全面综述：Faster R-CNN、R-FCN 和 SSD 计算机视觉这一年：这是最全的一份 CV 技术报告 计算机视觉这一年：2017 CV 技术报告 Plus 之卷积架构、数据集与新趋势 深度 | 2017 CV 技术报告之图像分割、超分辨率和动作识别 深度 | 2017CV 技术报告：从 3D 物体重建到人体姿态估计 自然语言处理 语音合成到了跳变点？深度神经网络变革 TTS 最新研究汇总 资源 | 从全连接层到大型卷积核：深度学习语义分割全指南 学界 | 词嵌入 2017 年进展全面梳理：趋势和未来方向 深度 | 一文概述 2017 年深度学习 NLP 重大进展与趋势 推荐系统 学界 | 一文综述所有用于推荐系统的深度学习方法 使用深度学习构建先进推荐系统：近期 33 篇重要研究概述 深度学习框架 业界｜谷歌 TensorFlow 的一份全面评估报告：好的坏的及令人讨厌的 初学者怎么选择神经网络环境？对比 MATLAB、Torch 和 TensorFlow 硬件 业界 | 剖析用于深度学习的硬件：GPU、FPGA、ASIC 和 DSP 神经形态计算与神经网络硬件最全调查：从研究全貌到未来前景 从 GPU、TPU 到 FPGA 及其它：一文读懂神经网络硬件平台战局 优化 从浅层模型到深度模型：概览机器学习优化算法 综述论文：当前深度神经网络模型压缩和加速方法速览 深度 | 从修正 Adam 到理解泛化：概览 2017 年深度学习优化算法的最新研究进展 一文概览深度学习中的五大正则化方法和七大优化策略 代码实现 从强化学习基本概念到 Q 学习的实现，打造自己的迷宫智能体 回归、分类与聚类：三大方向剖解机器学习算法的优缺点（附 Python 和 R 实现） 基于 TensorFlow 理解三大降维技术：PCA、t-SNE 和自编码器 一文读懂遗传算法工作原理（附 Python 实现） 10 大深度学习架构：计算机视觉优秀从业者必备（附代码实现） 从算法到训练，综述强化学习实现技巧与调试经验 2017 年度盘点：15 个最流行的 GitHub 机器学习项目 人工智能从入门到进阶，2016年机器之心高分技术文章全集 链接学习资源 深度学习资料大全：从基础到各种网络 30个深度学习库：按Python和C++等10种语言分类 2016年不可错过的21个深度学习视频、教程和课程 程序员实用深度学习免费课程:从入门到实践 9本不容错过的深度学习和神经网络书籍 深度学习专业名词表：从激活函数到word2vec 2016年年度十大Python库盘点 2010-2016年被引用次数最多的深度学习论文 自学数据科学&amp;机器学习，19个数学和统计学公开课推荐 Yoshua Bengio新书《Deep Learning》中文版开放预览（附PDF下载链接） 数据科学家应该掌握的12种机器学习算法（附信息图） 从入门到研究，人工智能领域最值得一读的20份资料 关于数据科学的十本好书 哈佛大学九大自然语言处理开源项目 ICML 2016演讲视频：数百个演讲带你读懂机器学习 Yoshua Bengio研究生科研指导演讲：解读人工智能全貌和下一个前沿 Yann LeCun演讲：人工智能的下一个前沿——无监督学习 今年GitHub排名前20的Python机器学习开源项目 机器学习工程师和数据科学家最应该读的16本书 任何阶段的学习者都适用的参考：机器学习领域书目全集 Yoshua Bengio深度学习暑期班学习总结，35个授课视频全部开放 Andrej Karpathy CS294课程总结：可视化和理解深度神经网络 Geoffrey Hinton最新演讲梳理：从人工神经网络到RNN应用 吴恩达NIPS 2016演讲现场直击：如何使用深度学习开发人工智能应用 NIPS 2016最全盘点：主题详解、前沿论文及下载资源 斯坦福大学周末学习盛宴：12位大牛解读深度学习 提升深度学习模型的表现，你需要这20个技巧 TensorFlow开源一周年：这可能是一份最完整的盘点 五大主流深度学习框架比较分析：MXNET是最好选择 基础介绍文章 2016伦敦深度学习峰会观感：人工智能面临的三大难题 2016机器学习与自然语言处理学术全景图：卡耐基梅隆大学排名第一 2016年美国机器人路线图出炉，最新机器人产业盘点 一张图看懂全球Bot布局 Yoshua Bengio：深度学习崛起带来人工智能的春天 神经网络架构演进史：全面回顾从LeNet5到ENet十余种架构 Andrej Karpathy：计算机科学博士的生存指南 贝叶斯神经网络简史 百度首席科学家吴恩达刊文：人工智能的能力和不足 高盛百页人工智能生态报告：美国仍是主导力量，中国正高速成长 伯克利教授Stuart Russell：人工智能基础概念与34个误区 初学者必读：解读14个深度学习关键词 智能时代每个人都应该了解：什么是深度学习？ CMU机器学习系负责人：人工智能与人类的未来是共生自主 CMU教授邢波：人工智能的路径、方向与未来 从供应链优化到差异化定价：机器学习十种方式变革制造业 对比深度学习十大框架：TensorFlow最流行但并不是最好 当AI遇上AR ——从微软HPU说起 洞悉AlphaGo超越围棋大师的力量：机器之心邀你一起强化学习 东南大学漆桂林教授：知识图谱不仅是一项技术，更是一项工程 人工智能全局概览：通用智能的当前困境和未来可能 访谈百度IDL林元庆：百度大脑如何在人脸识别上战胜人类「最强大脑」 TensorFlow 生态系统：与多种开源框架的融合 Kaggle创始人问答：深度学习会淘汰其他的机器学习方法吗？ 机器之心年度盘点 | 从技术角度，回顾2016年语音识别的发展 机器学习的基本局限性：从一个数学脑筋急转弯说起 人们都在说人工智能，其实现在我们真正做的是智能增强 人工智能、机器学习、深度学习，三者之间的同心圆关系 R vs Python：R是现在最好的数据科学语言吗？ 斯坦福NLP团队介绍交互式语言学习：从语言游戏到日程规划 斯坦福大学副教授Reza Zadeh：神经网络越深就越难优化 神经网络和深度学习简史（三）：强化学习与递归神经网络 神经网络和深度学习简史（四）：深度学习终迎伟大复兴 深度学习技术在股票交易上的应用研究调查 深度学习十大飙升趋势 深度学习入门，以及它在物联网和智慧城市中的角色 服务器端人工智能，FPGA和GPU到底谁更强？ Science「机器人子刊」创刊号，五大研究解读机器人领域最新进展 西红柿还是猕猴桃？一个案例帮你入门机器学习 详细解读神经网络十大误解，再也不会弄错它的工作原理 基于图的机器学习技术：谷歌众多产品和服务背后的智能 技术起点 10种深度学习算法的TensorFlow实现 数十种TensorFlow实现案例汇集：代码+笔记 机器学习入门必备：如何用Python从头实现感知器算法 ACM 最新月刊文章：强化学习的复兴 人工智能开发者的入门指南 从入门到精通：卷积神经网络初学者指南 机器学习敲门砖：任何人都能看懂的TensorFlow介绍 DeepMind提出的可微神经计算机架构的TensorFlow实现 RNN 怎么用？给初学者的小教程 深度学习教程：从感知器到深层网络 OpenAI 的 PixelCNN++实现：基于 Python的实现 Geoffrey Hinton最新演讲梳理：从人工神经网络到RNN应用 官方指南：如何通过玩TensorFlow来理解神经网络 一篇文章带你进入无监督学习:从基本概念到四种实现模型 NIPS 2016上22篇论文的实现汇集 机器学习初学者入门实践：怎样轻松创造高精度分类网络 解决真实世界问题：如何在不平衡类上使用机器学习？ 卷积神经网络架构详解：它与神经网络有何不同？ LSTM和递归网络基础教程 门外汉如何使用谷歌的Prediction API做机器学习 NVIDIA趣味解读：深度学习训练和推理有何不同？ 如何利用 Python 打造一款简易版 AlphaGo 如何用图像识别技术来变革商业？这里有份操作指南 MIT生成视频模型，预测静态图片的未来场景 EMNLP 2016干货：从原理到代码全面剖析可用于NLP的神经网络 使用机器学习翻译语言：神经网络和seq2seq为何效果非凡？ 神经网络快速入门：什么是多层感知器和反向传播？ 神经网络中激活函数的作用 逐层剖析，谷歌机器翻译突破背后的神经网络架构是怎样的？ 深度学习漫游指南：强化学习概览 深度学习与神经网络全局概览：核心技术的发展历程 最全的深度学习硬件指南 主流深度学习框架对比：看你最适合哪一款？ 继续进阶 40年认知架构研究概览：实现通用人工智能的道路上我们已走了多远？ 第四范式联合创始人陈雨强：机器学习在工业应用中的新思考 词嵌入系列博客Part1：基于语言建模的词嵌入模型 词嵌入系列博客Part2：比较语言建模中近似 softmax 的几种方法 词嵌入系列博客Part3：word2vec 的秘密配方 从分割到识别，全面解析Facebook开源的3款机器视觉工具 从硬件到软件：OpenAI 解读自家的深度学习基础架构 分布式深度学习：神经网络的分布式训练 Embedding 新框架模型：Exponential Family Embeddings FPGA vs. ASIC，谁将引领移动端人工智能潮流？ 概述性论文：卷积神经网络的近期研究进展 《Nature》 封面文章：人工智能引发材料科学变革 概述论文：迁移学习研究全貌 Andrej Karpathy：你为什么应该理解反向传播 GAN之父NIPS 2016演讲现场直击：全方位解读生成对抗网络的原理及未来 Google Brain 讲解注意力模型和增强RNN 谷歌Magenta项目是如何教神经网络编写音乐的？ 基于MXNet 的神经机器翻译实现 2016深度学习重大进展：从无监督学习到生成对抗网络 深度学习遇上基因组，诊断疾病和揭示深层生物原理或迎来突破 King+Woman-Man=Queen:用基于Spark的机器学习来捕捉词意 初学者必读:从迭代的五个层面理解机器学习 轻量级Matlab深度学习框架LightNet的实现 如何基于机器学习设计一套智能交易系统？ 如何在TensorFlow中用深度学习修复图像？ 机器学习中的并行计算：GPU、CUDA和实际应用 深度解读AlphaGo胜利背后的力量：强化学习 英伟达自动驾驶技术解读：用于自动驾驶汽车的端到端深度学习 深度解读最流行的优化算法：梯度下降 Science：斯坦福大学用迁移学习预测非洲贫困状况 用 Word2vec 轻松处理新金融风控场景中的文本类数据 Science：实用量子计算机已近在咫尺 深度学习硬件架构简述 深度学习系列Part2：迁移学习和微调深度卷积神经网络 图文并茂的神经网络架构大盘点：从基本原理到衍生关系 为你的深度学习任务挑选性价比最高GPU 详解谷歌神经网络图像压缩技术：高质量地将图像压缩得更小 用于视觉任务的CNN为何能在听觉任务上取得成功？ 自然语言处理领域深度学习研究总结：从基本概念到前沿成果 专访谷歌Jeff Dean：强化学习适合的任务与产品化应用 重磅论文：解析深度卷积神经网络的14种设计模式 前沿研究 并行运算，Facebook提出门控卷积神经网络的语言建模 FAIR与微软研究院合著论文：通过虚拟问答衡量机器智能 FusionNet融合三个卷积网络：识别对象从二维升级到三维 谷歌新论文提出神经符号机：使用弱监督在Freebase上学习语义解析器 Google Brain与OpenAI合作论文：规模化的对抗机器学习 谷歌新论文：使用生成对抗网络的无监督像素级域适应 谷歌ICLR 2017论文提出超大规模的神经网络：稀疏门控专家混合层 谷歌论文：使用循环神经网络的全分辨率图像压缩 谷歌新论文提出适应性生成对抗网络AdaGAN：增强生成模型 谷歌技术论文：用于YouTube推荐的深度神经网络 谷歌与微软合著论文：由知识引导的结构化注意网络 谷歌深度解读：机器人可以如何通过共享经历学习新技能 谷歌NIPS 2016提交的8篇论文：从无监督学习到生成模型 谷歌DeepMind论文：使用合成梯度的解耦神经接口 谷歌提交ICLR 2017论文：学习记忆罕见事件 谷歌提出深度概率编程语言Edward：融合了贝叶斯、深度学习和概率编程 谷歌新论文提出预测器架构：端到端的学习与规划 DeepMind最新论文：线性时间的神经机器翻译 DeepMind David Silver论文：学习跨多个数量级的值 DeepMind论文：在线Segment to Segment神经传导 DeepMind深度解读Nature论文：可微神经计算机 DeepMind论文：调控运动控制器的学习和迁移 DeepMind NIPS 2016论文盘点（Part1）：强化学习正大步向前 DeepMind NIPS 2016论文盘点（Part2）：无监督学习的新进展 ECCV 2016 最佳论文新鲜出炉 Geoffrey Hinton论文：使用快速权重处理最近的过去 哈工大讯飞联合实验室最新论文刷新机器阅读理解纪录 华盛顿大学论文：使用机器学习分析科学文献中的视觉信息 Ian Goodfellow 论文：通过视频预测的用于物理交互的无监督学习 Ian Goodfellow 论文：用于隐私训练数据的深度学习的半监督知识迁移 IBM论文：多尺度循环神经网络在对话生成中的应用 ICLR2016会议，不可错过Facebook提交的七篇论文 计算机科学领导者：卡内基梅隆大学ACL2016论文汇总 论文：TensorFlow，一个大规模机器学习系统 论文：通过连续奖励策略梯度学习在线比对 论文：基准评测当前最先进的深度学习软件工具 论文：一种用于训练循环网络的新算法Professor Forcing 论文：高斯混合模型的似然方法中的局部极大值：结构结果和算法结果 NIPS 2016现场：谷歌发布 28 篇机器学习论文 Nature论文：无监督表征学习，用电子健康病历增强临床决策 Nature论文：从不确定性表征到自动建模 NIPS 2016 公布571篇接收论文 OpenAI与NASA论文：用于张拉整体机器人运动的深度强化学习 OpenAI论文：神经GPU的扩展和限制 苹果发布第一篇人工智能研究论文：模拟+无监督方法改善合成图像 商汤科技论文解析：人脸检测中级联卷积神经网络的联合训练 斯坦福大学李飞飞最新论文：弱监督动作标记的连接时序模型 Vicarious在ICLR2017提交无监督学习论文：层级组合特征学习 Yann LeCun论文：基于能量的生成对抗网络 Yoshua Bengio 论文：一种神经知识语言模型 Yann LeCun提交ICLR 2017论文汇总：从GAN到循环实体网络等 Bengio论文：用于序列预测的actor-critic算法 Yoshua Bengio论文：迈向生物学上可信的深度学习 Yoshua Bengio论文：使用线性分类器探头理解中间层 Yoshua Bengio论文：Mollifying Networks 微软重磅论文提出LightRNN：高效利用内存和计算的循环神经网络 微软ACL 2016论文汇集，自然语言技术逼近人类对话水平 自然语言顶级会议ACL 2016谷歌论文汇集 专知汇总 链接 【专知荟萃01】深度学习知识资料大全集（入门/进阶/论文/代码/数据/综述/领域专家等）（附pdf下载） 【专知荟萃02】自然语言处理NLP知识资料大全集（入门/进阶/论文/Toolkit/数据/综述/专家等）（附pdf下载） 【专知荟萃03】知识图谱KG知识资料全集（入门/进阶/论文/代码/数据/综述/专家等）（附pdf下载） 【专知荟萃04】自动问答QA知识资料全集（入门/进阶/论文/代码/数据/综述/专家等）（附pdf下载） 【专知荟萃05】聊天机器人Chatbot知识资料全集（入门/进阶/论文/软件/数据/专家等）(附pdf下载) 【专知荟萃06】计算机视觉CV知识资料大全集（入门/进阶/论文/课程/会议/专家等）(附pdf下载) 【专知荟萃07】自动文摘AS知识资料全集（入门/进阶/代码/数据/专家等）(附pdf下载) 【专知荟萃08】图像描述生成Image Caption知识资料全集（入门/进阶/论文/综述/视频/专家等） 【专知荟萃09】目标检测知识资料全集（入门/进阶/论文/综述/视频/代码等） 【专知荟萃10】推荐系统RS知识资料全集（入门/进阶/论文/综述/视频/代码等） 【专知荟萃11】GAN生成式对抗网络知识资料全集（理论/报告/教程/综述/代码等） 【专知荟萃12】信息检索 Information Retrieval 知识资料全集（入门/进阶/综述/代码/专家，附PDF下载） 【专知荟萃13】工业学术界用户画像 User Profile 实用知识资料全集（入门/进阶/竞赛/论文/PPT，附PDF下载） 【专知荟萃14】机器翻译 Machine Translation知识资料全集（入门/进阶/综述/视频/代码/专家，附PDF下载） 【专知荟萃15】图像检索Image Retrieval知识资料全集（入门/进阶/综述/视频/代码/专家，附PDF下载） 【专知荟萃16】主题模型Topic Model知识资料全集（基础/进阶/论文/综述/代码/专家，附PDF下载） 【专知荟萃17】情感分析Sentiment Analysis 知识资料全集（入门/进阶/论文/综述/视频/专家，附查看） 【专知荟萃18】目标跟踪Object Tracking知识资料全集（入门/进阶/论文/综述/视频/专家，附查看） 【专知荟萃19】图像识别Image Recognition知识资料全集（入门/进阶/论文/综述/视频/专家，附查看） 【专知荟萃20】图像分割Image Segmentation知识资料全集（入门/进阶/论文/综述/视频/专家，附查看） 【专知荟萃21】视觉问答VQA知识资料全集（入门/进阶/论文/综述/视频/专家，附查看） 【专知荟萃22】机器阅读理解RC知识资料全集（入门/进阶/论文/综述/代码/专家，附查看） 【专知荟萃23】深度强化学习RL知识资料全集（入门/进阶/论文/综述/代码/专家，附查看） 【专知荟萃24】视频描述生成(Video Captioning)知识资料全集（入门/进阶/论文/综述/代码/专家，附查看） 【专知荟萃25】文字识别OCR知识资料全集（入门/进阶/论文/综述/代码/专家，附查看） 【专知荟萃26】行人重识别 Person Re-identification知识资料全集（入门/进阶/论文/综述/代码，附查看） 2017年机器之心发布的教程 链接概念1. 机器学习基础 一文读懂机器学习、数据科学、人工智能、深度学习和统计学之间的区别 人人都能读懂的无监督学习：什么是聚类和降维？ 如何解读决策树和随机森林的内部工作机制？ 教程 | 拟合目标函数后验分布的调参利器：贝叶斯优化 入门 | 区分识别机器学习中的分类与回归 深度 | 思考VC维与PAC：如何理解深度神经网络中的泛化理论？ 教程 | 理解XGBoost机器学习模型的决策过程 业界 | 似乎没区别，但你混淆过验证集和测试集吗？ 教程 | 初学者如何学习机器学习中的L1和L2正则化 机器学习算法集锦：从贝叶斯到深度学习及各自优缺点 入门 | 机器学习新手必看10大算法 教程 | 详解支持向量机SVM：快速可靠的分类算法 干货 | 详解支持向量机（附学习资源） 教程 | 遗传算法的基本概念和实现（附Java实现案例） 教程 | 利用达尔文的理论学习遗传算法 深度 | 详解可视化利器t-SNE算法：数无形时少直觉 入门 | 如何构建稳固的机器学习算法：Boosting&amp;Bagging 资源 | 神经网络调试手册：从数据集与神经网络说起 观点 | 三大特征选择策略，有效提升你的机器学习水准 教程 | 如何为单变量模型选择最佳的回归函数 机器学习老中医：利用学习曲线诊断模型的偏差和方差 教程 | 如何为时间序列数据优化K-均值聚类速度？ 入门 | 将应用机器学习转化为求解搜索问题 从重采样到数据合成：如何处理机器学习中的不平衡分类问题？ 2. 深度模型基础 从零开始：教你如何训练神经网络 教程 | 深度学习初学者必读：张量究竟是什么？ 解读 | 通过拳击学习生成对抗网络（GAN）的基本原理 干货 | 直观理解GAN背后的原理：以人脸图像生成为例 教程 | 从基本概念到实现，全卷积网络实现更简洁的图像识别 资源 | 初学者指南：神经网络在自然语言处理中的应用 教程 | 深度学习：自动编码器基础和类型 入门 | 请注意，我们要谈谈神经网络的注意机制和使用方法 教程 | 经典必读：门控循环单元（GRU）的基本概念与原理 入门 | 迁移学习在图像分类中的简单应用策略 解读 | 如何从信号分析角度理解卷积神经网络的复杂机制？ 教程 | 无监督学习中的两个非概率模型：稀疏编码与自编码器 深度 | 从任务到可视化，如何理解LSTM网络中的神经元 教程 | 将注意力机制引入RNN，解决5大应用领域的序列预测问题 教程 | 听说你了解深度学习最常用的学习算法：Adam优化算法？ 教程 | 如何解决LSTM循环神经网络中的超长序列问题 教程 | 一个基于TensorFlow的简单故事生成案例：带你了解LSTM 教程 | 如何判断LSTM模型中的过拟合与欠拟合 教程 | 如何估算深度神经网络的最优学习率 教程 | 如何为神经机器翻译配置编码器-解码器模型？ 教程 | 如何用深度学习处理结构化数据？ 改进卷积神经网络，你需要这14种设计模式 3. 强化学习基础 从强化学习基本概念到Q学习的实现，打造自己的迷宫智能体 教程 | Keras+OpenAI强化学习实践：深度Q网络 一份数学小白也能读懂的「马尔可夫链蒙特卡洛方法」入门指南 入门 | 蒙特卡洛树搜索是什么？如何将其用于规划星际飞行？ 教程 | Keras+OpenAI强化学习实践：行为-评判模型 从贝叶斯定理到概率分布：综述概率论基本定义 想了解概率图模型？你要先理解图论的基本定义与形式 数学 干货 | 机器学习需要哪些数学基础？ 深度神经网络中的数学，对你来说会不会太难？ 观点 | Reddit 热门话题：如何阅读并理解论文中的数学内容？ 教程 | 基础入门：深度学习矩阵运算的概念和代码实现 从概率论到多分类问题：综述贝叶斯统计分类 机器之心最干的文章：机器学习中的矩阵、向量求导 致初学者 教程 | Kaggle CTO Ben Hamner ：机器学习的八个步骤 教程 | Kaggle初学者五步入门指南，七大诀窍助你享受竞赛 从零开始，教初学者如何征战Kaggle竞赛 只需十四步：从零开始掌握Python机器学习（附资源） 如何从初入行者进阶为人工智能先锋青年？ 观点 | 如何从一名软件工程师转行做人工智能？ 教程 | 如何转行成为一名数据科学家？ 初学者怎么选择神经网络环境？对比MATLAB、Torch和TensorFlow 教程 | 初学者如何选择合适的机器学习算法（附速查表） 经验之谈：如何为你的机器学习问题选择合适的算法？ 资源 | 企业应该怎样选择数据科学&amp;机器学习平台？ 实验研究工作流程详解：如何把你的机器学习想法变成现实 观点 | 机器学习新手工程师常犯的6大错误 教程 | 如何用Docker成为更高效的数据科学家？ 从标题到写作流程：写好一篇论文的十条基本原则 论文格式排版你真的做对了吗? 常用格式及其LaTeX书写方法介绍 课程 蒙特利尔大学开放MILA 2017夏季深度学习与强化学习课程视频（附完整PPT） 斯坦福CS231n Spring 2017开放全部课程视频（附大纲） 斯坦福大学秋季课程《深度学习理论》STATS 385开讲 资源 | CMU统计机器学习2017春季课程：研究生水平 教程 | 斯坦福CS231n 2017最新课程：李飞飞详解深度学习的框架实现与对比 三天速成！香港科技大学TensorFlow课件分享 四天速成！香港科技大学 PyTorch 课件分享 吴恩达Deeplearning.ai课程学习全体验：深度学习必备课程（已获证书） 算法实现1. 机器学习基础实现 教程 | 从头开始：用Python实现带随机梯度下降的线性回归 初学TensorFlow机器学习：如何实现线性回归？（附练习题） 教程 | 从头开始：用Python实现带随机梯度下降的Logistic回归 教程 | 从头开始：用Python实现随机森林算法 教程 | 从头开始：用Python实现基线机器学习算法 教程 | 从头开始：用Python实现决策树算法 听说你用JavaScript写代码？本文是你的机器学习指南 教程 | 如何使用JavaScript构建机器学习模型 教程 | 初学文本分析：用Python和scikit-learn实现垃圾邮件过滤器 教程 | 如何通过牛顿法解决Logistic回归问题 每个Kaggle冠军的获胜法门：揭秘Python中的模型集成 教程 | 如何在Python中快速进行语料库搜索：近似最近邻算法 2. 深度网络基础实现 教程 | 初学者入门：如何用Python和SciKit Learn 0.18实现神经网络？ 教程 | 如何用30行JavaScript代码编写神经网络异或运算器 教程 | 使用MNIST数据集，在TensorFlow上实现基础LSTM网络 教程 | 如何使用Keras集成多个卷积网络并实现共同预测 教程 | 在Python和TensorFlow上构建Word2Vec词嵌入模型 教程 | 详解如何使用Keras实现Wassertein GAN 机器之心GitHub项目：从零开始用TensorFlow搭建卷积神经网络 教程 | 如何基于TensorFlow使用LSTM和CNN实现时序分类任务 作为TensorFlow的底层语言，你会用C++构建深度神经网络吗？ 入门 | 十分钟搞定Keras序列到序列学习（附代码实现） 入门 | 想实现DCGAN？从制作一张门票谈起！ 教程 | 通过PyTorch实现对抗自编码器 教程 | 基于Keras的LSTM多变量时间序列预测 3. 计算机视觉实现 教程 | TensorFlow从基础到实战：一步步教你创建交通标志分类神经网络 教程 | 如何使用TensorFlow和自编码器模型生成手写数字 教程 | 无需复杂深度学习算法，基于计算机视觉使用Python和OpenCV计算道路交通 教程 | 深度学习 + OpenCV，Python实现实时视频目标检测 教程 | 如何通过57行代码复制价值8600万澳元的车牌识别项目 教程 | 百行代码构建神经网络黑白图片自动上色系统 教程 | 盯住梅西：TensorFlow目标检测实战 深度 | 从数据结构到Python实现：如何使用深度学习分析医学影像 仅需15分钟，使用OpenCV+Keras轻松破解验证码 教程 | 如何使用TensorFlow API构建视频物体识别系统 教程 | 经得住考验的「假图片」：用TensorFlow为神经网络生成对抗样本 先读懂CapsNet架构然后用TensorFlow实现，这应该是最详细的教程了 教程 | 如何使用深度学习去除人物图像背景 资源 | 如何通过CRF-RNN模型实现图像语义分割任务 4. 自然语言处理实现 如何解决90％的自然语言处理问题：分步指南奉上 资源 | Github项目：斯坦福大学CS-224n课程中深度NLP模型的PyTorch实现 谷歌开放GNMT教程：如何使用TensorFlow构建自己的神经机器翻译系统 教程 | 从头开始在Python中开发深度学习字幕生成模型 资源 | 谷歌全attention机器翻译模型Transformer的TensorFlow实现 教程 | 如何使用TensorFlow构建、训练和改进循环神经网络 教程 | Kaggle网站流量预测任务第一名解决方案：从模型到代码详解时序预测 教程 | 用于金融时序预测的神经网络：可改善移动平均线经典策略 教程 | 如何用PyTorch实现递归神经网络？ 教程 | 用数据玩点花样！如何构建skip-gram模型来训练和可视化词向量 教程 | 利用TensorFlow和神经网络来处理文本分类问题 5. 强化学习实现 教程 | 深度强化学习入门：用TensorFlow构建你的第一个游戏AI 资源 | 价值迭代网络的PyTorch实现与Visdom可视化 解读 | 如何使用深度强化学习帮助自动驾驶汽车通过交叉路口？ 6. 深度学习框架 分布式TensorFlow入坑指南：从实例到代码带你玩转多机器深度学习 教程 | 从零开始：TensorFlow机器学习模型快速部署指南 资源 | TensorFlow极简教程：创建、保存和恢复机器学习模型 快速开启你的第一个项目：TensorFlow项目架构模板 TensorFlow初学者指南：如何为机器学习项目创建合适的文件架构 教程 | 七个小贴士，顺利提升TensorFlow模型训练表现 教程 | 如何从TensorFlow转入PyTorch 教程 | 如何利用C++搭建个人专属的TensorFlow 谷歌云大会教程：没有博士学位如何玩转TensorFlow和深度学习（附资源） 教程 | 维度、广播操作与可视化：如何高效使用TensorFlow 教程 | PyTorch内部机制解析：如何通过PyTorch实现Tensor 贾扬清撰文详解Caffe2：从强大的新能力到入门上手教程 教程 | TensorFlow 官方解读：如何在多系统和网络拓扑中构建高性能模型 教程 | 如何使用TensorFlow中的高级API：Estimator、Experiment和Dataset 教程 | Docker Compose + GPU + TensorFlow 所产生的奇妙火花 工具方法 教程 | 如何优雅而高效地使用Matplotlib实现数据可视化 教程 | 如何用百度深度学习框架PaddlePaddle做数据预处理 教程 | 一文入门Python数据分析库Pandas 代码优化指南：人生苦短，我用Python 资源 | 从数组到矩阵的迹，NumPy常见使用大总结 教程 | Python代码优化指南：从环境设置到内存分析（一） 资源 | 如何利用VGG-16等模型在CPU上测评各深度学习框架 教程 | 手把手教你可视化交叉验证代码，提高模型预测能力 教程 | 如何使用Kubernetes GPU集群自动训练和加速深度学习？ 教程 | Prophet：教你如何用加法模型探索时间序列数据 初学机器学习的你，是否掌握了这样的Linux技巧？ 云端 教程 | 新手指南：如何在AWS GPU上运行Jupyter noterbook？ 教程 | 只需15分钟，使用谷歌云平台运行Jupyter Notebook 入门 | 完全云端运行：使用谷歌CoLaboratory训练神经网络 边缘设备 教程 | BerryNet：如何在树莓派上实现深度学习智能网关 机器之心实操 | 亚马逊详解如何使用MXNet在树莓派上搭建实时目标识别系统 手把手教你为iOS系统开发TensorFlow应用（附开源代码） 教程 | 如何使用Swift在iOS 11中加入原生机器学习视觉模型 教程 | 如何使用谷歌Mobile Vision API 开发手机应用 开源 | 深度安卓恶意软件检测系统：用卷积神经网络保护你的手机 专栏 | 手机端运行卷积神经网络实践：基于TensorFlow和OpenCV实现文档检测功能 资源 | 用苹果Core ML实现谷歌移动端神经网络MobileNet 教程 | 如何用TensorFlow在安卓设备上实现深度学习推断 深度 | 向手机端神经网络进发：MobileNet压缩指南 硬件 成本14,000元，如何自己动手搭建深度学习服务器？ 资源 | 只需1200美元，打造家用型深度学习配置（CPU+GTX 1080） 教程 | 从硬件配置、软件安装到基准测试，1700美元深度学习机器构建指南 从硬件配置到框架选择，请以这种姿势入坑深度学习 从零开始：深度学习软件环境安装指南 这是一份你们需要的Windows版深度学习软件安装指南 教程 | 一步步从零开始：使用PyCharm和SSH搭建远程TensorFlow开发环境 实用指南：如何为你的深度学习任务挑选最合适的 GPU?（最新版） 深度 | 英伟达Titan Xp出现后，如何为深度学习挑选合适的GPU？这里有份性价比指南 Titan XP值不值？一文教你如何挑选深度学习GPU 吃喝玩乐撸撸猫 教程 | 你来手绘涂鸦，人工智能生成「猫片」：edges2cats图像转换详解 教程 | 萌物生成器：如何使用四种GAN制造猫图 学界 | 宅男的福音：用GAN自动生成二次元萌妹子 深度 | 如何使用神经网络弹奏出带情感的音乐？ 深度 | 人工智能如何帮你找到好歌：探秘Spotify神奇的每周歌单 解读 | 艺术家如何借助神经网络进行创作？ 教程 | 用生成对抗网络给雪人上色，探索人工智能时代的美学 圣诞快乐——Keras+树莓派：用深度学习识别圣诞老人 教程 | 摄影爱好者玩编程：利用Python和OpenCV打造专业级长时曝光摄影图 教程 | 基于遗传算法的拼图游戏解决方案 教程 | AI玩微信跳一跳的正确姿势：跳一跳Auto-Jump算法详解 Money, Money, Money 教程 | 从零开始：如何使用LSTM预测汇率变化趋势 自创数据集，使用TensorFlow预测股票入门 资源 | 利用深度强化学习框架解决金融投资组合管理问题（附 GitHub 实现） 比特币突破8000美元，我们找到了用DL预测虚拟货币价格的方法 教程 | 如何使用深度学习硬件的空余算力自动挖矿 教程 | 如何用Python和机器学习炒股赚钱？]]></content>
      <categories>
        <category>资源</category>
      </categories>
      <tags>
        <tag>资源汇总</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习中的模型评价与选择]]></title>
    <url>%2F2018%2F02%2F16%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BB%B7%E4%B8%8E%E9%80%89%E6%8B%A9%2F</url>
    <content type="text"><![CDATA[介绍机器学习中如何评估、选择模型 Holdout方法和正态逼近置信区间 重复Holdout和Bootstrap 交叉验证和超参数优化 结论 节选自机器之心 链接，原文还涉及很多偏数学、理论方面的东西英文原文 链接 机器学习模型的性能估计主要分为以下三步 使用训练集训练模型 用模型预测测试集的分类 计算在测试集上的错误率 Holdout方法 Holdout是最简单的模型评估技术，把数据集随机拆分成训练集和测试集 Resubstitution验证或Resubstitution评估指的是在同一个训练数据集上训练和评估一个模型 随机抽样可能导致训练集和测试集分布不平衡，甚至测试集中可能不包含少数类的样本。因此，可以使用层次化的方式划分数据集，保证划分得到的训练集和测试集保持原始比例 在数据量大、类别较平衡的数据集上，非层次化的随机抽样不是一个大问题，但最好还是使用层次化抽样 正态逼近置信区间：计算模型预测精度或计算误差置信区间。假设预测结果会遵循一个正态分布，然后根据中心极限定理计算单次训练-测试划分的平均值的置信区间。通过正态逼近计算出基于单个测试集的性能估计的不确定性。详细推导见原文 重复Holdout和Bootstrap 模型评估的偏差与方差：将数据集的较多数据作为测试集容易给评估带来悲观偏差。随着测试集样本数量的减少，悲观偏差降低，性能估计的方差增加 重复Holdout验证：多次随机划分训练集和测试集，重复Holdout方法估计模型性能然后取平均值的方法获得更具鲁棒性的评估，又称为蒙特卡洛交叉验证 Bootstrap方法：从经验分布中采样生成新样本，衡量性能估计的不确定性。如果把Holdout方法理解为不放回采样，那么bootstrap就可以理解为通过有放回重采样产生新数据 Leave-one-out Bootstrap（LOOB）方法：这种方法测试集数据和训练集数据没有重叠 交叉验证和超参数优化 three-way Holdout方法：将数据集划分为训练、验证和测试集三部分，用于超参数调优 K-Fold交叉验证：数据集中的每个样本都有被测试的机会。与重复holdout方法相比，k-fold交叉验证的测试数据没有重叠，而重复holdout是重复使用样本进行测试 2-fold和留一法（Leave-One-Out）交叉验证：分别是K-Fold交叉验证中k=2和k=n的特殊情况。留一交叉验证法中，每次迭代中，模型都在其中n-1个样本上进行拟合，然后在剩余的一个样本上进行评估。这种方法对小数据集特别有用 在计算可行的情况下，留一法交叉验证更值得推荐，近似无偏，但方差大；当数据集很大的时候，出于计算效率的考虑我们就会更倾向于holdout方法（相当于分成2部分，迭代2次即可），我们在深度学习中使用的就是Holdout方法而不是交叉验证 在k-fold交叉验证中，随着k的增加有如下趋势： 性能估计偏差减小（更准确）性能估计方差增大（更大的变化性）计算成本增加（在拟合过程中训练集更大，需要的迭代次数更多）在k-fold交叉验证中将k的值降到最小（如2或3）也会增加小数据集上模型估计的方差，因为随机抽样变化较大推荐用10折交叉验证 如果我们对数据归一化或进行特征选择，我们通常会在k-fold交叉验证循环中执行这些操作，即做归一化时不考虑测试集部分的数据，但可能效果不好。关于这一选择可参考链接。看了以后感觉不是很有所谓，差异不大(“for the greater majority of cases, IN and OUT are not significantly different”) 结论 当处理的样本量较大时，使用holdout方法进行模型评价非常合适 对于超参数优化，推荐10折交叉验证 对于小样本，留一法交叉验证则是一个不错的选择]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>模型评估</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度学习技巧]]></title>
    <url>%2F2018%2F02%2F14%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8A%80%E5%B7%A7%2F</url>
    <content type="text"><![CDATA[在这里总结一下看到过的各路炼丹技巧 深度学习的7大实用技巧* 链接 数据：有标签的数据越多，模型的性能越好。重温深度学习时代数据不可思议的有效性 优化器：RMSprop，Adadelta和Adam法都是自适应优化算法，会自动更新学习率。普通的随机梯度下降法需要手动选择学习率和动量参数。 在实践中，自适应优化器往往比普通的梯度下降法更快地让模型达到收敛状态。然而，选择这些优化器的模型最终性能都不太好，而普通的梯度下降法通常能够达到更好的收敛最小值，从而获得更好的模型性能，但这可能比某些优化程序需要更多的收敛时间。此外，随机梯度下降法也更依赖于有效的初始化方法和学习速率衰减指数的设置，这在实践中是很难确定的。如下图，Adam初期效果好，但SGD在训练结束后可以获得更好的全局最小值 可以混合使用两类优化器：由Adam优化器过渡到随机梯度下降法来优化模型。在训练的早期阶段，使用Adam优化器来启动模型的训练，这将为模型的训练节省很多参数初始化和微调的时间。一旦模型的性能有所起伏，我们就可以切换到带动量的随机梯度下降法来进一步优化模型，以达到最佳的性能 处理数据不平衡：不同类的训练数据数量差异较大，会使得预测结果倾向于数量大的类别。 对损失函数使用类别权重：对于数据量小的类别，损失函数中使用较高的权重值。这样对该类的任何错误都将导致非常大的损失值，以此来惩罚错误分类过度抽样：对于小样本类别，重复进行采样欠采样：对于大样本类别，跳过不选择部分数据数据增强：对于小样本类别进行数据增强，生成更多训练样本 迁移学习 数据增强：保证数据原始类别标签的同时，对一些原始图像进行非线性的图像变换，来生成/合成新的训练样本。即对图像进行任何操作，改变图像的外观，但不能改变整体的内容。如对于猫的图像，我们可以把图像进行任意的几何变换，只要还是一只猫，那么label就不会变，但在图像以外的问题上能否直接使用数据增强有待进一步查阅资料 水平或垂直旋转/翻转图像随机改变图像的亮度和颜色随机模糊图像随机裁剪图像 集成模型：对于一个特定的任务，训练多个模型并根据模型的整体性能决定最优的组合方案。具体的实现细节还不太清楚，但模型集成有三种比较典型的方式：链接。下2中有关于集成的几种常见设计 模型剪枝加速：目的是为了在提高训练速度的同时保持模型的高性能，目前已有许多相关的研究，如MobileNet等等。核心思想是按照一定标准为神经元进行排序，从而将排名低(即冗余的神经元，对模型贡献不大)的移除模型，也就是把不重要的卷积滤波器丢弃(因为神经元和卷积核一一对应) 一般的剪枝流程如下：Network -&gt; Evaluate Importance of Neurons -&gt; Remove the Least Important Neuron -&gt; Fine-tuning -&gt; Continue Pruning -&gt; Back to Evaluate Importance of Neurons 知乎专栏 参数初始化：Xavier和He随便选一个，但一定要做 uniform均匀分布初始化：w = np.random.uniform(low=-scale, high=scale, size=[n_in,n_out]) Xavier初始法，适用于普通激活函数(tanh,sigmoid)：scale = np.sqrt(3/n) He初始化，适用于ReLU：scale = np.sqrt(6/n) normal高斯分布初始化：w = np.random.randn(n_in,n_out) * stdev # stdev为高斯分布的标准差，均值设为0 Xavier初始法，适用于普通激活函数 (tanh,sigmoid)：stdev = np.sqrt(n) He初始化，适用于ReLU：stdev = np.sqrt(2/n) svd初始化：对RNN有比较好的效果。参考论文：https://arxiv.org/abs/1312.6120 数据预处理 常用：zero-center X -= np.mean(X, axis = 0) # zero-center X /= np.std(X, axis = 0) # normalize 不常用：白化 训练技巧 要做梯度归一化，即算出来的梯度除以minibatch size clip c(梯度裁剪): 限制最大梯度，其实是value = sqrt(w1^2+w2^2….)。如果value超过了阈值，就算一个衰减系系数，让value的值等于阈值: 5,10,15 Dropout对小数据防止过拟合有很好的效果，值一般设为0.5。小数据上dropout+sgd效果提升都非常明显。dropout的位置比较有讲究, 对于RNN,建议放到输入-&gt;RNN与RNN-&gt;输出的位置。关于RNN如何用dropout,可以参考论文 优化器：同1中所述，SGD收敛慢，但结果好。对于SGD，可以选择从1.0或者0.1的学习率开始，隔一段时间在验证集上检查一下，如果cost没有下降，就对学习率减半。也可以先用ada系列先跑，最后快收敛的时候，更换成sgd继续训练，同样也会有提升。据说adadelta一般在分类问题上效果比较好，adam在生成问题上效果比较好 除了gate之类的地方需要把输出限制成0-1之外，尽量不要用sigmoid。可以用tanh或者relu之类的激活函数。sigmoid的问题: 1. sigmoid函数在-4到4的区间里，才有较大的梯度。其他区间的梯度接近0，很容易造成梯度消失问题。2. 如果输入是0均值的，sigmoid函数的输出不是0均值的 rnn的dim和embdding size，一般从128上下开始调整。batch size一般从128左右开始调整，合适最重要，并不是越大越好。关于Batch Size设置的分析可参考这篇文章。对于batch，建议是把卡塞满的2的n次方 word2vec初始化，在小数据上不仅可以有效提高收敛速度，也可以可以提高结果 尽量对数据做shuffle LSTM的forget gate的bias，用1.0或者更大的值做初始化可以取得更好的结果，来自这篇论文。实际使用中，不同的任务可能需要尝试不同的值 使用Batch Normalization可以提升效果 如果模型包含全连接层（MLP），且输入和输出大小一样，可以考虑将MLP替换成Highway Network，对结果有一点提升，建议作为最后提升模型的手段。原理很简单，就是给输出加了一个gate来控制信息的流动，详细介绍请参考论文 一轮加正则，一轮不加正则，反复进行 Ensemble(模型集成) Ensemble是论文刷结果的终极核武器，一般有以下几种方式： 同样的参数，不同的初始化方式 不同的参数，通过cross-validation，选取最好的几组 同样的参数，模型训练的不同阶段，即不同迭代次数的模型 不同的模型，进行线性融合。例如RNN和传统模型 知乎回答 Relu+Bn可以满足95%的情况，除非有些特殊情况会用identity，比如回归问题，比如resnet的shortcut支路 Dropout，分类问题用dropout ，只需要最后一层softmax 前用基本就可以了，能够防止过拟合，可能对accuracy提高不大，但是dropout 前面的那层如果是之后要使用的feature的话，性能会大大提升 数据的shuffle和augmentation，aug不能瞎加，比如行人识别一般就不会加上下翻转的，因为不会碰到头朝下的异型种 降学习率，随着网络训练的进行，学习率要逐渐降下来。如果用tensorboard就能发现，在学习率下降的一瞬间，网络会有个巨大的性能提升。同样的fine-tuning也要根据模型的性能设置合适的学习率，比如一个训练的已经非常好的模型你上来就1e-3的学习率，那之前就白训练了，就是说网络性能越好，学习率要越小 tensorboard，帮助你监视网络的状态，来调整网络参数 随时存档模型，要有validation。把每个epoch和其对应的validation 结果存下来，可以分析出开始overfitting的时间点，方便下次加载fine-tuning 网络层数，参数量什么的都不是大问题，在性能不丢的情况下，减到最小。（不是说网络越深越好？） 对于batch size，建议是把卡塞满的2的n次方 输入减不减mean归一化在有了bn之后已经不那么重要了 卷积核的分解。从最初的5×5分解为两个3×3，到后来的3×3分解为1×3和3×1，再到resnet的1×1，3×3，1×1，再xception的3×3 channel-wise conv+1×1，网络的计算量越来越小，层数越来越多，性能越来越好，这些都是设计网络时可以借鉴的 不同尺寸的feature maps的concat，只用一层的feature map一把梭可能不如concat好，pspnet就是这种思想，这个思想很常用 resnet的shortcut确实会很有用，重点在于shortcut支路一定要是identity，主路是什么conv都无所谓，这是我亲耳听resnet作者所述 针对于metric learning，对feature加个classification 的约束通常可以提高性能加快收敛 链接和链接]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>炼丹技巧</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[六大聚类算法]]></title>
    <url>%2F2018%2F02%2F12%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%2F%E5%85%AD%E5%A4%A7%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[介绍六类主流的聚类算法，并总结了相应的优缺点 K-Means 均值漂移聚类 基于密度的聚类方法（DBSCAN） 用高斯混合模型（GMM）的最大期望（EM）聚类 凝聚层次聚类 图团体检测 原文来自机器之心 链接序号写成”.”格式全部崩了。。把分隔符去掉又显得太挤了。。。以后找到办法了再回来改TAT K-Means 优点：速度快，O(N) 缺点：类别数需要事先人为估计，无法通过算法来确定类别数；结果有可能不可重复 变种：K-Medians，每次用中值向量而不是均值来作为中心。对异常值不敏感，对大数据集速度慢，因为需要做排序 均值漂移聚类 算法：随机选择n个中心点，有n个半径为r的圆形滑动窗口，同时开始迭代。每次迭代时，滑动窗口的中心点都不断移向窗口内所有点的均值点，也就是移向点密度更大的区域，直到收敛(点密度无法再增加)。收敛时，若多个窗口发生重叠时，保留点数最多的那个窗口。这样就可以得到类别数以及每个类的中心位置。其他各个点属于哪一类则取决于迭代过程中该点在哪一类的窗口中的次数最多 优点：类别数量由算法自动获得；聚类中心朝最大点密度聚集的事实也是非常令人满意的 缺点：窗口大小/半径「r」的选择可能是不重要的 基于密度的聚类方法（DBSCAN） 算法：每次从一个未被访问过的数据点开始，若该点的邻域(ε 距离内的所有点都是邻域点)内点的数量大于参数minPoints，则该点为一个新类的第一个点，否则该点就是噪声点。若该点成为了一个新类的第一个点，则该点邻域内的所有点也成为了该类的一部分，同时以这些新点再去找它们邻域中的点并加到该类中来，直到收敛。收敛后，在未被访问过的点中再找一个重新开始之前的步骤 优点：不需要人为确定类别数；能够辨别噪声；能很好地找到任意大小和任意形状的类 缺点：当簇的密度不同时，它的表现不如其他聚类算法(因为密度不同，参数ε和minPoints的设置也需要不一样，难以估计) 用高斯混合模型（GMM）的最大期望（EM）聚类 K-Means中假设数据点的分布是圆形的(其分类原理是离哪个中心点更近就认为是哪一类)，限制较大。GMM中假设数据点是高斯分布的，意味着类可以是各种类型的形状(二维中为椭圆，因为x和y方向各自有标准差)。于是，任务就变成了使用EM来找到每个类别的高斯函数。下图是典型的K-Means不适用的情况： 算法：和K-Means一样，先选择类别数量，并随机初始化高斯分布参数。随后计算每个数据点属于一个特定类的概率(一个点越靠近高斯的中心，它就越可能属于该类)。基于这些概率，计算一组新的高斯分布参数使得类内数据点的概率最大化。其中，新参数是由数据点位置的加权和得到，权重就是之前提到的概率。重复之前的操作直到收敛 优点：类形状任意(K-Means其实是GMM的一个特殊情况，即所有维度的协方差均接近0)；由于GMM使用概率，因此每个数据点可以属于很多类，且对于每个类都有一个相应的概率 凝聚层次聚类 层次聚类分为自上而下和自下而上。对于自下而上，每个数据点都是单独的一个类，随后不断合并两个类，直到所有类最后都合并成一个包含所有数据点的类。自下而上层次聚类又被称为凝聚式层次聚类(HAC)。 算法：一开始有N个数据点，也就是N个类。随后自行定义一种距离度量标准如平均距离等，每次迭代时选择将两个距离最近的类合并在一起，直到只剩下一个类。我们只需要选择何时停止合并，就可以选择最终需要多少个类 优点：不需要指定类个数，我们可以自由选择看起来最好的类个数；对距离度量标准不敏感； 缺点：效率低，O(N^2) 图团体检测（Graph Community Detection） 当我们的数据可以被表示为一个网络或图（graph）时，我们可以使用图团体检测方法完成聚类。聚类的质量由模块性分数进行评估，模块性越高，该网络聚类成不同团体的程度就越好。因此通过最优化方法寻找最大模块性就能发现聚类该网络的最佳方法。 模块性可以使用以下公式进行计算： 其中L代表网络中边的数量，k_i和k_j是指每个顶点的degree，它可以通过将每一行和每一列的项加起来而得到。两者相乘再除以2L表示当该网络是随机分配的时候顶点i和j之间的预期边数。括号中的项表示了该网络的真实结构和随机组合时的预期结构之间的差。当A_ij==1且(k_i*k_j)/2L很小时，其返回的值最高。即当在定点i和j之间存在一个「非预期」的边时，得到的值更高。最后的δc_i, c_j是克罗内克δ函数（Kronecker-delta function）。 优点：在典型的结构化数据中和现实网状数据都有非常好的性能 缺点：会忽略一些小的集群，且只适用于结构化的图模型]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>无监督学习</tag>
        <tag>聚类</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[自编码器]]></title>
    <url>%2F2018%2F02%2F11%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%2F%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8%2F</url>
    <content type="text"><![CDATA[关于AE到VAE的介绍 待看：论文、cs231nLet13中的VAE推导]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>无监督学习</tag>
        <tag>自编码器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[cs231n学习笔记]]></title>
    <url>%2F2018%2F02%2F11%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%2Fcs231n%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[把cs231n 2017年的课程重新刷一遍，记录相关的笔记 cs231n课程主页 cs231n课程大纲 cs231n课程视频 Lecture 2 Image Classification课程资源Course Materials python/numpy tutorial image classification notes Course Materials 翻译 Python Numpy教程 图像分类笔记 课程笔记Python NumpyPython python –version # 查看Python版本 Python没有x++和x—的操作符 布尔逻辑：and；or；not；!=(xor) 字符串：文档 hw = ‘hello’ + ‘ ‘ + ‘world’ hw = ‘%s %s %d’ % (hello, world, 12) print(s.capitalize()) # 把第一个字母变大写，其余小写 print(s.upper()) # 全部大写 print(s.rjust(7)) # 右对齐，最左端不足补空格 print(s.center(7)) # 居中对齐 print(s.replace(‘l’, ‘(ell)’)) # 替换 print(‘ world ‘.strip()) # 去掉空格 容器：文档 列表Lists：Python中的数组，但是列表长度可变，能包含不同类型元素xs = [3, 1, 2] # 创建一个列表 print xs, xs[2] # 输出：&quot;[3, 1, 2] 2&quot; print xs[-1] # 即xs[2] xs[2] = &apos;foo&apos; # 列表可以容纳不同类型的元素 print xs # 输出：&quot;[3, 1, &apos;foo&apos;]&quot; xs.append(&apos;bar&apos;) # 向列表末尾中加元素 print xs # 输出：&quot;[3, 1, &apos;foo&apos;, &apos;bar&apos;]&quot; x = xs.pop() # 移除最后列表最后一个元素，并返回该元素 print x, xs # 输出：&quot;bar [3, 1, &apos;foo&apos;]&quot; 切片Slicing：用于获取列表中的元素nums = range(5) # range是一个内置函数，创建一个列表，包含0到5的integer print nums # 输出：&quot;[0, 1, 2, 3, 4]&quot; print nums[2:4] # 获得一个索引从2到4（不包括4）的切片; 输出：&quot;[2, 3]&quot; print nums[2:] # 获得一个索引从2到末尾的切片; 输出：&quot;[2, 3, 4]&quot; print nums[:2] # 获得一个索引从开头到2（不包括2）的切片; 输出：&quot;[0, 1]&quot; print nums[:] # 获得一个索引全部元素的切片; 输出：&quot;[0, 1, 2, 3, 4]&quot; print nums[:-1] # 切片的索引也可以是负数; 输出：&quot;[0, 1, 2, 3]&quot; nums[2:4] = [8, 9] # 把一个列表赋值给一个切片 print nums # 输出：&quot;[0, 1, 8, 8, 4]&quot; 循环Loops animals = [&apos;cat&apos;, &apos;dog&apos;, &apos;monkey&apos;] for animal in animals: print animal # 输出：&quot;cat&quot;, &quot;dog&quot;, &quot;monkey&quot;, 一个一行 animals = [&apos;cat&apos;, &apos;dog&apos;, &apos;monkey&apos;] for idx, animal in enumerate(animals): print &apos;#%d: %s&apos; % (idx + 1, animal) # 输出：&quot;#1: cat&quot;, &quot;#2: dog&quot;, &quot;#3: monkey&quot;, 一个一行 列表推导List comprehensions：避免通过循环等操作去对列表元素赋值nums = [0, 1, 2, 3, 4] even_squares = [x ** 2 for x in nums if x % 2 == 0] print even_squares # 输出：&quot;[0, 4, 16]&quot; 字典Dictionaries：文档d = {&apos;cat&apos;: &apos;cute&apos;, &apos;dog&apos;: &apos;furry&apos;} # 创建字典 print d[&apos;cat&apos;] # 输出：&quot;cute&quot; print &apos;cat&apos; in d # 检测字典中是否有&apos;cat&apos;的索引; 输出：&quot;True&quot; d[&apos;fish&apos;] = &apos;wet&apos; # 在字典中添加一个条目 print d[&apos;fish&apos;] # 输出：&quot;wet&quot; # print d[&apos;monkey&apos;] # KeyError: &apos;monkey&apos; not a key of d print d.get(&apos;monkey&apos;, &apos;N/A&apos;) # Get an element with a default; 输出：&quot;N/A&quot; print d.get(&apos;fish&apos;, &apos;N/A&apos;) # Get an element with a default; 输出：&quot;wet&quot; del d[&apos;fish&apos;] # 把&apos;fish&apos;:&apos;wet&apos;条目从字典中去掉 print d.get(&apos;fish&apos;, &apos;N/A&apos;) # 字典中没有fish这个索引了; 输出：&quot;N/A&quot; 循环Loops d = {&apos;person&apos;: 2, &apos;cat&apos;: 4, &apos;spider&apos;: 8} for animal in d: legs = d[animal] print &apos;A %s has %d legs&apos; % (animal, legs) # 输出：&quot;A person has 2 legs&quot;, &quot;A spider has 8 legs&quot;, &quot;A cat has 4 legs&quot; d = {&apos;person&apos;: 2, &apos;cat&apos;: 4, &apos;spider&apos;: 8} for animal, legs in d.iteritems(): print &apos;A %s has %d legs&apos; % (animal, legs) # 输出：&quot;A person has 2 legs&quot;, &quot;A spider has 8 legs&quot;, &quot;A cat has 4 legs&quot; 字典推导Dictionary comprehensions：与列表类似nums = [0, 1, 2, 3, 4] even_num_to_square = {x: x ** 2 for x in nums if x % 2 == 0} print even_num_to_square # 输出：&quot;{0: 0, 2: 4, 4: 16}&quot; 集合Sets：文档 animals = {&apos;cat&apos;, &apos;dog&apos;} # 创建集合 print &apos;cat&apos; in animals # 检测集合中是否有&apos;cat&apos;; 输出：&quot;True&quot; print &apos;fish&apos; in animals # 输出：&quot;False&quot; animals.add(&apos;fish&apos;) # 往集合里加&apos;fish&apos; print &apos;fish&apos; in animals # 输出：&quot;True&quot; print len(animals) # 集合元素数量; 输出：&quot;3&quot; animals.add(&apos;cat&apos;) # 往集合中添加已有的元素，什么都不会做 print len(animals) # 输出：&quot;3&quot; animals.remove(&apos;cat&apos;) # 从集合中移除&apos;cat&apos; print len(animals) # 输出：&quot;2&quot; 元组Tuples：文档d = {(x, x + 1): x for x in range(10)} # Create a dictionary with tuple keys print d t = (5, 6) # Create a tuple print type(t) # Prints &quot;&lt;type &apos;tuple&apos;&gt;&quot; print d[t] # Prints &quot;5&quot; print d[(1, 2)] # Prints &quot;1&quot; # 可以作为字典的索引，也可以作为集合的元素 Numpy 文档 科学计算库 创建数组 文档a = np.array([1, 2, 3]) b = np.array([[1,2,3],[4,5,6]]) a = np.zeros((2,2)) # Prints &quot;[[ 0. 0.] # [ 0. 0.]]&quot; b = np.ones((1,2)) # Prints &quot;[[ 1. 1.]]&quot; c = np.full((2,2), 7) # Prints &quot;[[ 7. 7.] # [ 7. 7.]]&quot; d = np.eye(2) # Prints &quot;[[ 1. 0.] # [ 0. 1.]]&quot; e = np.random.random((2,2)) # Might print &quot;[[ 0.91940167 0.08143941] # [ 0.68744134 0.87236687]]&quot; 访问数组 文档 切片 a = np.array([[1,2,3,4], [5,6,7,8], [9,10,11,12]]) # [[ 1 2 3 4] # [ 5 6 7 8] # [ 9 10 11 12]] b = a[:2, 1:3] # [[2 3] # [6 7]] # 修改b数组也会改变a数组 row_r1 = a[1, :] # &quot;[5 6 7 8] (4,)&quot; row_r2 = a[1:2, :] # &quot;[[5 6 7 8]] (1, 4)&quot; # row_r1会导致得到的数组比原数组阶数低 整形数组 a = np.array([[1,2], [3, 4], [5, 6]]) print a[[0, 1, 2], [0, 1, 0]] # 与np.array([a[0, 0], a[1, 1], a[2, 0]])等价 a = np.array([[1,2,3], [4,5,6], [7,8,9], [10, 11, 12]]) b = np.array([0, 2, 0, 1]) print a[np.arange(4), b] # 输出：&quot;[1 6 7 11]&quot;，即a[0, 0]; a[1, 2]; a[2, 0]; a[3, 1] 布尔数组访问a = np.array([[1,2], [3, 4], [5, 6]]) bool_idx = (a &gt; 2) # Prints &quot;[[False False] # [ True True] # [ True True]]&quot; print a[bool_idx] # Prints &quot;[3 4 5 6]&quot;；等价于a[a &gt; 2] x.dtype查看数据类型 文档 数据计算 函数文档；数组操作文档 x = np.array([[1,2],[3,4]], dtype=np.float64) y = np.array([[5,6],[7,8]], dtype=np.float64) print np.add(x, y) print np.subtract(x, y) print np.multiply(x, y) # 逐个相乘，不是矩阵乘法 print np.divide(x, y) print np.sqrt(x) v = np.array([9, 10]) w = np.array([11, 12]) print v.dot(w) # 向量内积；等价于np.dot(v, w) print x.dot(y) # 矩阵乘法 print np.sum(x) # 1+2+3+4=10 print np.sum(x, axis=0) # 计算每列的sum; &quot;[4, 6]&quot; print np.sum(x, axis=1) # 计算每行的sum; &quot;[3, 7]&quot; print x.True # 转置 广播Broadcasting：让不同大小的矩阵直接在一起进行数学计算 文档 import numpy as np x = np.array([[1,2,3], [4,5,6], [7,8,9], [10, 11, 12]]) v = np.array([1, 0, 1]) y = x + v # 使用广播把v加到x的每一行中 print y # &quot;[[ 2 2 4] # [ 5 5 7] # [ 8 8 10] # [11 11 13]]&quot; 广播规则 如果数组的秩不同，使用1来将秩较小的数组进行扩展，直到两个数组的尺寸的长度都一样。 如果两个数组在某个维度上的长度是一样的，或者其中一个数组在该维度上长度为1，那么我们就说这两个数组在该维度上是相容的。 如果两个数组在所有维度上都是相容的，他们就能使用广播。 如果两个输入数组的尺寸不同，那么注意其中较大的那个尺寸。因为广播之后，两个数组的尺寸将和那个较大的尺寸一样。 在任何一个维度上，如果一个数组的长度为1，另一个数组长度大于1，那么在该维度上，就好像是对第一个数组进行了复制。 广播例子 import numpy as np v = np.array([1,2,3]) # v has shape (3,) w = np.array([4,5]) # w has shape (2,) print np.reshape(v, (3, 1)) * w # 矩阵乘法 # [[ 4 5] # [ 8 10] # [12 15]] # 首先v变成(3,1)，而w是(2,)，即(1,2)，两者广播运算后，维度为(3,2) x = np.array([[1,2,3], [4,5,6]]) print x + v # [[2 4 6] # [5 7 9]] # x是(2,3)，v是(3,)/(1,3)，广播后为(2,3) print (x.T + w).T # [[ 5 6 7] # [ 9 10 11]] # x.T为(3,2)，w为(1,2)，相加后为(3,2)，再转置得到(2,3) print x + np.reshape(w, (2, 1)) # 和上面的表达式等价，把w变形后可以直接广播相加 print x * 2 # [[ 2 4 6] # [ 8 10 12]] # 对矩阵乘一个常数 Scipy 文档 图像操作 from scipy.misc import imread, imsave, imresize # Read an JPEG image into a numpy array img = imread(&apos;assets/cat.jpg&apos;) print img.dtype, img.shape # Prints &quot;uint8 (400, 248, 3)&quot; # We can tint the image by scaling each of the color channels # by a different scalar constant. The image has shape (400, 248, 3); # we multiply it by the array [1, 0.95, 0.9] of shape (3,); # numpy broadcasting means that this leaves the red channel unchanged, # and multiplies the green and blue channels by 0.95 and 0.9 # respectively. img_tinted = img * [1, 0.95, 0.9] # Resize the tinted image to be 300 by 300 pixels. img_tinted = imresize(img_tinted, (300, 300)) # Write the tinted image back to disk imsave(&apos;assets/cat_tinted.jpg&apos;, img_tinted) Matlab文件读写(scipy.io.loadmat/scipy.io.savemat) 文档 点之间的距离 文档 import numpy as np from scipy.spatial.distance import pdist, squareform # Create the following array where each row is a point in 2D space: # [[0 1] # [1 0] # [2 0]] x = np.array([[0, 1], [1, 0], [2, 0]]) print x # Compute the Euclidean distance between all rows of x. # d[i, j] is the Euclidean distance between x[i, :] and x[j, :], # and d is the following array: # [[ 0. 1.41421356 2.23606798] # [ 1.41421356 0. 1. ] # [ 2.23606798 1. 0. ]] d = squareform(pdist(x, &apos;euclidean&apos;)) print d Matplotlib 作图库 # [plot文档](https://matplotlib.org/api/pyplot_api.html#matplotlib.pyplot.plot) import numpy as np import matplotlib.pyplot as plt # Compute the x and y coordinates for points on a sine curve x = np.arange(0, 3 * np.pi, 0.1) y_sin = np.sin(x) y_cos = np.cos(x) # Plot the points using matplotlib plt.plot(x, y_sin) plt.plot(x, y_cos) plt.xlabel(&apos;x axis label&apos;) plt.ylabel(&apos;y axis label&apos;) plt.title(&apos;Sine and Cosine&apos;) plt.legend([&apos;Sine&apos;, &apos;Cosine&apos;]) plt.show() # 使用subplot在一张图中画多幅图；[subplot文档](https://matplotlib.org/api/pyplot_api.html#matplotlib.pyplot.subplot) import numpy as np import matplotlib.pyplot as plt # Compute the x and y coordinates for points on sine and cosine curves x = np.arange(0, 3 * np.pi, 0.1) y_sin = np.sin(x) y_cos = np.cos(x) # Set up a subplot grid that has height 2 and width 1, # and set the first such subplot as active. plt.subplot(2, 1, 1) # Make the first plot plt.plot(x, y_sin) plt.title(&apos;Sine&apos;) # Set the second subplot as active, and make the second plot. plt.subplot(2, 1, 2) plt.plot(x, y_cos) plt.title(&apos;Cosine&apos;) # Show the figure. plt.show() # 图像读入与显示(imread/imwrite) import numpy as np from scipy.misc import imread, imresize import matplotlib.pyplot as plt img = imread(&apos;assets/cat.jpg&apos;) img_tinted = img * [1, 0.95, 0.9] # Show the original image plt.subplot(1, 2, 1) plt.imshow(img) # Show the tinted image plt.subplot(1, 2, 2) # A slight gotcha with imshow is that it might give strange results # if presented with data that is not uint8. To work around this, we # explicitly cast the image to uint8 before displaying it. plt.imshow(np.uint8(img_tinted)) plt.show()]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>公开课</tag>
        <tag>计算机视觉</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Daily Reading]]></title>
    <url>%2F2018%2F02%2F10%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%2FDaily-Reading%2F</url>
    <content type="text"><![CDATA[记录一下每天看的公众号和知乎相关内容 2017-12-261. 十大机器学习Python库(机器之心) 大部分都没见过，可以看看 2017-12-271. 如何在NLP领域干成第一件事(AI科技评论) 找开源项目并重现，争取进一步改善；找如ACL会议论文集，找感兴趣的方向8，了解该方向的方法/数据/团队； 2. Python相关文章(AI科技评论)* 静态代码检查工具Flake；视频合成器；从零开始用遗传算法和深度学习演化有机体的生长过程；破解验证码；Chutes&amp;Ladders游戏模拟；创建Chatbot；图像散列；介绍BP；Memorization技术，加速Python；分享Python学习技巧 3. Attention模型(机器学习算法与自然语言处理) 各种Attention机制的综述 4. 神经进化策略(机器之心)* 可能未来使用神经进化策略替代反向传播，了解一下 2017-12-281. CS中的线代(机器之心)* 一篇普渡的论文，45页 2. 2018AI研究趋势(机器之心)** 汇总资源：开放资源(介绍了DeepMind/OpenAI等的博客，着重推荐Distill)；机器翻译；理解视频(预测下一帧等+数据集介绍)；多任务/多模式学习；强化学习(游戏Dota/星际争霸)；解释人工智能(可视化/InterpretNet)；保护人工智能被对抗样本愚弄；超越梯度(合成梯度/进化策略/SGD改进/学习优化/不同空间的优化)；3D和图形的几何深度学习 3. FoolNLTK：一个便捷的中文处理工具(机器之心) 基于BiLSTM的开源中文分词模型 4. NIPS 2017的收获与思考(AI科技评论) 3篇Best paper：博弈论相关，打败德州扑克顶级玩家；随机优化风险最小化问题；提出全新的拟合优度算法。都好高端，不知道讲的是什么这篇文章作者主要关注了强化学习； *还介绍了4篇MSRA的paper：有一篇关于机器翻译的论文可能可以用在LipNet项目中？ 5. 如何理解hekaiming的Focal Loss(PaperWeekly) 解决分类问题中类别不平衡、分类难度差异的一个loss 6. 草根学Python(机器学习研究会)** 寒假和笨方法学Python一起看 2017-12-291. 2017最火的五篇深度学习论文(专知) CycleGAN-图像迁移；Wasserstein GAN-提出更好的用于训练GAN的目标函数；simGAN-产生模拟数据，使用未标记的真实数据来改进模拟数据(无监督的)AlphaGo zero-无人类知识先验的情况下学会下围棋深度图像先验-理解神经网络模型中先验的作用(没搞懂) 2. 机器学习、NLP、Python和Math最好的150余个教程(AI科技大本营)** 资源汇总 2017-12-301. ICCV2017: 基于检测和跟踪的视频中人体姿态估计(专知)* 人体姿态估计相关 2. 如何与深度学习服务器优雅的交互(夕小瑶)** 服务器相关操作 3. 机器之心年度盘点：2017年人工智能领域备受关注的科研成果(机器之心) AlphaGo-无须人类知识标注，自我对抗德州扑克击败人类-深度学习/纳什均衡的博弈求解自归一化-比BN更好的归一化GAN和各种变体深度神经网络碰上语音合成-WaveNet大批量数据并行训练ImageNet-分布式同步SGD训练(将ResNet-50在ImageNet上的训练时间缩短到48分钟)Capsule-抛弃反向传播递归皮质网络-新型概率生成模型，旨在超越神经网络从TPU到NPU-NPU是华为麒麟970手机端的芯片 4. 机器学习非凸优化技术(机器之心)5. 在线深度学习：在数据流中实时学习深度神经网络(机器之心) 2018-02-101. 想要实现深度神经网络？一张 Excel 表格就够了(机器之心) 链接 有一个比较完整的CNN流程，关于CNN的一些理解：链接 2. Facebook提出DensePose数据集和网络架构：可实现实时的人体姿态估计(机器之心)* 链接 人体姿态估计相关 2018-02-111. 从语义上理解卷积核行为，UCLA朱松纯等人使用决策树量化解释CNN(机器之心)* 链接 解释CNN模型：借助决策树在语义层面上解释 CNN 做出的每一个特定预测，即哪个卷积核（或物体部位）被用于预测最终的类别，以及其在预测中贡献了多少。斯坦福也有类似的工作。 2. 李沐《动手学深度学习》课程视频汇总(机器之心) 链接 使用Apache MXNet的最新前端Gluon作为开发工具。视频链接；中文文档 3. 一文读懂什么是变分自编码器(专知) 链接 AE到VAE 2018-02-121. 机器学习的经典算法 链接 回归算法：线性回归(数值问题，最小二乘法，使用梯度下降逼近求解函数极值问题) &amp; 逻辑回归(分类问题，结果中加一个Sigmoid函数) 用于拟合逻辑回归中非线性分类线的两种算法：神经网络(ANN) &amp; SVM*(和高斯“核”结合，表达复杂的分类界限。核最典型的特征就是可以将低维的空间映射到高维空间，低维空间的非线性分类线就相当于高维空间的线性分类线) 聚类算法：无监督的一种，K-Means。见3 降维算法：无监督的一种，PCA* 推荐算法：基于物品内容的推荐（将与用户购买的内容近似的物品推荐给用户） &amp; 基于用户相似度的推荐（将与目标用户兴趣相同的其他用户购买的东西推荐给目标用户），协同过滤算法* 其他：高斯判别、朴素贝叶斯、决策树 2. ​爬虫与反爬虫 链接 有趣的爬虫与反爬虫工作介绍 3. 六大聚类算法 链接 参看博文六大聚类算法 4. 最新云端&amp;单机GPU横评 链接 性能：Volta性能优于Nvidia 1080Ti（约1.1-1.3倍）和P100（约1.2-1.5倍） 成本：Paperspace Volta性价比高。Google P100比Paperspace Volta贵10%，Amazon Volta比Paperspace Volta贵40% Paperspace Volta适合只需要1个GPU的用户，性能好 Google P100最为灵活，它允许用户在任意实例上使用1、2、4个P100 GPU（或最多 8个K80 GPU），允许用户自定义CPU和GPU配置来满足计算需求。尽管由于架构所限，Tesla P100的性能略显落后，但从成本角度考虑，其性价比很有优势 Amazon Volta性能优于Google P100，也可以连接1、4或8个GPU。但用户无法自定义基础实例类型。性价比比较低。如果迫切需要用8个GPU或在EC2上搭建模型，那么目前仍推荐使用Amazon Volta 5. 理解深度学习中的矩阵运算* 链接 -论文链接 6. 最新7篇VAE相关论文 链接 -利用带混合解码器的条件变分自编码器生成主题汉语诗歌 一种使用条件变分自编码器的Zero Shot Learning的生成模型 变分自编码器在不同模态之间的双向生成 使用深度密度先验的MR图像重建 变分递归神经机器翻译 变分自编码器的推断次优性 使用条件VAEs和GANs从视觉属性中合成人脸 7. UC Berkeley提出特征选择新方法：条件协方差最小化* 链接 模型解释性。这篇论文的综述部分可以看一下，论文本身数学性比较强，Github 降维可以增强模型的可解释性，特征选择则是常用的降维方法。特征选择算法通常可分为：滤波器（filter）、封装（wrapper）以及嵌入（embedded）。 滤波器方法基于数据的本质属性选择特征，与所用的学习算法无关。如可以计算每个特征和响应变量之间的相关性，然后选择相关性最高的变量 封装方法的目标是寻找能够使某个预测器的性能最优化的特征。如可以训练多个支持向量机，每个支持向量机使用不同的特征子集，然后选择在训练数据上损失最小的特征子集。因为特征子集的数量是指数规模的，所以封装方法通常会使用贪心算法 嵌入方法将特征选择和预测结合成一个问题。它通常会优化一个目标函数，这个目标函数结合了拟合优度和对参数数量的惩罚。一个例子就是构建线性模型的LASSO方法，它用L1 penalty来表征对参数数目的惩罚 本文提出了条件协方差最小化（CCM）方法，这是一个统一前两个观点的特征选择方法。这个方法基于最小化条件协方差算子的迹来进行特征选择。思想是选择能够最大化预测基于协变量响应依赖的特征。 8. 深度学习的7大实用技巧* 链接 参看博文深度学习技巧 2018-02-131. 机器学习中的模型评价、模型选择及算法选择* 链接 参看博文机器学习中的模型评价与选择 2. 波士顿动力的机器人会开门了，中国尚需奋力追赶 链接 里面的几个视频非常有趣 3. R语言入门指导 链接 比较细比较初级的教程 4. 谷歌云TPU服务正式全面开放 链接 张量处理器（Tensor Processing Unit，TPU）是机器学习专属芯片，去年又推出了第二代产品（Cloud TPU）。TPU论文 谷歌云平台（GCP）提供Cloud TPU beta版自2018年2月12日起用，旨在帮助机器学习专家更快地训练和运行ML模型 云TPU如今在数量受限的情况下可用，价格以秒计费，大约为每云TPU每小时6.50美元 2018-02-141. 多维(1-6)数据可视化策略* 链接 -可视化资源：pandas、matplotlib、seaborn、plotly、bokeh库；D3.js；The Visual Display of Quantitative Information.pdf -教程的Github资源 数据集：UCI机器学习库中的Wine Quality Data Set，分为葡萄酒中红色和白色酒 框架：matplotlib和seaborn 一维数据：利用pandas画直方图、核密度图等 二维数据：配对相关性矩阵（pair-wise correlation matrix）并将其可视化为热力图；在感兴趣的属性之间使用配对散点图；平行坐标图；散点图；联合分布图等等 多维数据：高于3维的最好方法是使用图分面、颜色、形状、大小、深度等。还可以使用时间作为维度，为随时间变化的属性制作一段动画。每多一个维度，就使用一种新的表征，如色调、深度、大小等 2018-02-151. 从LeNet到SENet——卷积神经网络回顾** 链接 2018-02-161. 2017年机器之心AI高分概述文章全集** 链接 去年机器之心的盘点：链接 非常好的一份教程全集，有时间多看看 2. 谷歌大脑Wasserstein自编码器：新一代生成模型算法 链接 ICLR 2018. 具有VAE的一些优点，也结合了GAN结构的特性，可以实现更好的性能 3. 机器学习研究的12个宝贵经验 链接 关于机器学习的一些经验分享，非调参经验 2018-02-171. 区块链vs传统数据库：分布式运行有何优势 链接 区块链是一种容错率很高的分布式数据存储模式 去中心化控制消除了中心化控制的风险。任何能够充分访问中心化数据库的人都可以摧毁或破坏其中的数据，因此用户依赖于数据库管理员的安全基础架构。区块链技术使用去中心化数据存储来避开这一问题，从而在自己的结构中建立安全性。区块链技术很适合记录某些种类的信息，传统数据库更适合记录另外一些种类的信息。对于每个组织而言，理解它想从数据库中获得什么非常关键，我们需要在选择数据库之前，判断每种数据库的优缺点 2018-02-181. 胶囊网络为何如此热门 链接 CapsNets需要的训练数据少； CapsNets能更好地处理图像多义性表达 CapsNets具有“同变性”，输入的微小变化会导致输出的细微变化。即详细的姿态信息在整个网络中都被保留 CNNs需要额外的组件来实现自动识别一个部件归属于哪一个对象（如，这条腿属于这只羊）。而CapsNets则免费提供部件的层次结构 CapsNets在如CIFAR10或ImageNet大规模图像测试集上的表现不如CNNs好；需要大量计算，不能检测出相互靠近的同类型的两个对象（这被称为“拥挤问题”，且已被证明人类也存在这个问题） CapsNets的主要思想还是非常有前途的，似乎只需要一些调整就可以发挥全部潜力 2018-02-191. 神经网络“剪枝”的两个方法 链接 一般用于减少网络非零参数数量的方法有三种： (1) 正则化(regularization)：修改目标函数，如使用L0范数 论文(2) 修剪(pruning)：面向大规模神经网络，并删除某些意义上冗余的特征或参数 论文(3) 增长(growing)：从小型网络开始，按某种增长标准逐步增加新的单元 减少非零参数的目的如下： (1) 保持相同性能的前提下降低计算成本，加速推断和训练(2) 减少参数数量可以减少参数空间的冗余，从而提高泛化能力 这篇文章介绍了两篇近期关于神经网络修剪的论文，分别是 L_0 正则化方法和 Fisher 修剪方法 2018-02-201. 从1400篇机器学习文章中精选出Top 10 链接 Google Brain去年干了太多事，Jeff Dean一篇长文都没回顾完 一文详解如何使用Python和Keras构建属于你的 AlphaZero AI 通过概况长序列来生成维基百科 深度学习必备的矩阵微积分知识 一种值得采纳的全局优化算法 Tensorflow-Project-Template：tensorflow项目模板架构的最佳实践 如何解决90％的自然语言处理问题：分步指南奉上 CheXNet：一次深入的回顾 机器学习新手顶级算法之旅 学习数据科学、机器学习与AI没有多大交集，一文告诉你三者最大区别 2. 区块链技术综述* 链接 文章通过解构区块链的核心要素，提出了区块链系统的基础架构模型，详细阐述了区块链及与之相关的比特币的基本原理、技术、方法与应用现状，讨论了智能合约的理念、应用和意义 3. 超越Adam，从适应性学习率家族出发解读ICLR 2018高分论文 链接 讨论了 Adam 等适应性学习率算法的收敛性缺点，并提出了一种新的 Adam 变体 4. Git 的4个阶段的撤销更改 链接 工作流分为工作区、暂存区、本地仓库、远程仓库 git add . 把所有文件放入暂存区；git commit 把所有文件从暂存区提交进本地仓库；git push把所有文件从本地仓库推送进远程仓库 git diff / git diff –cached / git diff master origin/master查看目前在工作流的哪个状态 撤销 (1) 编辑器中修改了文件，还未执行git add .：git checkout .或git reset –hard(2) 执行了git add .，但还没有执行git commit：git reset-&gt;git checkout .或直接git reset –hard(3) git commit也执行了，但还没有push：git reset –hard origin/master(既然污染了本地仓库，那就从远程仓库复制到本地)(4) push也执行了：git reset –hard HEAD^ -&gt; git push -f(先恢复本地仓库，再push到远程仓库)]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>论文阅读</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Pytorch学习]]></title>
    <url>%2F2018%2F02%2F10%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%2FPytorch%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[记录一下Pytorch学习过程以及踩过的一些坑 Pytorch官方文档 Pytorch官方教程 由于教程中有太多的代码了，因此这里就只记录看过的内容的索引，方便以后自己写代码的时候回来找 60分钟教程 链接What is Pytorch 介绍Tensors 对于Tensor的操作 Tensor和Numpy的相互转化 Torch的Tensor和Numpy数组使用的是同一块内存空间，改变其中一个也会影响另一个（指的是Numpy数组的内容是用a.numpy()赋值给b的情况） 把Tensor移到GPU Autograd: automatic differentiation（用于反向传播） Variable Variable是核心类，包含Tensor并支持在Tensor上的一切操作.data访问数据；.grad访问梯度(如x.grad访问的是目标函数对于x的梯度)；.grad_fn涉及创建该变量的函数(如b=a+2，那么b.grad_fn就是a+2这个函数)，若变量是人为直接定义的，那.grad_fn就是None Gradients If you want to compute the derivatives, you can call .backward() on a Variable. If Variable is a scalar (i.e. it holds a one element data), you don’t need to specify any arguments to backward(), however if it has more elements, you need to specify a grad_output argument that is a tensor of matching shape.上面那段话在教程中相关的例子： out.backward() # is equivalent to doing out.backward(torch.Tensor([1.0])) gradients = torch.FloatTensor([0.1, 1.0, 0.0001]) y.backward(gradients) # 相当于求得的梯度中，不同维度的值会乘上0.1/1.0/0.0001 Neural Networks（搭建网络） 坑 x.view(-1, 8)是改变x的维度，如原来x是16*1，有16个元素，使用view(-1, 8)后为了保证还是16个元素，-1那一维就是16/8=2]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>Pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Welcome to my blog]]></title>
    <url>%2F2018%2F02%2F09%2F%E9%9A%8F%E7%AC%94%2FWelcome-to-my-blog%2F</url>
    <content type="text"><![CDATA[一直想搞个博客，但是一直懒得搞，拖延癌晚期/(ㄒoㄒ)/~~。。之前搞过一个jekyll的，这次想了想还是换hexo了 搭建细节主要参考了http://blog.csdn.net/gdutxiaoxu/article/details/53576018 大致流程如下： 安装node.js 本地安装Hexo 配置Hexo 把Hexo和Github Page联系 随后就是换主题、搞一下私人定制的事情啦，接下来几天慢慢看看官方文档，把这个博客做的好看一点o(￣▽￣)ブ 2018-02-10 update新添站内搜索功能、生成站点地图功能，参考https://www.ezlippi.com/blog/2017/02/hexo-search.html**删除安装的插件直接npm unistall 插件名即可根据官方文档通过修改_config.yml添加了一系列功能 2018-02-12 update在博文中添加图片，参考链接在博文中的写法：Markdown中写注释，参考链接 待添加：评论功能(next继承来必力)；http://blog.csdn.net/qq_33699981/article/details/72716951To be continued…]]></content>
      <categories>
        <category>随笔</category>
      </categories>
      <tags>
        <tag>随笔</tag>
      </tags>
  </entry>
</search>
