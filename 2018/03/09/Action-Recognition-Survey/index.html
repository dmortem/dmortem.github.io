<!DOCTYPE html>



  


<html class="theme-next gemini use-motion" lang="en">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="计算机视觉,深度学习,论文阅读,Action Recognition," />










<meta name="description" content="关于Action Recognition领域的综述，Going Deeper into Action Recognition: A Survey 链接">
<meta name="keywords" content="计算机视觉,深度学习,论文阅读,Action Recognition">
<meta property="og:type" content="article">
<meta property="og:title" content="Action Recognition Survey">
<meta property="og:url" content="dmortem.github.io/2018/03/09/Action-Recognition-Survey/index.html">
<meta property="og:site_name" content="Chiyu Wu">
<meta property="og:description" content="关于Action Recognition领域的综述，Going Deeper into Action Recognition: A Survey 链接">
<meta property="og:locale" content="en">
<meta property="og:image" content="/2018/03/09/Action-Recognition-Survey/1.png">
<meta property="og:image" content="/2018/03/09/Action-Recognition-Survey/7.png">
<meta property="og:image" content="/2018/03/09/Action-Recognition-Survey/8.png">
<meta property="og:image" content="/2018/03/09/Action-Recognition-Survey/2.png">
<meta property="og:image" content="/2018/03/09/Action-Recognition-Survey/3.png">
<meta property="og:image" content="/2018/03/09/Action-Recognition-Survey/4.png">
<meta property="og:image" content="/2018/03/09/Action-Recognition-Survey/5.png">
<meta property="og:image" content="/2018/03/09/Action-Recognition-Survey/6.png">
<meta property="og:image" content="/2018/03/09/Action-Recognition-Survey/10.png">
<meta property="og:image" content="/2018/03/09/Action-Recognition-Survey/11.png">
<meta property="og:image" content="/2018/03/09/Action-Recognition-Survey/9.png">
<meta property="og:image" content="/2018/03/09/Action-Recognition-Survey/12.png">
<meta property="og:updated_time" content="2018-03-13T16:00:00.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Action Recognition Survey">
<meta name="twitter:description" content="关于Action Recognition领域的综述，Going Deeper into Action Recognition: A Survey 链接">
<meta name="twitter:image" content="/2018/03/09/Action-Recognition-Survey/1.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="dmortem.github.io/2018/03/09/Action-Recognition-Survey/"/>





  <title>Action Recognition Survey | Chiyu Wu</title>
  





  <script type="text/javascript">
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?44f4925176b4eb14cc21af08b3c64555";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>




</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Chiyu Wu</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            About
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      
        
        <li class="menu-item menu-item-sitemap">
          <a href="/sitemap.xml" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-sitemap"></i> <br />
            
            Sitemap
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            Search
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off"
             placeholder="Searching..." spellcheck="false"
             type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="dmortem.github.io/2018/03/09/Action-Recognition-Survey/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Chiyu Wu">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/uploads/image.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Chiyu Wu">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">Action Recognition Survey</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-03-09T16:40:36+08:00">
                2018-03-09
              </time>
            

            
              <span class="post-meta-divider">|</span>
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-check-o"></i>
              </span>
              
                <span class="post-meta-item-text">Post modified&#58;</span>
              
              <time title="Post modified" itemprop="dateModified" datetime="2018-03-14T00:00:00+08:00">
                2018-03-14
              </time>
            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/计算机视觉/" itemprop="url" rel="index">
                    <span itemprop="name">计算机视觉</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          
            <span class="post-meta-divider">|</span>
            <span class="page-pv"><i class="fa fa-file-o"></i>
            <span class="busuanzi-value" id="busuanzi_value_page_pv" ></span>
            </span>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>关于Action Recognition领域的综述，Going Deeper into Action Recognition: A Survey <a href="https://www.sciencedirect.com/science/article/pii/S0262885617300343" target="_blank" rel="noopener">链接</a></p>
<a id="more"></a>
<h2 id="What-is-an-action"><a href="#What-is-an-action" class="headerlink" title="What is an action?"></a>What is an action?</h2><ul>
<li>Action is the most elementary human 1-surrounding interaction with a meaning.</li>
</ul>
<h2 id="Taxonomy"><a href="#Taxonomy" class="headerlink" title="Taxonomy"></a>Taxonomy</h2><img src="/2018/03/09/Action-Recognition-Survey/1.png">
<h3 id="Representation-based-Solutions"><a href="#Representation-based-Solutions" class="headerlink" title="Representation based Solutions"></a>Representation based Solutions</h3><ul>
<li>Earliest works in action recognition use <strong>3D models</strong> to describe actions</li>
<li>Constructing 3D models is difficult and expensive</li>
</ul>
<h4 id="Holistic-Representation"><a href="#Holistic-Representation" class="headerlink" title="Holistic Representation"></a>Holistic Representation</h4><ul>
<li><p>Motion Energy Image (<strong>MEI</strong>) and Motion History Image (<strong>MHI</strong>). “Bobick and Davis, 2001”</p>
<blockquote>
<p>The MHI template shows how the motion image is moving. Each pixel in MHI is a function of the temporal history of the motion at that point (i.e., higher intensities correspond to more recent movements)</p>
</blockquote>
<img src="/2018/03/09/Action-Recognition-Survey/7.png">
</li>
<li><p>The volumetric <strong>extension of MEI templates</strong>. （见下左图） “Blank, 2005” </p>
<blockquote>
<p>represent an action by a 3D shape induced from its silhouettes in the space-time</p>
</blockquote>
</li>
<li><p>Space-Time Volume (<strong>STV</strong>) “Yilmaz and Shah, 2005”</p>
<blockquote>
<p>An STV is build by stacking the object contours along the time axis</p>
</blockquote>
<img src="/2018/03/09/Action-Recognition-Survey/8.png">
</li>
<li><p><strong>Holistic representations flooded the research in action recognition roughly between 1997 to 2007. However, nowadays local and deep representations are favored</strong> </p>
</li>
</ul>
<h4 id="Local-Representation"><a href="#Local-Representation" class="headerlink" title="Local Representation"></a>Local Representation</h4><ul>
<li>interest point detection -&gt; local descriptor extraction -&gt; aggregation of local descriptors (从找到一个感兴趣点，到从兴趣点周围的一片区域提取一个描述子，再到把一堆局部描述子聚合在一起)</li>
</ul>
<h5 id="Interest-point-detection"><a href="#Interest-point-detection" class="headerlink" title="Interest point detection"></a>Interest point detection</h5><ul>
<li><p>3D-Harris Detector “Laptev, 2005”</p>
<blockquote>
<p>The 3D-Harris detector identifies points with large spatial variations and non-constant motions</p>
</blockquote>
</li>
<li><p>3D-Hessian Detector “Willems, 2008”</p>
</li>
<li><p>In certain domains, e.g., facial expressions, true spatiotemporal corners are quite <strong>rare</strong>, even if an interesting motion is occurring</p>
<blockquote>
<p>disintegrate spatial filtering from the temporal one</p>
</blockquote>
</li>
<li><p><strong>action clips</strong> are more likely to be obtained in uncontrolled environments</p>
<blockquote>
<p>a shaky camera can fire a series of irrelevant interest points<br>prune irrelevant features using statistical properties of the detected interest points</p>
</blockquote>
</li>
<li><p>spatiotemporal features obtained from <strong>background</strong>, known as static features, especially the ones that are near motion regions are <strong>useful</strong> for action recognition</p>
</li>
</ul>
<h5 id="Local-Desscriptors"><a href="#Local-Desscriptors" class="headerlink" title="Local Desscriptors"></a>Local Desscriptors</h5><ul>
<li><p>To obtain the local descriptor at an interest point, earlier works almost unanimously opt for cuboids. Later people introduced the notion of <strong>trajectories</strong></p>
</li>
<li><p>Edge and Motion Descriptors</p>
<blockquote>
<p>Histogram of Gradient Orientations (<strong>HOG3D</strong>) “Klaser, 2008”<br>Optical Flow Fields: Histogram of Optical Flow (<strong>HoF</strong>) “Laptev, 2008” &amp; Motion Boundary Histogram (<strong>MBH</strong>) “Dalal, 2006”</p>
</blockquote>
</li>
<li><p>Pixel Pattern Descriptors</p>
<blockquote>
<p>Volume Local Binary Patterns (<strong>VLBP</strong>) “Zhao, 2007” &amp; Local Binary Pattern histograms from Three Orthogonal Planes (<strong>LBP-TOP</strong>) “Kellokkumpu, 2008”<br>To describe a region R in an image, first use low-level features or mid-level features to extract a set of features zi, then use d*d covariance matrix of zi (RCD) as the descriptor for region R “Tuzel, 2006”</p>
</blockquote>
</li>
<li><p>From Cuboids to Trajectories</p>
<blockquote>
<p>An spatiotemporal interest point might not reside at the exact same spatial location within the temporal extends of a cuboid.<br>A trajectory is a properly tracked feature over time</p>
</blockquote>
</li>
<li><p>Use <strong>dense</strong> interest points instead of sparse</p>
</li>
</ul>
<h5 id="Aggregation"><a href="#Aggregation" class="headerlink" title="Aggregation"></a>Aggregation</h5><h3 id="Deep-Architectures-for-Action-Recognition"><a href="#Deep-Architectures-for-Action-Recognition" class="headerlink" title="Deep Architectures for Action Recognition"></a>Deep Architectures for Action Recognition</h3><ul>
<li>Spatiotemporal networks (空间+时间；3D-CNN)</li>
<li>Multiple stream networks (拆分为空间流和时间流)</li>
<li>Deep generative networks (无监督)</li>
<li>Temporal coherency networks (作为预训练网络)</li>
</ul>
<h4 id="Spatiotemporal-Networks-From-3D-CNN-to-3D-CNN-LSTM"><a href="#Spatiotemporal-Networks-From-3D-CNN-to-3D-CNN-LSTM" class="headerlink" title="Spatiotemporal Networks (From 3D-CNN to 3D-CNN+LSTM)"></a>Spatiotemporal Networks (From 3D-CNN to 3D-CNN+LSTM)</h4><ul>
<li><p>Arm the CNN with temporal information —— <strong>3D CNN</strong>. Use 3D kernels(filter extended along the time axis) to extract features from both spatial and temporal dimension. “<a href="http://ieeexplore.ieee.org/abstract/document/6165309/" target="_blank" rel="noopener">3D Convolutional Neural Networks for Human Action Recognition</a>“</p>
<img src="/2018/03/09/Action-Recognition-Survey/2.png">
<blockquote>
<p>3D CNN have a very rigid temporal structure. The network accepts a predefined number of frames as the input.<br>在空间上可以通过pooling来解决fixed spatial dimension，但是时间上为什么可以还没有解释；同时对于不同动作，输入的帧数也不明确</p>
</blockquote>
</li>
<li><p>How temporal information should be fed into CNN ? (Different <strong>Fusion Schemes</strong>)</p>
<ul>
<li>Max-pooling更好. “Ng, 2015”</li>
<li><strong>Slow Fusion</strong>: CNN accepts several consecutive parts of the video, and processes them through the very same set of layers to produce responses across temporal domain. These responses are then processed by fully connected layers to produce the video descriptor. <a href="https://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Karpathy_Large-scale_Video_Classification_2014_CVPR_paper.pdf" target="_blank" rel="noopener">“Karpathy, 2014”</a><img src="/2018/03/09/Action-Recognition-Survey/3.png"></li>
<li><strong>Early Fusion</strong>: the network is fed with a set of adjacent frames. “<a href="http://ieeexplore.ieee.org/abstract/document/6165309/" target="_blank" rel="noopener">3D Convolutional Neural Networks for Human Action Recognition</a>“</li>
<li><strong>Late Fusion</strong>: frame-wise features are fused at the last layer. “Karpathy, 2014”<blockquote>
<p>Multi-resolutional approach using two separate networks not only boosts the accuracy, but also reduces the number of parameterss to be learned. (见上右图) “Karpathy, 2014”</p>
</blockquote>
</li>
</ul>
</li>
<li><p>Find <strong>generic video descriptors based on a 3D convolutional network</strong> (C3D). “Tran 2015”</p>
<ul>
<li>A network with 3 × 3 × 3 homogeneous filters (<strong>constant depth</strong> at every layer) performs better than varying the temporal depth on filters. Flexibility on the temporal extent is obtained with the inclusion of 3D pooling layers. </li>
<li>Obtained by averaging the outputs of the first fully connected layer.</li>
</ul>
</li>
</ul>
<ul>
<li><p>Improvements are observed by <strong>extending the temporal depth of the input</strong> as well as <strong>combining the decision of networks with different temporal awareness at the input</strong>. “Varol, 2016”</p>
</li>
<li><p><strong>Factorizing a 3D filter into a combination of a 2D and 1D filters</strong>, solving the problem of increasing the number of parameters of the 3D-CNN network. </p>
</li>
<li><p>Feed an <strong>LSTM</strong> network with features extracted from a 3D convolutional network. The two networks are trained separately. “Baccouche, 2011”</p>
<img src="/2018/03/09/Action-Recognition-Survey/4.png">
</li>
<li><p>Another <strong>end-to-end</strong> architecture based on <strong>LSTM</strong> named LRCN. (下图a) “Donahue, 2015”</p>
<ul>
<li>Group is a set of convolutional filters operating only on a particular set of feature maps from the previous layer.</li>
</ul>
</li>
</ul>
<h4 id="Multiple-Stream-Networks"><a href="#Multiple-Stream-Networks" class="headerlink" title="Multiple Stream Networks"></a>Multiple Stream Networks</h4><ul>
<li>A class of deep neural networks is devised to separate <strong>appearance</strong> based information from <strong>motion</strong> related ones for action recognition. “Simonyan, 2014” <ul>
<li>Multiple-stream deep CNN, with <strong>two parallel networks</strong></li>
<li><strong>Spatial</strong> stream network accepts raw video frames while the <strong>temporal</strong> stream network gets optical flow fields as input</li>
<li>Fine-tuning a pretrained network on the ILSVRC-2012 image dataset (Russakovsky et al., 2015) leads to higher accuracy</li>
<li>Stacking optical flow fields at the input of the temporal stream network (i.e., early fusion) is beneficial</li>
<li>The temporal stream network is modified to have more than one classification layer. Each classification layer operates on a specific dataset, aiming to <strong>learn a representation</strong>, which is not only applicable to the task in question, but also to other tasks.</li>
<li>The two streams are fused together using the softmax scores</li>
</ul>
</li>
</ul>
<ul>
<li><p><strong>Fusion at an intermediate layer</strong> not only improves the performance but also reduces the number of parameters significantly “Feichtenhofer, 2016”</p>
<ul>
<li>having the fusion right after the convolutional layers will remove the requirement of costly fully connected layers in both streams<img src="/2018/03/09/Action-Recognition-Survey/5.png">
</li>
</ul>
</li>
<li><p><strong>Extension</strong> of the two stream network</p>
<ul>
<li>Dense trajectories traced over convolutional feature maps of the two-stream network are aggregated using the Fisher vector “Wang, 2015”</li>
<li>A third stream using audio signal is added to the network “Wu, 2015”</li>
</ul>
</li>
</ul>
<ul>
<li>The <strong>optical flow frames</strong> are <strong>the only motion related information</strong> used in two stream networks “Feichtenhofer, 2016”<ul>
<li>Optical flow cannot capture subtle but long-term motion dynamics</li>
<li>Certain details in actions are still out-of-reach in deep solutions</li>
</ul>
</li>
</ul>
<h4 id="Deep-Generative-Models"><a href="#Deep-Generative-Models" class="headerlink" title="Deep Generative Models"></a>Deep Generative Models</h4><ul>
<li><strong>Dynencoder</strong>: a class of deep auto-encoders to capture video dynamics<ul>
<li>Has three layers: xt -&gt; ht -&gt; ht+1 -&gt; xt+1</li>
<li>To reduce the training complexity, the parameters of the network are learned in two stages. In the pretraining stage, each layer is trained separately. Once pretraining is completed, an end-to-end fine tuning is performed</li>
<li>The <strong>reconstruction error</strong> of a video given a Dynencoder can be used as a mean for classification</li>
</ul>
</li>
</ul>
<ul>
<li><strong>LSTM Autoencoder Model</strong><ul>
<li>Consist of the <strong>encoder LSTM</strong> and <strong>the decoder LSTM</strong></li>
<li>The states of the encoder LSTM contain the appearance and dynamics of the sequence</li>
<li>Has <strong>reconstructive decoder</strong> and <strong>predictive decoder</strong><img src="/2018/03/09/Action-Recognition-Survey/6.png">
</li>
</ul>
</li>
</ul>
<ul>
<li><strong>Adversarial Models</strong><ul>
<li>The discriminative model learns to determine whether a sample is coming from the generative model or the data itself.</li>
</ul>
</li>
</ul>
<h4 id="Temporal-Coherency-Networks"><a href="#Temporal-Coherency-Networks" class="headerlink" title="Temporal Coherency Networks"></a>Temporal Coherency Networks</h4><ul>
<li>Temporal Coherency is a form of <strong>weak supervision</strong></li>
<li>Temporal coherency states that consecutive video frames are correlated both semantically and dynamically (i.e., abrupt motions are less likely)</li>
<li>Temporal coherency is not always a strong assumption to rely on</li>
<li><p><strong>Siamese Network</strong> (Chopra, 2005; Varior, 2016; Lu, 2016) is trained with tuples to determine whether a given sequence is coherent or not. </p>
<blockquote>
<p>Give more attention to human poses<br>Avoid ambiguities between positive and negative tuples<br>Compared to networks trained from scratch, pretrained networks based on the temporal coherency have potential to improve the accuracy</p>
</blockquote>
<img src="/2018/03/09/Action-Recognition-Survey/10.png">
</li>
<li><p>Action is split into two phases for classification, the precondition set Xp and the effect set Xe.</p>
<blockquote>
<p>An action is identified by the transformation required to map a high-level descriptor extracted from Xp to a high-level descriptor extracted from Xe.</p>
</blockquote>
<img src="/2018/03/09/Action-Recognition-Survey/11.png">
</li>
<li><p>Rank pooling (Fernando, 2015) is an effective solution for capturing temporal evolution<br>of a sequence. </p>
</li>
</ul>
<h2 id="Datasets"><a href="#Datasets" class="headerlink" title="Datasets"></a>Datasets</h2> <img src="/2018/03/09/Action-Recognition-Survey/9.png">
<ul>
<li>No universal solution for all datasets</li>
<li><p>the <strong>KTH</strong> and the <strong>Weizmann</strong> datasets contain human actions in controlled conditions, and their scope is limited to basic actions such as walking, running and jumping. </p>
<blockquote>
<p>comparing solutions on the KTH and Weizmann datasets is <strong>less insightful</strong> unless a specific need is considered.</p>
</blockquote>
</li>
<li><p><strong>HMDB-51</strong> and <strong>UCF-101</strong> datasets contain camera motion (and shakes), viewpoint variations and resolution inconsistencies.</p>
<blockquote>
<p>these datasets are not well-suited for measuring the performance of <strong>action localization</strong></p>
</blockquote>
</li>
<li><p><strong>Hollywood2</strong> and <strong>Sports-1M</strong> datasets contain view-point/editing complexities (sudden viewpoint variations in the video streams)</p>
<blockquote>
<p>methods that rely on <strong>temporal coherency</strong> may fail on Sports-1M</p>
</blockquote>
</li>
<li><p>Algorithms benefiting from <strong>object details</strong> are expected to perform better</p>
</li>
<li><p>For <strong>Deep Learning</strong>, <strong>Sports-1M</strong> dataset is great while <strong>KTH and Wiezmann</strong> often leads to unsatisfactory performance</p>
</li>
</ul>
<h2 id="Performance"><a href="#Performance" class="headerlink" title="Performance"></a>Performance</h2> <img src="/2018/03/09/Action-Recognition-Survey/12.png">
<ul>
<li>the state-of-the-art solutions based on both representation and deep learning perform <strong>equally well</strong><blockquote>
<p>One reason is the <strong>insufficiency of data</strong><br>A dominant theme to get around this limitation is to benefit from <strong>models pre-trained on images</strong></p>
</blockquote>
</li>
</ul>
<h2 id="Current-State-of-the-art-solutions"><a href="#Current-State-of-the-art-solutions" class="headerlink" title="Current State-of-the-art solutions"></a>Current State-of-the-art solutions</h2><h3 id="Handcrafted-Solutions"><a href="#Handcrafted-Solutions" class="headerlink" title="Handcrafted Solutions"></a>Handcrafted Solutions</h3><ul>
<li><strong>Dense trajectory descriptors</strong>, incorporated in various pooling strategies such as <strong>FV</strong>’s and <strong>Rank-Pooling</strong></li>
</ul>
<h3 id="Deep-Net-Solutions"><a href="#Deep-Net-Solutions" class="headerlink" title="Deep-Net Solutions"></a>Deep-Net Solutions</h3><ul>
<li>the <strong>spatiotemporal networks</strong> and <strong>two-stream networks</strong> outperform other network structures<blockquote>
<p>Equipped with 3D convolution filters<br>Training deeper networks demands more rigorous data augmentation techniques</p>
</blockquote>
</li>
</ul>
<h3 id="Fusion-with-Dense-Trajectories-always-help"><a href="#Fusion-with-Dense-Trajectories-always-help" class="headerlink" title="Fusion with Dense Trajectories always help"></a>Fusion with Dense Trajectories always help</h3><ul>
<li>The structures learned by deep networks are complementary to the handcrafted trajectory descriptors</li>
</ul>
<h2 id="What-algorithmic-changes-to-expect-in-the-future"><a href="#What-algorithmic-changes-to-expect-in-the-future" class="headerlink" title="What algorithmic changes to expect in the future?"></a>What algorithmic changes to expect in the future?</h2><ul>
<li><p>Moving towards <strong>deep architectures</strong> for action recognition is dominating the action recognition research lately</p>
<blockquote>
<p>Training deep networks difficult on video data -&gt; <strong>Knowledge Transfer</strong> (Use models trained on images or other sources)</p>
</blockquote>
</li>
<li><p>Blend <strong>3D convolutions</strong>, <strong>temporal pooling</strong>, <strong>optical flow frames</strong>, and <strong>LSTMs</strong> to boost the performance</p>
</li>
<li><p>Use <strong>data augmentation techniques</strong>, <strong>foveated architecture</strong> and <strong>distinct frame sampling strategies</strong></p>
</li>
</ul>
<h2 id="Most-State-of-the-art-Progress"><a href="#Most-State-of-the-art-Progress" class="headerlink" title="Most State-of-the-art Progress"></a>Most State-of-the-art Progress</h2><h3 id="Quo-Vadis-Action-Recognition-A-New-Model-and-the-Kinetics-Dataset-CVPR-2017"><a href="#Quo-Vadis-Action-Recognition-A-New-Model-and-the-Kinetics-Dataset-CVPR-2017" class="headerlink" title="Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset (CVPR 2017)"></a>Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset (CVPR 2017)</h3><ul>
<li>Propose a <strong>new dataset</strong> named “<strong>Kinetics</strong> Human Action Video dataset”, with 400 human action classes and over 400 clips per class, and is collected from realistic, challenging YouTube videos</li>
<li>Introduce a new <strong>Two-Stream Inflated 3D ConvNet (I3D)</strong>, reaching <strong>80.2% on HMDB-51</strong> and <strong>97.9% on UCF-101</strong> after <strong>pre-training on Kinetics</strong></li>
</ul>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/计算机视觉/" rel="tag"># 计算机视觉</a>
          
            <a href="/tags/深度学习/" rel="tag"># 深度学习</a>
          
            <a href="/tags/论文阅读/" rel="tag"># 论文阅读</a>
          
            <a href="/tags/Action-Recognition/" rel="tag"># Action Recognition</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2018/03/07/Qua-Vadis-Action-Recognition-A-New-Model-and-the-Kinetics-Dataset/" rel="next" title="Qua Vadis, Action Recognition? A New Model and the Kinetics Dataset">
                <i class="fa fa-chevron-left"></i> Qua Vadis, Action Recognition? A New Model and the Kinetics Dataset
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2018/03/09/Python常用函数/" rel="prev" title="Python常用函数">
                Python常用函数 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/uploads/image.jpg"
                alt="Chiyu Wu" />
            
              <p class="site-author-name" itemprop="name">Chiyu Wu</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">14</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">7</span>
                  <span class="site-state-item-name">categories</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">17</span>
                  <span class="site-state-item-name">tags</span>
                </a>
              </div>
            

          </nav>

          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/dmortem" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="mailto:cywu@zju.edu.cn" target="_blank" title="E-Mail">
                      
                        <i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                  </span>
                
            </div>
          

          
          

          
          
            <div class="links-of-blogroll motion-element links-of-blogroll-block">
              <div class="links-of-blogroll-title">
                <i class="fa  fa-fw fa-link"></i>
                Links
              </div>
              <ul class="links-of-blogroll-list">
                
                  <li class="links-of-blogroll-item">
                    <a href="http://seanshum.cn/" title="Shen Dong" target="_blank">Shen Dong</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="https://yao-zou.github.io/" title="Zou Yao" target="_blank">Zou Yao</a>
                  </li>
                
              </ul>
            </div>
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#What-is-an-action"><span class="nav-number">1.</span> <span class="nav-text">What is an action?</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Taxonomy"><span class="nav-number">2.</span> <span class="nav-text">Taxonomy</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Representation-based-Solutions"><span class="nav-number">2.1.</span> <span class="nav-text">Representation based Solutions</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Holistic-Representation"><span class="nav-number">2.1.1.</span> <span class="nav-text">Holistic Representation</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Local-Representation"><span class="nav-number">2.1.2.</span> <span class="nav-text">Local Representation</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Interest-point-detection"><span class="nav-number">2.1.2.1.</span> <span class="nav-text">Interest point detection</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Local-Desscriptors"><span class="nav-number">2.1.2.2.</span> <span class="nav-text">Local Desscriptors</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Aggregation"><span class="nav-number">2.1.2.3.</span> <span class="nav-text">Aggregation</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Deep-Architectures-for-Action-Recognition"><span class="nav-number">2.2.</span> <span class="nav-text">Deep Architectures for Action Recognition</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Spatiotemporal-Networks-From-3D-CNN-to-3D-CNN-LSTM"><span class="nav-number">2.2.1.</span> <span class="nav-text">Spatiotemporal Networks (From 3D-CNN to 3D-CNN+LSTM)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Multiple-Stream-Networks"><span class="nav-number">2.2.2.</span> <span class="nav-text">Multiple Stream Networks</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Deep-Generative-Models"><span class="nav-number">2.2.3.</span> <span class="nav-text">Deep Generative Models</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Temporal-Coherency-Networks"><span class="nav-number">2.2.4.</span> <span class="nav-text">Temporal Coherency Networks</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Datasets"><span class="nav-number">3.</span> <span class="nav-text">Datasets</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Performance"><span class="nav-number">4.</span> <span class="nav-text">Performance</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Current-State-of-the-art-solutions"><span class="nav-number">5.</span> <span class="nav-text">Current State-of-the-art solutions</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Handcrafted-Solutions"><span class="nav-number">5.1.</span> <span class="nav-text">Handcrafted Solutions</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Deep-Net-Solutions"><span class="nav-number">5.2.</span> <span class="nav-text">Deep-Net Solutions</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Fusion-with-Dense-Trajectories-always-help"><span class="nav-number">5.3.</span> <span class="nav-text">Fusion with Dense Trajectories always help</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#What-algorithmic-changes-to-expect-in-the-future"><span class="nav-number">6.</span> <span class="nav-text">What algorithmic changes to expect in the future?</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Most-State-of-the-art-Progress"><span class="nav-number">7.</span> <span class="nav-text">Most State-of-the-art Progress</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Quo-Vadis-Action-Recognition-A-New-Model-and-the-Kinetics-Dataset-CVPR-2017"><span class="nav-number">7.1.</span> <span class="nav-text">Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset (CVPR 2017)</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Chiyu Wu</span>

  
  
    
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Gemini</a> v5.1.4</div>




        
<div class="busuanzi-count">
  <script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv">
      <i class="fa fa-user"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
      
    </span>
  

  
    <span class="site-pv">
      <i class="fa fa-eye"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
      
    </span>
  
</div>








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  

  
  

  

  

  

</body>
</html>
